 Welcome to Observe. One of the key ways to investigate data in your environment is through the log explorer available on the left-hand menu. Here you can select a logs-related data set, and here I'll take a look at container logs from my demo data that's hosted on Kubernetes. Here I can view these logs, and then on the left-hand menu here I can filter to specific fields or columns. And the query will automatically update to show that data. I can also use the expression builder up here to filter for things like an error. And then I can investigate my error messages, or I could also look at the related resources that indicate where this log came from, like the pod, container, cluster, namespace, and more. If I click into any of these related resources, I can quickly get an overview or view related metrics in real time. Now if I'm in an incident, and I want to see these logs in near real time, I can use the live mode up here. And lastly, I can use the actions menu up here to quickly pivo
t into creating a monitor, and then alert on these error messages.
 Damn girl, that was both impressive and given the ability to navigate so quickly to an offending line of code Quite terrifying makes for some great blamful problem solving Now if you're like me this information isn't quite enough to be convincing because there's not a human demonstrating social proof Well, you're in luck. Let's hear from some actual customers who are using the product We've got two customer videos the first is from line data There are financial services company that is rapidly moving everything to AWS and has security and compliance challenges So this is like key for them because they need their customers to trust them

 //Intro Music//
 Thanks for watching the Observe launch. Now this is our second launch, and we're going to focus a little bit less on the company today and more on new product features and some of our amazing customers. If you're new to Observe, the first thing that you should know is 
that we're taking a very different approach. Recent research by the 4.5.1 group shows that organizations use seven tools to troubleshoot and monitor their applications. They've got many tools because they have fragmented data. We believe that Observability is fundamentally a data problem. If we can solve that, the tools problem will take care of itself. Now you'll often hear vendors talk about Observability as three pillars, logs, metrics, and traces. When we look at the customer experience in using these tools, we find that the DevOps team is spent in all of their time doing Ops, tasks like tagging and archiving data. There appears to be not much dev in DevOps these days. Too much effort is going below what I call the value line, and the SRE and engineering teams don't see enough value. There's got to be a different approach. With Observe, we flip the iceberg. We use a commercial database snowflake as our underlying data store, so a lot of those operational tasks just fundamentally di
sappear. Our engineering team focuses their effort like above the value line. This enables more users, including roles such as customer success, to do what they really want to do, which is to understand and analyze their application. Now what's our approach to delivering all of this value? The most fundamental step is putting all of the telemetry data in one place. After all, it's 2021, and cloud storage is practically free. If your data isn't in one place, it becomes really hard to relate the piece parts, which is critical to providing context, which is critical to observability. Now, the magic in the system is that the raw data, it's messy, it's machine generated, it's gobbledygook, it's not understandable by humans. Conventional wisdom is to provide users with a search bar and have them go looking for breadcrumbs. We think that that's a terrible starting point, and so we transform the machine data into something called the Observe Data Universe. Event data is curated into things cal
led resources that users understand. These are things like customers and shopping carts and pods and containers. No other product has this abstraction layer, and it allows us to do something pretty amazing. Most importantly, Observe establishes relationships between these resources. This allows users to quickly locate additional contextual information for the problem that they're looking at. The user can navigate the graph using our grappling feature without knowing the exact path between the nodes. Let's now talk about some of the new features in Observe. When we looked at the state of the art in systems that analyze metrics, we found them to be confusing, often presenting hundreds or thousands of metrics and tags to the user. If you knew the metric that you wanted to look at, you were all set, but if you didn't, you were never going to find it. Users get lost in their own tag soup. We wanted to change the game. Observe curates metrics. Curates metrics. So that users only see the metr
ics that are relevant to the part of the system that they're looking at. In this example, a view of the customer were seeing average response times, error counts, and the number of tickets that they've raised. If we want to add something more to the dashboard, we just drag and drop it from a curated list. And because the user only sees relevant metrics, they're not overwhelmed. Let's turn our attention now to alerts. Alerts aren't new in concept, but it's amazing how unwieldy they are to deal with, even today. Too many inboxes are filled with too many alerts that contain too little information. As systems become more complex, something has to change. Like metrics, observes implementation is a game changer. Our alerts feature relevant contextual information, so the user knows exactly where to start their investigation. Now, we can do this because of the data universe, the graph of connected data sets that exists behind the scenes. In this example, we're alerting on errors in application
 logs, but we also know exactly which customers are affected. Why? Because the customer's resource is linked to the application logs. Now, speaking of customers, we've made great strides with our early customer since last October. I'm pleased to announce that we have over 20 paying customers that use Observe on a daily basis. We're part of our founding customer program, and helping us define the product roadmap. I want to say a big word of thanks to all of those customers for their trust in Observe at this early stage. And finally, a word about our pricing model, which I believe again is another game changer. Our pricing is usage-based, so customers only pay us when they're using the system. Even better, we itemize bills right down to individual data sets. Imagine if you got an electrical bill, and it itemized the top 10 appliances that were consuming electricity in your house. That would be a beautiful thing. That's exactly what we do. We believe in usage-based pricing, but we also be
lieve in being fully transparent with where the money is going. Thanks so much for your time today. I really appreciate you tuning into the update. After this break, we'll be back to hear from the Observe founders on what they were thinking when they were implementing many of these new features. Thank you for your time.
 On to take just five minutes of your time to tell you a little bit about what observed does and the customer problems we solve. As most of you are aware we live in a digital economy. Whether you're a product company, services companies, or with COVID, a government agency like Education, you're doing everything online. Every company has to become a technology company. Key to delivering a digital product or service is software and we build software differently. We use microservices and we deliver new features every day using continuous delivery. There's never been more change going into production on a daily basis. And when something goes wrong, the customer notices. The
 stakes are high here because we know that when a consumer has a bad experience online in 76% of cases they don't go back. Now when something does go wrong, the scene resembles a murder mystery movie. The smartest guy in the room trying to figure out the answer to two questions. What happened, which is the easy bit, but why did it happen, which is a nightmare? And it's hard to find out why something happened because the date is fragmented. No one, even the smartest person in the room has access to all of the information. They have to piece it together using their intuition and knowledge, which is not sustainable. But the worst bit is that it costs a lot of money to know nothing about your organization and what's going on in your systems. Legacy vendors charge for volume of data ingested on metric points that preventing you from ingesting the very data you need to troubleshoot your problems. Something has to change. And this is why we created Observe. The current log analytics and metri
cs monitoring and APM vendors, they're not getting the job done. New problems need a new approach and we firmly believe that observability is that approach. Now when we think about observability, we think about it holistically. We don't just think about cloud infrastructure or database infrastructure. We don't just think about the business applications or the service desk or maybe the CICD infrastructure. We think about everything. Why? Because everything is related. When the customer has an issue with the application, they're going to raise a service desk ticket. The issue may be in the database or it could be in the underlying Kubernetes infrastructure or it could be with the AWS infrastructure. You don't know. So you have to have this wide angle field of vision. We wanted to take a different approach with Observe. We didn't want to become a tools company. The customer has enough tools. We fundamentally believe that observability is a data problem. An Observe is a data company. Now k
ey to everything here is us being able to ingest all of your data. We don't subscribe to this three pillars of observability view, logs, metrics and traces. We treat everything as an event and we start all data together. What we do subscribe to is cloud storage economics. We think you should be able to ingest as much data as you want and keep it for as long as you want. Once we've ingested the data, we shape the data. Why is this important? Well, we didn't want to give users a search box and have you go looking for breadcrumbs. We curate the data and turn it into users and sessions and shopping carts and pods and containers. It provides a much more logical starting point. Most importantly, after we've shaped the data, we relate the data. Behind the scenes, we create a graph of connected data sets and we don't do it using tags. Tags are a nightmare. It's not a sustainable way to join data that become un maintainable. We think that by connecting data sets, the user can navigate data more
 quickly and bring all of the context they need to bear on the problem that they're looking at. And finally, we keep track of time. Modern systems are a femoral. Different things run in different places at different times. The question maybe isn't what happened. It's what happened at 2 a.m. last Tuesday when the user received an error on the website. Observe keeps track of state of all the components so you can wind back the system to any point in time. You may be thinking at this point, well, observe, is this some kind of data tool for data scientists? The answers no. We built the interface for the SRE team and we realize that the SRE team may have junior members that are on call and are trying to triage problems in the middle of the night. So you wanted an intuitive visual dashboard approach to allow those users to use the product. But we also recognize that when you're investigating an issue, some of these issues are complex and hairy. You've got to get in and deal with the data dir
ectly. So whether you're a junior member of the team or a seasoned member of the team, you can work with Observe. Taking a step back, what Observe does is it allows you to find problems and order of magnitude faster. And we do this because we present things at a familiar and we allow you to quickly navigate to additional contextual information, which will help you solve the problem. The example on the screen shows a navigation from help test tickets right the way through logs, right the way through to even Jenkins builds. And the best news is that we have usage-based pricing. We separate storage and compute. So you can ingest data at the cost of S3 and you only pay when you use it. Getting started with Observe is simple. In fact, you can do what you're doing today. But as you drive towards this goal of a fully observable environment, Observe will allow you to keep on ingesting the different data types that you think you need to find the problems that you see. Thank you very much and th
anks for listening today.
 Hello and welcome to the observability trends webinar series. I'm Grant Swanson your host and today we will explore the topic of Generative AI and how to use OLEGPT for advanced observability. We'll kick off with a short presentation followed by a live product demo. Feel free to type any questions into the question window at any time during the webinar. All questions will be answered via email immediately after the session concludes. I also wanted to highlight that we will be conducting a hands-on observability workshop on Tuesday March 12th. Everyone who registered for the webinar will receive a link to the webinar recording and a link to the upcoming workshop. Now I'd like to introduce our guest speaker, Field CTO, Tom Bacheler. Welcome Tom. Thank you, Grant and hello from me, Tom Bacheler, Field CTO here at Observe. In today's session we're going to walk through and observe a Generative AI and how we're leveraging these new technologies to go and help peop
le achieve their observability goals. So let's dive into a couple of things before we get into a quick demonstration of some of these new capabilities. So first of all, what are the goals and what are we going to try to do with some of these new technologies? So let's just look at a couple of things that theories or areas that we've been working on here over the past few months. One of the first things we realized and this happens with any tooling is people quickly want to be able to go and understand how they can achieve a goal. Maybe they want to be able to go and visualize some logs. Maybe they want to go and pull out some metrics, etc. Now, often where people want to do this, especially if they're brand new, logon results using search engines to go see if they can go and kind of find some examples or someone's built some steps or something like that. While we found using GPT, we can go do that for them. We'll go and have a look at how we can just easily get people that step by step
 information and go and have a look and how they can go and achieve what they're looking to go do. A second thing that we found is people actually often want to help understanding the logs themselves. That could be kind of helping us in unstructured logs. That could be, I want to know what this error message means. So we'll go and take a look at how we can help people go and achieve those goals. And finally, we have a query language. We call it observe processing and analytics language. So we want to be able to go and help people construct those queries and not have to resort to diving through reference documentation to understand exactly how to use different verbs and functions or maybe even switch want to use. So let's take a look at what are some of the specific features that we've added to the product recently. So the first one we have here is Oli help get quick answers to product questions. For example, how do I go and view my container logs? Let's go and get that step by step int
o people's hands. They don't have to go dive through documentation. Oli explain help people explain logs in plain English. A lot of the motivation for this feature came from what we identify as a common pattern. People will use a login tool. They will go and find maybe a particular error message or something like that. They will take that error message. They will paste that into a search engine. And then they typically end up on kind of some other site where people are discussing what that error message means, how we might go fix that, etc. So as a thought there that we could potentially sure circuit that process and get people some of those answers quickly. What are my personal favorites and you had a lot of excitement for this one when I first saw this work. Oli extract help people build reg X's to pause unstructured logs. This is fantastic both for new and experienced users and as we get into the demo we'll touch a little bit more on why we think kind of some of that is particularly
 exciting. And then the thought we have here is Oli copilot helping people build queries in the pool. Again, this one is another one that I find super interesting. Just before we dive into the demo, just a little story on this one. So as we'll see as we go through what Oli copilot will do is it will go and suggest actual specific opal queries to go and answer some query wants to run that we've gone and described in plain English. Now, if anyone who's listening has built one of these features it, you know, themselves or use some of these kind of generative AI technologies heavily. We will know that sometimes it can. So it can invent things that that maybe doesn't exist or maybe aren't true. Well, there are a couple of occasions that we're building this where it would go and hallucinate particular functions or verbs that exist in a language. Those things did not exist. So we had some interesting decisions to make when we hit those situations of. Do we now go and refine that model? So if 
I refine that model, can I make sure we don't kind of it doesn't give that syntax or that function, etc. That doesn't exist. But there were other situations where we look at that and we actually thought to ourselves. We really should go have that in a language. It makes complete sense that the the the the gently AI has hallucinated that and that should exist. So we think to ourselves, OK, that's great. Well, we should maybe go kind of add that in a couple of cases where that's happened, which I thought was kind of pretty interesting. However, we're not all here to go and sit through some slides. I know we want to dive in and see some of these features in action. So I'm going to go ahead and do that. So give me a quick second. So here I have I quickly dive over into observe into our product. And this is really kind of where you'll land if you come. So just for those of you who are not familiar. If we see here, I have. I have some kind of key data sets that we have highlighted here and t
his is a, you know, in this environment. This is just a really simple demo around. So this is the things that people would typically want to kind of go to first and go and take a look, etc. But let's imagine there's maybe there's some instant maybe maybe I'm a developer and I've released something and someone's letting me know that something wrong or I want to go and have a look at something or something like that. And as such, I may not use this tool and very often, right? It may not be something that I come into today. I might be brand new in the organization. So then I say, OK, I know whatever I released this runs in containers. I want to go have a look at my container logs. How do I go to that? What if I click on the GPT here? I get a chat box and we're all fairly familiar with this and I can put some questions in here. How can I view my container logs? And let's go ask that question. Let's go see what happens. OK, awesome. So it's returned a number of different steps to me. So her
e we go. So OK, so let's go and have a look at so step one log into observe. Well, we've kind of already covered that. We're here. We're logged in. We're sat in the tool, etc. From the left navigation bar under investigate, click on logs. OK, cool. We can go ahead and do that. OK, number three, in the search log data set field enter container logs and select it from the search results. OK, well, I could do that. Actually, it turns out by default, I have container logs here. So I think we're in a pretty good spot. And this is kind of interesting. So now here I have a whole set of container logs. And I will all I did was just follow some simple steps that the bot generator for. So that was kind of pretty cool. OK, so let's you know, we're in our logs and you know, we'll keep the use case here pretty simple. So today we're we've been to kind of work quite a lot with kind of engine X access. Knockers there, particularly your fancy or advanced actually quite the opposite. Something hopefull
y many people on this session are familiar with. So we can really kind of go and focus on the features, rather than what goes and sits in the logs and what's happening there, etc. OK, so let's do a couple of things in here. So let's filter on my web server. Unless just filter on standard out. So we just go and have a look at some of these logs here. OK. So this is great. So I have access logs coming from all of my engine X containers. That's awesome. There's a few different things that are less and awesome. So if I kind of double click on one of these logs. So I have this string. You know, it is sort of structured, but it's not really kind of an unstructured log. So you see I have some interesting things in here. I kind of have an IP address. So that kind of stands out. I can see I've got a method. I've got a URL that sits up here. That's great. Then I have OK, so this is probably my response code. I may not know what this number is. And some of the other things, etc. So even in this v
ery simple case. I actually come up with situations where you can kind of glean some information, but other information kind of may not be so hard. Maybe be a little bit harder to go get it. I might actually end up and go off and hit a search engine and kind of ask for the format of these logs or something like that. So I can pull things out, etc. I don't understand a little bit more about what's going on. Well, this is some of the steps that we wanted to short circuit as we build out some of this technology. Right, I don't, you know, that's kind of taken me away from from what I want to achieve is extra steps that sit in there. Now I start to kind of have tap proliferation and more and more things open. I'm trying to synthesize it a lot in my brain. And you know, one of the things that we think about when you know whenever building observability tooling that people using for troubleshooting is, you know, there's going to be quite a lot going on. There's probably a lot to think about. 
It might be quite a stressful situation. So can we remove kind of some of that load, remove some of the things people need to think about maybe. Just to go and try and make people's lives a little bit easier. Well, in the presentation, I talked about all the explain that that could go and, you know, take some of these messages and help us understand what they mean or maybe help us come out and play English or something like that. Let's go try that with one of these. So I hit this little dot here and I can say explain this message. So this goes off to GPT and goes and ask what this message meant. So here we go. So we can see here. This is, okay, this is in the common log for that. So maybe we did or did not know that as we went and hit this. And now we've got day and time. I can see, okay, response code. Ah, that number where maybe I wasn't quite sure what that was. That's actually my response sites. Okay, I can see the referer user agent, etc. So again, this is breaking this down and. 
And we can see here that was questions made with Firefox on a Macintosh computer. Okay, I didn't even know how to interpret that. That host that browser strings. So that's pretty interesting that we've gone on got that. So that's got a pretty cool that we can see with a variety of different log messages. How this could be kind of helpful and quickly accelerate just, you know, understanding the meaning of what we're seeing in the data, etc. So that's some problems. So I did this for one message. If I kind of closes window, we can see we kind of have quite a lot of messages. You know, actually I've got 27,000 here in kind of the last 15 minutes. And I might want to go and look at kind of bigger windows of time now. Clearly, I cannot go through 27,000 messages asking it to explain to maybe go and. Get some counts of things go and charts and things, all of that kind of stuff. I need to have a better way of understanding this unstructured law. Now, typical way that we go about this is we wo
uld use a regular expression. Now I might sit here and write a regular expression myself. I might go and, you know, go and look one up on the internet and go and hope that that's right and kind of copy paste that in and use that to go pause out this log. Well, here's something that the personally I'm kind of very excited about. So if I could have clicked on this column header and I've got extract them string. And just like traditionally, this is where I could type in a regular expression. Even if you're pretty adept at regular expression, it's going to take some time to go and write that out and. And get that right and put out all the my capture groups and then make sure they're all tight correctly and you know it's not that I couldn't do it. But it's going to take me to you know, amount of time. I spent a lot of my career writing regular expressions and it is. It is not uncommon for me to sit down and start now I don't profess to be amazing at writing them. But it's not uncommon that 
I would take a lot of my this and I would sit down to go write that regular expression. You know, using some of the regular expression kind of help the tools out there. And again, my head down on my head pops up and 45 minutes have disappeared and now I suddenly have my regular expression. Well, what we can do using using GPT here is I can go and click this extract reg X button. And let's go ahead and click that. So what is going to happen now is this is going to build a reg X for me that I can just go and apply to all of my data. So let's go ahead and fly here and go and see what we get. So we see here we kind of started to build this. And this is the original log line I had. But now I have all of these new columns, right? I have my IP address date and time. Here we go. And now I start to pull out some useful things. I kind of pull down my URL here. I have my response code, my response size, etc. So as we, you know, as I'm sure we're familiar with, you know, that unstructured informat
ion. We can kind of eyeball and go through this. If we want to start doing visualizations, representations on this data. Understanding a little bit more around kind of try to pull out, maybe try to pull out the signal from the noise and kind of a sea of logs. We definitely want this data broken out into columns. Because then all of those operations become a little bit easier. And actually, let's do a couple of things in that regard. Let's go and kind of play about with this data now that we have it and see kind of, you know, why we want to use these tools to go and pull this information out. So real quick, let me go and hit visualize here. And I'm trying to change this. I want to go back. I will get the kind of past four hours worth of data rather than a little 50. Okay. So what we built here is. It's just a line chart and it's just showing a count of values right now. And we could have done this with the rule, right? It's just a set of logs and we just go and count them up. But you kn
ow, we use the reg X extracts. We had the GPT build the reg X forest. And of course, with observed underlying, you know, schema on demand. So that can go and just we can go add in additional columns into our data on the fly. And then it was us to go and use these things and you know, why is it a little bit more interesting and another little bit better. So in my cut in my expression builder here, over time using can values of all events, but I have no Greek white. Right. So it's just a straight count. Well, now we pulled out some of these things into those columns. For example, status codes. I can go and group these things by status code and then go and run this query and see what's happening. Okay. So first things first, my overwhelming kind of count is is 200s. So that's kind of pretty good. But if I look down here, I have some 404s. I might kind of have some kind of 500s in here, 502s. Oh, that's kind of a 117. Then that's a little bit of spike of a 502s and 503s. So all of these ki
nd of errors go sit in there. Again, kind of schema on demand, doing that extract makes it real easy, not just the visualization, but also to go ahead and apply filters. Now, one of the things that happened is it wasn't just about having the capture groups and pulling out those fields. And that reg X also added in type information. Now, that's important because maybe I just want to care about kind of my 500 and up status codes. I really want to go and just look at those errors. And that's really hard in this because I got some 404s. So maybe someone's accessing something that doesn't exist. A whole bunch of 200s. It's really difficult to kind of go figure out those errors and what's going on. Well, I can have status code in here. We see that has a type of in 64 that was embedded in the reg X that got generated. I can say, great. Then equal to 500. So just filter down to those errors that we have in my data. And again, we're going to run this. Okay. Now I have a very different chart. So
 I kind of get these sporadic kind of 502s. I've got some 503s. This is really what I'm seeing here. So now it looks like I've got down into data that's showing me some things that aren't going right right. And really is maybe where I want to focus my time and focus on my efforts and. Well, now we go down to that data. We probably have a lot less. So let's go and have a look. They didn't go and see what's going on. So we'll click back into the logs. We keep all of the same filters that we had for the visualizations. But we had thousands of thousands of log lines. And now we have just 1,380. So an awful lot less. So here we go. And I see my kind of smattering of error codes, etc. Now, let's say I want to get into. I want to look at some summary information. I want to know like I've got a whole set of URLs here. Like which endpoints on my application are kind of seeing these errors? Is it across the board? Is it a smattering of a few? You know, even with we reduce the data set a lot. But
 we're still dealing with over a thousand rows. And just by building that is going to be hard to kind of figure out where that impact is what's going on, et cetera. Now this is where kind of, you know, the final thing of, you know, we went through kind of the colby bot. We sort of had a look at the oh they explain. We looked at the extract. And now let's get into co pilot and see how we might leverage that and to see what's going on. So we focused all the work we've done. And really kind of been with this build us so far where we've leveraged the UI to go and apply some filters. and change between visualization logs, et cetera. But sometimes we just want to step down and get into the query. So that's where I go and hit it open. And we see a couple of things here already. So remember a few minutes ago, I kind of filtered on a web server and standard out. Well, this is the opal for those filters. When I did my all the extract, this is the reggae that it generated. So yeah, imagine trying
 to go and create this in the middle of an incident. Much better to have something I can just go and have generating that can be going my day. And of course here we have my status code filter. Now our question when we came in here was, which endpoints were affected and how much of those things. So I probably want to kind of get a count where URLs. So let me just add a comment here, count by URLs. So I'm going to hit a button here. We'll see this copie that will flash up. So we'll go ahead and hit that. And it's thinking and now it's generating. Some opal point. And this is the correct opal. This is a stat by and it's going to count the URL column and group by URLs. And I did not have to know how stat by works. Let's go ahead and execute this. Okay, fantastic. So now I have all of these endpoints. And I can see kind of yeah, really filter down on the errors. So I can see a count of each endpoint by errors. But this is still a little bit awkward, right? It's hard for me to tell what's mo
st impacted, what's least impacted, et cetera. Well, let's try this again here. I am going to keep this super simple. I'm just going to say sorts. I'm not going to give it kind of any more description other than that. Just I want these things sorted. Can you please go and figure this out? So we will hit the copie button. And here we go. Okay, so we have the sort verb. That's great. Descending. That's probably what I want. Fantastic. Let's see what happens when we run this. Okay, this is amazing. So now I can see that this manufacturer endpoint, the fall is probably a parameter in here. It looks like that's kind of the most impacted. And I'm starting to answer some of those questions and go and get into this. So that's a lot I wanted to run through today. So thank you very much for your attention. But yeah, we see we hit on using the Ollibod to go and get guidance over how to go and achieve a goal in a product. We looked at explain message to go take a log line and figure out what that 
means in plain English. We use all the extract to go and extract data from unstructured into structured logs and using schema on demand. And then of course, we've just seen how we've used the copilot. To go and help us go and write open queries. The other thing we found with the copilot is is not just for new users. A lot of experienced users are using this and leveraging this as well. Simply because it just saves time. I know what I want to do. I can go and tell the tool what I want to achieve and it will just go and generate that open for me. I can get my results and I can move on with my day. So thank you very much everyone. This has been a great session. I've been Tom Bachelor. Have a great day.
 마실 날이자oured Fortunately only sliced 됐다.
ığım o〜
 When we created our dashboarding feature, we wanted to go beyond the high-level isolated summaries of what's going on in your infrastructure software or business. We wanted to make dashboards a starting point for investigations and insights,
 not the frustrating dead ends they typically are. Because we understand the relationships in your data, the things in your business and software, when you create a dashboard, the panels naturally connect to each other and relevant content like metrics can be brought in quickly, no need to search. When you work with a dashboard, we continue to leverage this understanding of the things in the dashboard and use their relationships to connect you to other views of that data, whether it's more detailed dashboards or worksheets ready to continue your investigation.
 Welcome to the launch of our latest generation of Observe. With me today is astrophysicist Dr. Erica Hamden, who's also engaged in a launch of our own. Erica, the internal codename for our release is Project Hubble. So our project is called the faint, intergalactic medium, redshifted emission balloon, or fireball for sure. Okay, well you definitely outnamed us. Now we've been working on Observe for five years, and Project Hubble
 allows our customers to discover things they've never seen before, at a scale they thought impossible. So we've been working on Project Fireball for the last 15 years. Okay, you beat us again there. We use Fireball to observe some of the faintest structures known, huge clouds of hydrogen gas that we think flow into and out of galaxies. So we're both looking for things that are hard to find. Your universe is full of stars and gases, and hours is full of data. Seems like there's a lot of similarities between our two launches. Yeah, they're definitely our surprising similarities. Okay, so let's explore that a little bit more later, and for now let's go do a company update. This is a big day for Observe and our customers. This is the largest and most ambitious release to date. We're also announcing today our latest funding, $50 million in Series A3, led by Sutter Hill Ventures. This money will be predominantly used to expand our sales team. We've already hit our sales plan for this fiscal
 year, and a seeing at growth accelerate. That growth is showing up in our usage statistics. Our platform is now ingesting almost 200 terabytes a day, and we're issuing almost 55 million snowflake queries per day, representing over 1.5% of their daily query volume. Hubble introduces a 100% new user interface. Dozens of feedback sessions and months of usability testing have led us to this point. We've always had incredible power in our platform, and with this release, we've made sure that power users right the way down to the most junior users can be productive. Productivity is not just about our new user interface. We fully embraced generative AI in all of our troubleshooting workflows. With Hubble, we don't just have a chatbot that can explain error messages and how to perform common tasks. It can write regular expressions. It can even act as an assistant during incidents. Reading alerts, navigating to relevant logs and metrics, and summarizing current status, conclusions, and timelin
es. This is just the start. We've scratched the surface of what can be achieved with generative AI, and believe that rich and structured nature of our metadata gives us a competitive advantage. Now, speaking of competitive advantage, let's talk about our platform. Hubble moves us beyond the magical one petabyte a day barrier in a single workspace, no matter what the size of the organization, observe can handle it. At the same time, we've been working to reduce latency, introducing a new live capability in our explorers, which allow the user to query data just 15 to 20 seconds after it was created. Finally, observe is opening up. We've introduced a new public API so that our platform can be accessed programmatically. We have a new command line interface, and we can share data in and out using snowflake data sharing. With Hubble, observe has never been as productive, as able to ingest as much data, query in as little time, and as easy to share data with other users and applications. It's
 our biggest and best release ever, and it's available for you to try in our new free trial at observing.com. I believe that Fireball's insights ultimately will enable us to answer the question of how our own galaxy was formed. Now, it's great to talk about the successes of new telescopes and all their discoveries after the fact, but the reality is that with any ambitious project like this, there are a lot of failures. Fireball, we had mirror problems, cooling system issues, calibration failures, and failures from sources we couldn't possibly have predicted in advance. We even had an angry baby falcon land on our spectrograph one day. I know it comes as no surprise to all of you when I say that software fails too, probably more often than you'd like. And that's where Observe comes in. Observe knows that failures will happen and is prepared to support you when that happens. Let's hear how they've been helping TopGolf troubleshoot incidents faster, because no one likes it when angry bird
 crashes. Hi, my name's Ethan Lilly. I'm a senior engineering manager here at TopGolf, and I've been here for about four and a half years, and I love it here. It's a great place to work. Observeability is very important to us at TopGolf. Technology is what drives the whole venue. If if any one piece goes down, it could affect the entire player experience from the games to getting into your bay, to just making a reservation online. So we have to make sure everything is up and running at all times. So before we used to have to use multiple tools, I used to have three monitors up in my desk with different tools open all at the same time, trying to correlate this time stamp over here with this time stamp over here. But with Observe, it's all in one place, and you can see it all in one view, and you don't have to jump around different monitors. You can just easily click through the UI and find exactly what you're looking for. Observe, we knew right away that this tool was different. This to
ol is very helpful in giving us the data we need and correlating that data for us. So Observe is much easier to use than other tooling, and it definitely gets us up and running faster and keeps us that way. One of our favorite games at Topgolf is Angry Birds. Players will hit their ball and will use camera systems to track the ball flight and then overlay that onto the game so that it tracks your bird flies exactly the same path as your ball. We had an issue at one of our venues where some of the balls were not getting traced properly. So using Observe, we were able to correlate the game log data with our hardware metrics and figure out that there was a misconfiguration on the back end infrastructure. After fixing it, players were able to once again hit the balls and let the birds fly. At Topgolf, we continue to add new venues every year. Each new venue comes with a whole new set of data. With Observe, we're able to keep our data for up to a year, which allows us to monitor that data o
ver an extended period of time and show us different trends that may be appearing that we may not have noticed before. Before we used to have to pay per user for people to access our monitoring data, with Observe, we can have as many users as we want, allowing us to give access to everyone that we would like to see our data, including our support teams, our development teams, and anybody else that we think might be useful. We needed a solution that would scale with us and Observe is that solution. Over the past six months, we've become obsessed with ease of use in a new user experience and observe. We've conducted dozens of user interviews and live usability tests that have made major improvements to observe UI based on this feedback. I'd like to share a few of these improvements with you now. First, I'm excited to announce our new log explorer UI. The log explorer is intended to be a one-stop destination to browse, search, and analyze all of your log data. When you come to the log exp
lorer, you can quickly flip between all of the log sources being sent to Observe. You can easily search for errors and other keywords, and you can visualize the results of your search all without leaving this one page. Because all data and Observe is linked together, you can also drill in on these log messages to see additional context, like the configuration of related resources or even related metrics. After finding the log data you're looking for, you can share a link with a colleague, create a monitor to watch for errors, or add your visualization to a dashboard. The log explorer UI is driven by a new query builder, which was designed specifically for working with log data. This means that you can access all of this functionality without learning or using a new query language. Along with the log explorer, we've also introduced a new metric explorer UI. The metric explorer is intended to be a first-class destination to find, plot, and analyze all of your metrics data. You can quickl
y search and browse all of your metrics with a metric picker. After picking a metric, you can use the metric expression builder to configure filters and aggregations for plotting your metrics. You can then drill in on these charts and quickly pivot to any related logs and dashboards. Once you're satisfied with your chart, you can share with others, you can create a monitor, or you can add it to a dashboard. Like the log explorer, all of these actions can be accomplished without using a query language, so new users can settle in quickly. For both of these explorers, we had two goals in mind. We wanted to streamline the adoption of Observe for users migrating from existing open source commercial monitoring tools. These new experiences should feel familiar and intuitive to new users no matter where you're coming from. Second, we wanted to make it even easier to take advantage of Observe's unique ability to link and correlate all of your data together through connected resources. That is, 
anytime you see a user, a service, or a container, and Observe, it should be as simple as clicking on it to navigate to related logs, metrics, traces, and dashboards. Now, in addition to the new explorers, we've also revamped the navigation bar in the product to make dashboards, monitors, and ingest configuration easier to access. We've introduced a customizable homepage to help you organize content for your team. We've updated dashboard editor with a new focus editing mode, and much, much more. I'm excited for you to try the new Hubble UI. The light sensor, the detector, is the heart of any telescope. I've been working on detector development since 2008, and when our first sensor worked, it was glorious. And now our sensors are 10 times better than the previous state of the art. We get very excited when new technology comes along because it gives us a new way to see the universe and our place in it. One of the best things about new technology is that sometimes we find things we weren'
t even looking for in the first place. The latest and greatest innovation in the world of tech is AI. I'm sure you've heard about how large language models are going to change your life for the better. Observe believes that AI is going to improve user productivity somewhere between 20 and 50%. Enabling users to get to those new insights faster than ever before. To discover things they didn't even know they needed. Here's John, along with another Erica with a C, not a K, to explain exactly how. When Siri and Alexa came out, they changed how we interact with our devices. Now that generative AI, like chat GPT, is available, it's changing how we interact with our tools. All EGPT is observed generative AI assistance to help users troubleshoot incidents faster. Observe users can ask questions in plain English using a familiar chatbot interface to fetch data, extract fields, and make sense of error messages and much more. All EGPT can even act as an assistant for on-call engineers to remedi a
n incident via Slack. All EGPT answers users questions about how to perform common tasks such as finding relevant data sets or what configuration to use in order to ingest permeteous metrics. Hundreds of users summon all EGPT on a weekly basis. And with the Hubble release, we have fine-tuned it on our latest documentation and best practices. With Hubble, all EGPT provides users with more data insights. For example, highlighting an error message in a log now explains that error message regardless of source. Go language or web server or Kubernetes. In addition, new logs often arrive unstructured, so users may have to write complex reject statements to parse them before they can even perform analytics. This can be a nerve-wracking experience during an incident when time is precious. All EGPT not only generates the rejects, but also names the resulting columns appropriately, such as URL or status code. With Hubble, all EGPT is now able to act as an incident assistant within Slack. The inci
dent assistance can read observ alerts surfaced in a Slack channel and take the user to relevant log lines, inspect patterns, and help the user get to the root cause. It can even page the right on-call engineer if required. New users joining the incident Slack channel now benefit from an all EGPT generated incident summary. And when the incident is resolved, the conclusion and a timeline is generated for post-modern use. All EGPT can also save these summaries for future reference. So if an error comes back, the hard-earned learnings are right there. In addition, you can upload your own runbooks, so help can be specific to your own context. Another use of generative AI is OpalColePilot that was introduced earlier this year. New users can now learn, observes powerful query language, Opal, much more quickly than before. A user who observe can type something like search errors and logs, count by container, and chart it. Then immediately see and run, degenerated OpalCode. To power OpalColeP
ilot, we trained a private language model which not only reduces hallucinations, but it also improves performance and security. Customer data is held within Observe and is not accessible by third parties. More applications are using LLMs every day, so observing the data pipelines, feeding those LLMs is becoming an important use case. Observes open III application provides out-of-the-box content so users can understand latency, error rates, and accuracy of responses that the LLM is providing. All EGPT and OpalColePilot use generative AI to provide a natural language interface for users to troubleshoot incidents. The rich contextual data in Observe makes it easier for generative AI to connect the dots for users and assist their troubleshooting through a familiar chatbot interface. The Hubble release is a big step, but also the first step in leveraging this revolutionary technology. Sometimes people have a scientific question, and there's no way to answer it with existing telescopes. That
's the only reason you should build a new telescope because there is no other way to get the data that you need. Fireball hangs on a cable from a giant balloon and observes for one night only from 130,000 feet, because reaching the edge of space is much cheaper than going to actual space. My ultimate goal, what I really want, is to take our view of the universe from one based on observing the easiest thing, only the light from stars and galaxies, to one where we can see and measure every atom that exists. That's all that I want to do. But more seriously, a complete view of a system, in my case, a galaxy and all the atoms in its environment, is the only way to really understand what is happening. Similarly, observe has a unique perspective on observability. They want users to be able to see and analyze all the telemetry data that exists and do it economically. To do that, they needed a new architecture. Here's Chi, their head of engineering, to explain exactly how they've done it. In Hu
bble, observe scaled its core engine in data loading, query performance and data management to meet the enterprise requirements of today and tomorrow. First, data loading. Observe already loads hundreds of terabytes data a day for production users. To plan for their future expansion, we have added parallel inserts and time-partition tables to scale to over one petabyte a day. With Hubble, we are also introducing live mode, which helps incidents on core immediately see the effect of a fix. We added this feature with all and all switches for each log and metric data stream. The production data freshness is now down to 20 seconds or less. Our next update will take the number further down to below five seconds. Second, query performance. Hubble has added search indexes to speed up needle in haystack searches. Queries that in the past took minutes. Now take a few seconds. In addition, the OPPO language have been expanded to include a new text search syntax for predicates and references to c
ontextual data. Trend aligns and a few distribution of logs often point to the next investigation steps. We call these stats. Hubble's sampling technology makes stats queries an order of magnitude more efficient, which makes it easier to perform analytics on large data volumes. When serious incidents happen, hundreds of engineer query logs non-stop at the same time. Many exist in two or fails this challenge. To solve this problem, Hubble supports more than 2,000 concurrent users. Furthermore, the engine dynamically utilized snowflake warehouse up to six excel in total 512 machines in size as appropriate. Third, data management. Enterprise cloud environments are becoming more complex. In order to handle this, Hubble also adds fine-grained user access control, multi-side fail-safe, and settings to manage freshness of the data. We have seen many enterprise adopt a data lake strategy for low-cost long-term storage. We believe that observability data should be integrity part of it. To suppo
rt this, Hubble enables data to be both directly loaded from parkway format files in a data lake, and for enriched data to be shared back to data lake without copying. I'm very proud of the Hubble release. It is our largest and most impactful release to date, and is now ready for prime time in supporting the most demanding workloads. Like any leader and being a leader in this organization, when you have a bunch of engineers that have to use a tool, the easier it is to use that tool saves the company really a ton of money. And we move so fast here. It's a rocket ship over here. We're moving so fast. It's crazy. And we didn't have time to have to devote really learning observe. It was really easy to learn right out of the box. Another thing you guys have incorporated into your observability tool is AI. I can ask a question, and instead of it giving me a link to a document, it takes the steps from the document and just prints them out. I thought that was brilliant. We want to do that. Tro
ubleshooting is great. It's fast and easy, and it doesn't matter what part of the organization is doing the troubleshooting. It has learned how to do it in a very similar way. It's really nice. One thing that did happen, we did have an accidental data spike. Our data went from two or three terabytes a data, ten terabytes a day. And the system didn't even hiccup. It just kept running. It always had. The speed was there. Everything was there. But one of the things that is the most special part about this product I have to say is your customer support is... It is beyond stellar. It's amazing. But my big fear was going to be the cost. Because we were... To Kubernetes, I know the logs and Kubernetes are just... They're huge, right? When you go from bare metal to Kubernetes, your logs triple and quadruple. And when we looked at the price, I really couldn't believe it. It was a fair price. It would easily ramp up our ability to do observability. And it gave us some really great tools. All of 
these things have really married me to this product. I can't see using another observability tool. It gives us everything we need. The support is just stellar. It is first class. Thanks for joining us for today's launch. I hope you learned a little about Observe's latest and greatest innovations. And a little bit about just how awesome space telescopes are as well. And how effective, new technology and an openness to discovery are crucial for finding out how the universe around you works. I know many of you spend your days investigating failures in distributed applications. Remember, failure is inevitable when you take on ambitious challenges and push the limits of knowledge. But that's what we're both doing. Hopefully with Observe and all the innovations from Project Hubble, you can more quickly overcome those failures and improve the experience you're providing for your customers. Thanks for watching and stay curious. Thanks for watching. Thanks for watching.
 Hi, my name is Badge an
d I'm one of the engineers here at Observe. Today we're going to take a walk through the Observe solution and focus on some recent updates to the platform, particularly with respect to alerts and metrics. To begin, I'm actually not going to dive right into Observe. Instead, let's take a look at this Slack channel where I get alerts from Observe. Recently, an alert fired, telling me that some of our customers are facing errors. One of the key properties of alerts in Observe is that they leverage GraphLink to provide context around the notification. And in this case, even though we don't have a lot of errors, we seem to have a subset of customers who are experiencing a high error rate. And this could lead to some poor customer satisfaction. So let's dive in. Now I even observe. Here I get an overview of that notification. I can see that this is still an active incident. And for us, like any other SaaS company, when issues occur, a key question is who was impacted. This page helps me answ
er this question without diving further, so that I can evaluate the impact of the problem. For example, it looks like a few more customers have been added to the impacted list since we received the alert. Let's pause and think about what the root cause may be. Customer impacting problem could easily be pods stuck in pending state, maybe no memory usage over 90%, perhaps repeated fail logins through database, or really numerous other root causes. And in all of these cases, we want to get specific answers out of our investigation to understand the impact of the problem. By answering questions like which databases effected for which application in which AWS region? To do so, let's go take a look at the data that fired this alert. Now I can see the log lines that cause my alert to fire. I immediately noticed I have a lot of out of memory errors, which is a little alarming. If I scroll across, I can see more information. Like here, I have the stack trace for that error. But right now, still
 not really sure if this is a code issue or resource contention at the infrastructure level. I need to do a little more digging to figure that out. Because these datasets are linked together and observed in a relationship graph, I can jump to the Kubernetes pods that generated these log lines. Here, I see the pods where those error messages came from. These pods are currently active. And using the time scrubber at the top, I can travel back to an earlier time when these pods came alive first, which seems to be shortly before my alert fired. Now, what I'm really curious to look at is the metrics for these pods. If I scroll down, I automatically get in context metrics for these pods. And notice that I didn't have to hunt around following tags, carrying around names or IDs for my logs platform to my metric platform. Because I observe is a one stop shop for my logs metrics and other technical or business data, I can pull together metrics for my resources with ease. So because I was getting
 memory errors, it's really the memory metrics that I'm interested in. Scrolling down, I see a chart with my CPU usage metrics. And I can open it to see a more detailed view. And, aha, I see the signature so-to shape that is indicative of a memory leak. This is starting to look more like a code issue. The final piece of validation to check is whether these pods are restarting. To do that, I'm going to look at the notifications for these pods. So, observe alerts serve a larger function than just not finding you, be a slack or pager duty. They can also be used in context of an investigation. Here, I see notifications about pods restarting frequently, which is the last piece of the puzzle. So, I quickly got to a good spot in my investigation. I know which deployment is failing. I can open a ticket for the relevant team with this link so they can see the problem in context and fix it. But, I can take this one more step further. In my environment, I'm using continuous integration and deploy
ment. My CI-CD data also comes into observe. By using Graphlink, I can ask Observe to find build events from Jenkins to help me figure out the exact change that introduces code issue. Let's click navigate to here and select Jenkins builds. Before we take a look at the build data, let's see how we got here. At the top of the screen, we have the breadcrumbs. We started out by looking at the error logs, then jumped to the problematic pods, and then Observe seamlessly took me from pods to the relevant Jenkins builds. There are actually several hops in our relationship graph we have to go through. But as a user, I didn't need to know what path to take. Observe knew that pods are composed of containers that are running images that are built by Jenkins. Okay, let's go back to our Jenkins build data. Here, I see the exact change that codes issue. Apparently, Tom made a small change to cache code, which calls a memory leak. I can now tell Tom that his recent change is impacting our customers an
d needs to be fixed. Well, there's more more thing. As you remember, we started this investigation with an alert telling us that our users were experiencing errors. Well, I wonder if some of them actually open support tickets related to this. Using Observe, I can actually find the Zendesk tickets that are potentially related to this bad code change. Again, leveraging Graphlink, I can jump to my customer tickets without worrying about the underlying path. Here, Observe is showing me all the support tickets opened by users who interacted with that problematic build. I can let our support team know that we identified root cause of the issue, and we can communicate our customers that the issue will be resolved shortly.
 Welcome to Monitoring and Observe. You can find monitors from the left-hand menu, where they'll be listed here under the monitors tab. You can create monitors from scratch, or you can use a predefined template provided by different applications, like Amazon or Kubernetes. A
nd from here, I can search through predefined monitors and to use a template, I'll duplicate it just like this. And I'll be brought into the corresponding monitor configuration which most likely doesn't need to be changed at all. I could adjust the configuration, and then rename the monitor, save it, and then close the configuration and traverse back to the monitors tab. I'll see the new monitor created from a template here in my own monitors list. Now I can also create a monitor from scratch by clicking Create Monitor and see various different options for creating a monitor. So let's start with a metric threshold and the first thing we'll do is select our metric. So I'll search for CPU utilization. Once selected, I can see various different options for helping configure this monitor. The first one we'll look at is the expression builder, where I can actually filter to specific values like a container here. I can also adjust how this data is being aggregated. I could also change the gr
ouping. I could also add additional metrics and or logic to this monitor. And you'll notice that above here, there's a threshold preview which has been changing as the expression builder changes. Going a bit further, now I'll configure the trigger condition which will actually tell my monitor to send a notification. And so when our CPU utilization metric is greater than, the threshold, which is what we'll set here, but we'll look first to see if that threshold should be exceeded at all times, at least once on average or in total, for the CPU utilization I'll keep at all times. During the last 10, maybe five minutes. So my threshold again, I'll set here, which would typically be something like 80%. However, I'll notice that the CPU utilization right now is so low, it's hard for me to get a preview for when an alert would be triggered. And so I might change my threshold to a much lower value so that I can see what type of notifications would be triggered if this type of data exceeded the
 threshold. So these bands here define when the threshold was exceeded for that amount of time. Now I can see these notifications or events that would be triggered here in the preview of triggered alerts with the corresponding data that would have been sent as part of the notification. Speaking of the notification, that will be configured and sent through an action. We can choose predefined shared actions or we can define one ourselves in line with the monitor. So let's choose PagerDuty. So we'll see the PagerDuty webhook URL predefined, as well as the request body that needs to be in line with the PagerDuty API definition. And within this, there's mustache templating to reference the underlying monitor data to further customize your action. So let's take a look at this drop down menu where we can preview some of that monitor data and understand what can be referenced. Once completed with our action, we can look at the final configuration options for our monitor, which give us an optio
n to send reminders at some interval. I won't use those for right now. And so with that complete, I can name my monitor, save it. And so now I can navigate back to the monitors tab to see my full list of monitors and see the newest one here.
 I love simple and dog-like. That's why I date men. Well, that's been the Observe Product Update. I hope you enjoyed it moderately more than the average tech product show. We'd love to hear from you, so thank you so much for tuning in. You know, at the end of the day, we're all just humans, standing before a monitor, asking it to please give us something meaningful.
 Hi there. I'm Cloud Economist, Cory Quinn at the Duck Bill Group. You may have no idea what a Cloud Economist is, but that's okay. I made the term up. When you define what you do, that gives you a false position of authority from which you can insist you're right no matter what the topic is. Speaking of making up a field in which to master, let's talk about observability. What is obser
vability? Ask five people get seven different answers. I mean to me, it's hipster monitoring. For better or worse, that definition is not yet caught on because thought leadership is harder than we thought. The big problem in the space though is that we look at so much data coming in through so many different tools. It becomes one giant painful of glass and there's no good way to disambiguate signal from all of that noise. That is the fundamental tenet of why observability is important and why we're not all just stuck with Nagio still trying to patch it a little bit further like we were 15 years ago. Backing it up a step further. If we look at what observability really means, it means it's time once again for you to find one of the vanishing number of monarchies in the world and kidnap a princess for ransom because you're going to need to do that in order to pay the ridiculous fee. Now what is it going to cost you? Well, that's a fun story. The reason that they're so good at extracting 
signal from all of this noise is there are so many different pricing dimensions you won't know. In fact, it requires the energy output of a small coal plant in order to figure out the bill for these things alone. But I digress. That's not really what observability is. Instead, let's get observability to find for us by professionals, namely industry analysts. Instead of being paid to sell you things, they have a much more objective perspective because they're paid by people who are paid to sell you things. Let's begin.
 My name is Andre Butsar. I am the director of Cloud and DevOps at Mind Data. I went to college for computer engineering. I actually started my career in helpdesk, moved on to system engineering. I play a lot with Raspberry Pi, Arduino, and automating things around the house. My data is a FinTech company. We provide front-middle and back-office products for hedge funds, wealth managers, and banks. One of my responsibilities is to manage the Cloud Platform team, which is r
esponsible for building as-code frameworks to be reused by the various business units and engineering teams, as well as develop DevSecOps and GitHub's processes to improve developer velocity and quality of life. A lot of organizations, they might have monitoring, but monitoring is only a piece of observability. Observability is what brings all of the different pieces together, tracing metrics, monitoring, logging. Security is always tough of mind for me. We have dozens and dozens of AWS accounts, but with Observe, we can create linkages not only across resources but across accounts, which makes it very easy for our engineers to isolate issues, especially when you have dependencies on different services from different accounts. So we can troubleshoot an issue in our multi-account AWS architecture in Observe without ever having to log into an actual AWS account. It's not just security of the application, but it's also the security of the infrastructure is the compliance, the auditing. Wh
en we developed our GitHub process, we had SOC 2 and Monty. We're able to link Cloud Trail to the pull request information in Observe so that every single Cloud Trail event has links back to a pull request. So in the Observe UI, I can look at a Cloud Trail event and I will see who created the pull request, what the comment is exactly what's happening, who opened it, who merged it, who closed it, what had happened. Observe stood out to me for the simple fact that you can link disparate data sets together. You no longer have to copy and paste and remember what it is that you're looking for because one data set can need you to the next and the next like a trail of breadcrumbs until you find your way to the issue. The impact on line data has been tremendous, providing our engineering team all of the same resources that our SREs have in order to manage their engineering environments. That has allowed us to develop more secure applications and release better quality code into production.
 Wh
en Siri and Alexa came out, they changed how we interact with our devices. Now that generative AI, like chat GPT, is available, is changing how we interact with our tools. All EGPT is observed generative AI assistance to help users troubleshoot incidents faster. Observe users can ask questions in plain English using a familiar chatbot interface to fetch data, extract fields, and make sense of error messages, and much more. All EGPT can even act as an assistant for on-call engineers to remedi an incident via Slack. All EGPT answers users' questions about how to perform common tasks such as finding relevant data sets, or what configuration to use in order to ingest permit theist metrics. Hundreds of users summon all EGPT on a weekly basis, and with the Hubble release, we have fine-tuned it on our latest documentation and best practices. With Hubble, all EGPT provides users with more data insights. For example, highlighting an error message in a log now explains that error message, regard
less of source, go language, or web server, or Kubernetes. In addition, new logs often arrive unstructured, so users may have to write complex reject statements to parse them before they can even perform analytics. This can be a nerve-wracking experience during an incident when time is precious. All EGPT not only generates the rejects, but also names the resulting columns appropriately, such as URL or status code. With Hubble, all EGPT is now able to act as an incident assistant within Slack. The incident assistant can read observer alerts surfaced in a Slack channel and take the user to relevant log lines, inspect patterns, and help the user get to the root cause. It can even page the right on-call engineer if required. New users joining the incident Slack channel now benefit from an all EGPT generated incident summary, and when the incident is resolved, the conclusion and a timeline is generated for post-modern use. All EGPT can also save these summaries for future reference, so if a
n error comes back, the hard-earned learnings are right there. In addition, you can upload your own runbooks, so help can be specific to your own context. Another use of generative AI is OpalCodePilot that was introduced earlier this year. New users can now learn, observes powerful query language, Opal, much more quickly than before. A user who observe can type something like search errors and logs, count by container, and chart it, then immediately see and run the generated Opal code. To power OpalCodePilot, we trained a private language model which not only reduces hallucinations, but it also improves performance and security. Customer data is held within Observe and is not accessible by third parties. More applications are using LLMs every day, so observing the data pipelines, feeding those LLMs is becoming an important use case. Observes OpenAI application provides out-of-the-box content so users can understand latency, error rates, and accuracy of responses that the LLM is providi
ng. All EGPT, an OpalCodePilot, used generative AI to provide a natural language interface for users to troubleshoot incidents. The rich contextual data in Observe makes it easier for generative AI to connect the dots for users and assist their troubleshooting through a familiar chatbot interface. The Hubble release is a big step, but also the first step in leveraging this revolutionary technology.
 And then we've got top golf. They have over 50 locations in the US and the last thing they want when people are drinking beer and playing Angry Birds. Yes, with a golf ball. Is a problem that they have to wait for their IT guys to sort out.
 In most metrics products, you're dropped into a sea of often cryptically named metrics, with little knowledge of how to get the right metric for the situation you're in, or even how to get that metric drawn correctly. Alright, so typical situation. There's a problem with X. What metrics can I find that might tell me about X? You're scrolling through pag
es and pages of dashboards trying to find something as relevant. You don't find it now browsing through thousands and thousands of raw metrics of how much you're looking for. The metric naming is all over the place, tax rate consistent, it's a mess. The way you observe stores and processes metrics data takes advantage of the unique capabilities of snowflake and is radically different from existing systems. In observe, the way you find metrics is dramatically different. When you look at a resource, we can find and surface the right metrics about that thing without any need to search. So we're trying to flip this on its head. So instead of paging through all those metrics and dashboards, it just goes straight to a resource page and will show you any metrics that are relevant. We'll leave an auto-generate dashboards based on those metrics. This metrics experience is almost disturbingly simple compared to similar products where you weigh through thousands of pointless tags and metric names
. Our perspective, I guess largely is that you don't even need those tags anyway. Alright, if you can just correlate datasets together, if you can follow these transitive relationships in the data, you can answer these super nuanced questions. One thing I'm particularly excited about is our solution to the cardinality problem. Cardinality is the number of unique combinations of metric names such as CPU utilization, memory utilization, etc. and tags, such as the application name, Kubernetes pod name, and so forth. Existing systems require users to carefully plan and watch the cardinality of their metrics data to find the right balance between cardinality, cost, and performance. Observe does a way with all of that. In observe, cardinality is largely irrelevant for cost and performance. Applications may emit as many metrics and unique tags as they please, with no need for users to play with tuning knobs such as indexes. This data can be retained and queried potentially forever at full fid
elity and with high performance. So when we were thinking about what to do with our learning feature, we wanted to make sure that we did more than just like fire and forget a learning. So at first it was some server is on fire and then it was these servers are on fire and now we need to know what services run on those servers and what customers are using those services. Learning about things or resources completely changes not only the experience of configuring alerts and monitors, but also how to make use of them when they finally trigger. The alert you want to see as a business operator is a customer is not having a good time. Oh no, because we do both time and relations, we can deliver that. We put a lot of effort into making sure there was somewhere meaningful to go when you click on our alert generated by observe. So instead of looking at random log entries or metric chart or whatever, you quickly see a list of like which users to this impact, how often has it happened in the past
, what other issues might these users be experiencing? If you have a roster of things you care about and those things relate to each other, you would be a fool not to use observe. You should use observe.
 Observability is a love letter to your future self. That's incredibly poetic by analyst terms, in sconce with an a love letter to their future client. Now, that future client, Jeremy Burton, CEO of Observe, is going to talk somehow for five minutes or less about what observability means to him. Ideally, at the end of that video, he'll be accepted to college.
 Since our launch, we've had customers using Observe to store, analyze, and alert on their open telemetry data. The feedback we received from these early adopters is a like a more prescriptive out-of-the-box solution for troubleshooting their microservice applications instrument with Open Telemetry. Today, I'm happy to announce the new Observe app for Open Telemetry, which does just that. We're firm believers in Open Telemetry's v
ision of commodity, vendor-neutral data collection, and as a result, getting Open Telemetry data into Observe is a breeze. Observe has an endpoint for ingesting OTOP data, so all you have to do is configure Open Telemetry collector to export data at it. And if you're using our Kubernetes integration, we handle all of the setup for you. Once this data is in, you can use our out-of-the-box dashboards to understand the performance of all of the services your application depends on, and even drill into individual traces. With our new waterfall visualization, it's easy to diagnose problematic traces by seeing which spans per slow or had an error. The best part is that we treat tracing data like any other data to observe, which means that you can use our schema and demand and grappling capabilities to refine and link your Open Telemetry data to any other log, metric, or resource data in Observe. So what this mainstream out-of-the-box experience is that your Open Telemetry data is linked to t
he Kubernetes, AWS, and host monitoring apps in Observe, so you can easily correlate application issues with infrastructure issues. Now, we ourselves use Open Telemetry to analyze the performance of Observe, and we're proud to now share this capability with you.
 Hi, my name is Chris Hain. I'm at the CTO's office at F5. I was thinking about a troubleshooting flow that we'd done the other day through an observed product and how simple that was compared to the exact same flow at a previous company. That flow was much more painful and I'd like to tell you a little bit about how that used to work. With Observe, you know, click, click, click, and I'm there with this prior system, it was, you know, an alert hits my phone. I know that the alert data comes from a system. I know the underlying data for that system comes from a time series database. I need to know how to query that time series database for the set of data that I want. I need to know where my, you know, dashboarding infrastructur
e lives and where all my different dashboards are that might relate to that alert. I need to know where the logging infrastructure is. I kind of narrowed the problem down to a load balancer being the issue and I wanted to look at the logs for that. I need to know where the load balancer logs live, which system that was. It's not in the Kubernetes infrastructure of logs. It's not in the application infrastructure logs. Those are entirely different systems at entirely different URLs and entirely different pieces of software in some cases. Knowing that, knowing how to query it, what I'm looking for and how to get it, it's a lot of things that I have to keep in my brain and kind of know that really just don't have a lot of portability and use anywhere in any other context in that specific job. As soon as I left that role, you know, all of that is just kind of useless information. Observe does a much better job. It's much easier to navigate around. You've got the single pane of glass. You'v
e got these resources that are interrelated and it's just a better way of operating. So that's my story.
 Let's start today with a business update. We've raised an additional $70 million for a grand total of $114 million. Now on the show today, we will not bow to any sponsor. So we won't be disclosing the name of our new investor. But with no fees or minimums and no overdraft fees, is it even a decision? What's in your wallet? We're ingesting 40 terabytes of data every single day. That's three times the amount of information in the Library of Congress. We execute 25 million snowflake queries per day, which is 1% of snowflakes daily query volume. Next month, we scan 27 trillion rows. Now, if a row is a step, we'd have a staircase that would take us all the way to Pluto. We've seen explosive growth in usage, with a 346% increase in monthly active users. We'll be going into COVID in 2020, we had just 15 employees. And now we have 88 employees. That's a 486% increase. All right, so we've b
een working closely with a big batch of new customers, or the past few months, get them up and running and observe. One of the common requests we've been getting from these new customers is to provide really more prescriptive guidance on how to configure observe to monitor and troubleshoot their applications. So towards that goal, we've been working on our out-of-the-box solutions to support more common scenarios you might encounter, and I wanted to highlight a couple of these improvements. So first, we've greatly expanded on the observed AWS integration that we introduced last year. So now, in addition to the 100 and on data sets that we provide to organize all of this data streaming out of AWS, we now provide dozens of pre-configured alerts to help you proactively monitor your services. And we've also expanded on the breadth of this integration with new support for Fargate backed ECS and EKS clusters, and much, much more. Second, we've released a new solution for monitoring plain Lin
ux servers. Our Linux host monitoring solution brings together everything we've learned about collecting data from plain servers, and includes support for scraping logs, monitoring host metrics like this space and memory usage, gathering configuration information like what packages are installed, and even collecting performance metrics about individual processes running on those servers. We provide out-of-the-box data sets, dashboards, and alerts for all this data, and we even link it with Graphlink to all the content available in our other integrations. Now, beyond these two, we're trialling new integrations for Google Cloud Platform, for Jenkins, and there's much, much more to come throughout the year. We've introduced metrics generated by machines, and with most tools, you feel like you need to be a machine to understand them. A few years ago, I watched an engineer try and build a Prometheus alert. It took them days to get it right, and the end result made little sense to other engi
neers on call. That's why I'm happy to announce our new metric-alerting experience. It's built by and for humans. We've introduced a metrics expression builder. The builder makes it easy for anyone to alert on metrics, even if they have to do more complex arithmetic. And for the power users out there, you can always dive into the opal and customize your query. Our expression builder takes advantage of Graphlink. That means you aren't restricted to filtering or grouping by tags. You can bring in any related field in our system, like a customer or a user, and do so regardless of cardinality. At every step of the way, you can look at the preview, which will help you understand how the alert would have behaved in the past. We've also revamped the alert page, so it's easier than ever to find the critical information you need when responding to an incident. When we created our dashboarding feature, we wanted to go beyond the high-level, isolated summaries of what's going on in your infrastru
cture software or business. We wanted to make dashboards a starting point for investigations and insights, not the frustrating dead ends they typically are. Because we understand the relationships in your data, the things in your business and software, when you create a dashboard that panels naturally connect to each other, and relevant content like metrics can be brought in quickly, no need to search. When you work with a dashboard, we continue to leverage this understanding of the things in the dashboard, and use their relationships to connect you to other views of that data, whether it's more detailed dashboards or worksheets ready to continue your investigation. Imagine it's 3 a.m. and you get paged. You open a dashboard and see a giant spike. It should be easy to get to the root cause from here, but it's not. The spike is actually a dead end. When we built our dashboarding feature, we did not want to build another single glass of paint. Observe on the stands relationships in your 
data, so our dashboards do too. When you see a spike, you can click on it and actually figure out what it means, whether that's a node, user, log, metric, or whatever else you have linked in the system. We keep refining our query language, OPPO, that powers all the data processing and observe, and here are a few highlights. I sometimes want to merge log lines to combine JavaScript traces into a single event, or debounds noisy sensor data. We added a powerful verb called merge events, which lets me do just that. And with Schemaan read, I can always go back and change it later and see it update my view of the past. Once I find the event I'm looking for, I can use the surrounding feature to find things that happen right around the time of that event in that context. When working with intermediate results in the console, I cannot directly type in sub queries without having to make cards in the UI. This helped us build our in-product usage reporting, which is a product feature built entirel
y as an application on the Observe platform itself. These are some examples of the 20 new verbs and features we built to improve productivity and capability, and you can read more about all of them at our newly upgraded documentation site at docs.observing.com. I'm going to show you that Observe apps are so easy to set up that even me, a six-grader can get up and running in seconds. I love Kubernetes, even though the staples that pods can't be updating parallel, it's a real shame. But hey, I'm just happy that they added an optional label selector to cube-cuddle diff. That brings up to parity with cube-cuddle apply. My friends have been complaining about my Minecraft server going down recently, so I'm going to set up Observe. And someone be happy is easy. First, you go to the Observe app browser and select Kubernetes. I can see here it's given me a token and some instructions and a cube-cuddle command to run. I'm going to fire up my terminal and copy and paste the cube-cuddle command. A
fter a few seconds, our pods are up and running, and the data will start flowing into Observe. Let's go back to Observe and see what we've got. Observe has provided a bunch of out-of-box content. I get alerts for common issues and data sets for everything you might want. Hards, containers, workloads, logos, seed visor, metrics, ingresses, and more. Now, I can get back on my Minecraft server and find out what's going on with Observe. We continuously make efficiency improvements to our backend to save our customers' time and money when using Observe. Some highlights from recent work. A 75-95% reduction in transform costs for resource data sets with semi-structured fields. That means JSON. Incremental streaming implementations of most aggregate functions that includes min, max, average, percentile, and count distinct, which are found in many monitor definitions. Streaming aggregation reduces transform costs by large factor work applicable. On top of these two big optimizations, changes to
 how we materialize monitor state and schedule monitor execution have led to an overall reduction of 25% in monitor costs across all our customers, where some customers seeing cost reductions of over 60%. And finally, a dramatic reduction of transform task queuing thanks to various improvements to our last-degree source management. Previously, a bit more than 1 in 100 transforms from tasks was queued for 60 seconds or more, which was already pretty good, whereas now it is less than 1 in 100,000 tasks. The net defect is fresher data sets and monitors running and firing more accurately than ever before. Hi, my name is Chris Hain. I'm at the CTO's office at F5. I was thinking about a troubleshooting flow that we'd done the other day through an observed product, and how simple that was compared to the exact same flow at a previous company. That flow was much more painful, and I'd like to tell you a little bit about how that used to work. With Observe, you know, click, click, click, and I'm
 there with this prior system, it was, you know, an alert hits my phone. I know that the alert data comes from a system. I know the underlying data for that system comes from a time series database. I need to know how to query that time series database for the set of data that I want. I need to know where my, you know, dashboarding infrastructure lives and where all my different dashboards are that might relate to that alert. I need to know where the logging infrastructure is. I kind of narrowed the problem down to a load balancer being the issue, and I wanted to look at the logs for that. So I needed to know where the load balancer logs live, which system that was, it's not in the Kubernetes infrastructure of logs, it's not in the application infrastructure logs. Those are entirely different systems at entirely different URLs and entirely different pieces of software in some cases. So knowing that, knowing how to query it, what I'm looking for and how to get it, it's a lot of things t
hat I have to keep in my brain and kind of know that really just don't have a lot of portability and use anywhere in any other context in that specific job. As soon as I left that role, you know, all of that is just kind of useless information. Observe does a much better job. It's much easier to navigate around. You've got the single pane of glass. You've got these resources that are interrelated. And it's just a better way of operating. So that's my story. So, I'm going to start with the first one. I'm going to start with the first one. I'm going to start with the first one. I'm going to start with the first one. I'm going to start with the first one. I'm going to start with the first one. I'm going to start with the first one. I'm going to start with the first one. I'm going to start with the first one. you
 One of the common concerns with usage-based pricing is how to keep spend within a desired budget. At Observe, we are obsessed with giving our customers full control of their tool
ing costs. I'm excited to tell you about three new features that are going to help with the same. First up, usage governor is a new feature that can help customers stay within a configurable budget. It works by tracking your average credit usage in a rolling time window. It will warn users when usage hits 80% of the configured budget or reject queries when usage exceeds the limit. The admin has the capability to define budgetary limits for both transforms and query to stay within their budget. Moving on to the next feature, Acceleration consent, which is designed for when a user queries outside of the accelerator range of a data set. Observe will now ask users for their consent before kicking off an on-demand backfill. This will make sure user is aware and they're able to use their budget where they need it the most. Next up is our usage dashboard to which we've made a few improvements. Users can now drill down to understand their credit usage by data set by monitor but package and by 
user. The dashboard will also have an overview of usage governor's behavior. This will include things like credits consumed against the configured limit analyst of bypass queries. We are confident that these features will empower our customer to be in the dry receipt and take advantage of a true usage based pricing model.
 Welcome to Observe Enablement. In today's video, we'll be providing an intro to dashboards within Observe. To introduce dashboards within Observe, we'll be going over how to use dashboards within Observe, how to create them, and then we'll go into all of the different details on how to edit dashboards. And so let's jump right into using dashboards, where first we're going to look at the basic navigation of how to find dashboards and how to look at the data within them. And then we'll look at how to use various different parameters to filter our data within the dashboard, and then we'll show how we can drill into a dashboard and really investigate our data further. So
 let's jump into a quick demo just on how to use dashboards within Observe. So now that we're here within Observe, we can see that I'm now at the Dashboards main landing page. Here on the left menu, I can select the Dashboards tab and see the long list of all dashboards within my Observe instance, which, depending on how many apps you have installed, may be extensive. You can expect to find your custom dashboards, probably further towards the bottom, with no app. Within this list, I can search for dashboards I care about, like the Kubernetes Home Dashboard, and then navigate to it by clicking into it, and see a top-level overview of all of my Kubernetes data. Note that this dashboard comes by default out of the box with the Kubernetes app. And so with the dashboard, I can see all of my different data tiles. I can scroll down to see various other tiles, and I can also notice there are different sections within my dashboard that can be collapsed or expanded. Lastly, you'll notice there's
 actually text sections within a dashboard that can help describe what's going on, or provide instructions on how to use the dashboard most effectively. And I can look to the top right corner to see additional options for using a dashboard. The first one, most importantly, is changing the time range, so I can change how far back or when I'm looking at specific data. I can change the time zone that I'm looking at. I can see the status of my query, cancel it or refresh it. All of which is very familiar to a worksheet or a data set. But up here, I can also see a Create Worksheet option, which will create a worksheet based on all of the different stages and tiles in this dashboard. I can save a link to this dashboard with or without the current filters and time to share to my other teammates. I can favorite this dashboard to come back and use later. I can save this to my home page by selecting a public folder already displayed on the whole. And I can choose again whether or not I want to r
etain the current filters or time range. And lastly, I can use this pencil icon to edit the dashboard definition. Of course, we'll take a look at that further here in a moment. Now this concludes how we might navigate a dashboard after we found it on the Dashboards tab here. But now we want to look a little bit closer into parameters within a dashboard and how we might use those to filter the data. And so there's two different types of dashboard parameters. The first one is a data set filter, which you'll always find at the top of a dashboard within this top level menu here. And when I click into a data set filter, I might see one or more different resources or event streams that I can filter to. And I could select between those and then once selected on one, I can see the relevant fields or columns that I can filter to. So in the case of this Kubernetes home dashboard, I can filter to the name of this cluster by selecting on the field and selecting equals to the certain value that I w
ant. And now my dashboard will update and show only the corresponding Kubernetes data for the cluster equal to observe demo. Now let's navigate to a different dashboard so to exemplify the other type of dashboard parameter. Here looking at the open telemetry service inspector dashboard, another one that comes out of the box, but this one from the open telemetry app, has two parameters here which are drop down parameters. Very different from data set filters, drop down parameters can be arranged within your dashboard, however you'd like. They can be based on predefined custom input values or the input values can be based upon the data itself. So I can choose different values and then see the dashboard update accordingly. Now there are different types of parameters similar to this drop down parameter. These options include numeric parameters or open text parameters. However, we'll look more into parameters in a separate enablement video. And so if you'd like to learn on how to configure 
and create parameters within a dashboard, you'll need to look to that video outside of this one. Okay, so now we've taken a moment to appreciate how we'd use a dashboard in respect to navigating it using parameters. And now we want to conclude by showing how we can drill into data within a dashboard. And so there's two main ways we can do that. We can one drill into linked data and then we can also drill into dashboard cards themselves. And so the first way by drilling into linked data, we can select any linked resource within a dashboard as noted by the green highlighted value here. And when I select on it, pun intended. I can see that there's various values here in the right menu similar to working with links in dashboards or worksheets. And then I can open this up to drill even further into this select operation and see all of the relevant data for spans that use this operation. Coming back to my open telemetry service inspector dashboard. The second way we can drill into data aside
 from links is by going into the dashboard cards themselves. And so each individual dashboard card will have a little menu here as shown by these ellipses which when clicked provide various options for helping work with this specific dashboard card. I can see all of the relevant content that is linked and used to provide this dashboard visualization. I can also copy the image so I can paste it in any presentation or slack message that needs to be sent. And then finally, this option up here at the top is one of the ones most notable because if I open in a worksheet, I'm going to then be provided my own separate worksheet which recreates this exact dashboard card in my own separate scratch pattern. And so what I can do here is I can open up the Opal Console. I can look to the visualization options here in the right menu and I can edit this by changing the data or changing the visualization to look at this data even further. And thus concludes the different ways we can drill into data by 
starting from a dashboard. Now, before we move back into our presentation to talk more about creating dashboards and editing dashboards, it's worth pointing out here that while in a worksheet, we see these visualization options here on the right menu. Visualizing data within observe is the crux of creating dashboards. And so this menu here should be very familiar whether you're configuring visualizations in a dashboard or a worksheet. Okay, and so now that we've seen how to use a dashboard, we're more motivated to understand how we would create a dashboard. And so to create a dashboard, we're going to want to look at how we can create one from scratch. We can create one from an existing dashboard and we can also create dashboards from work sheets. And so without further ado, let's jump into another demo. Here back in observe, I'm at the dashboards tab under the explore menu. Here I can see the list of dashboards, but I can also create a dashboard from scratch by clicking the new dashbo
ard button in the right hand menu. And when I do that, as you might expect, I'm brought to a blank dashboard where I can immediately start adding cards. Now we'll get into editing a dashboard in just a moment, but let's go back to our dashboards tab here. And then we can see how to create a dashboard in a different way. So in addition to creating dashboards from scratch, we can click into an existing dashboard and then click on the edit dashboard definition button in the right hand menu. And from here, we could edit this dashboard. However we'd like for the sake of a simple demo, I'll just delete a section or delete another. And then I'll notice that I actually can't save these changes because this dashboard is out of the box. What I'll have to do is save it as a new dashboard. What I could have also done is clicked on these ellipses and duplicated this dashboard and started my editing from there. And now the third way we can create a dashboard is by creating one from a worksheet. And 
so I could click into an existing worksheet that I had already used to model out some data. Here I have my scratch visualizations in which I have multiple stages, each of which visualize a different metric from my hotel data. And so if I wanted to work on this throughout the week and save my progress and then ultimately publish this to its own dashboard, all I need to do is go up in the right hand menu of any worksheet and I can click add worksheet to dashboard. And I have two options. I can select an existing dashboard or I can create a new dashboard. And then all of these cards from my worksheet that I just created, which could also include parameters as well, are added to this dashboard, which I can save. And so that was a brief look into the three different ways we can create a dashboard and get to modeling data. And now we'll really dive into the details of editing dashboards. The first thing we'll look at when editing dashboards is dashboard cards. As we saw in the various differ
ent examples, those visualization cards will see how to edit and create those. Of course, editing creating those involves working with the different visualization options. And so we'll take a dedicated moment to appreciate those. And then we'll show some of the miscellaneous objects that can be created within a dashboard, things like sections, text cards or images. Note that parameters again are covered in a separate enablement video. So let's demo one last time and this will conclude how to use a dashboard, how to create one and after we're done editing it, we'll be able to save it and share it amongst our team. So back into the observed platform one last time for this dashboard enablement video, we're going to start from this dashboard main page and create a new dashboard from scratch to show how we can edit different visualization cards and set up our dashboard. And so the first thing I'll probably want to do is title my dashboard. And then I'll see an option here to add a card. Whe
n selected, I'll see there are various different options divided into a few different groups. The first group for dashboard cards really represents the different visualizations and the raw data that you ultimately want to show on your dashboard. And so let's add one card for each type to appreciate how they differ. So the first one, a data set table is going to prompt you to select a data set within your environment. This of course can be any resource set or event stream that you'd like. In this case, I'll stick to a Kubernetes theme and look for container logs. And this exemplifies a data set table card because I can show the raw data without really any visualization at all. So now let's add the second type of card, a data set visualization. And here I'm going to select a data set again. And so to show some parity between a data set table and a data set visualization, I'm going to choose container logs again. And here a default visualization is configured. And in this case, it looks l
ike it's showing the count of container logs over time. This of course will be able to edit in just a moment, but let's conclude with the three different types of dashboard cards and select the metric visualization. And here I can then search for any of the different metrics within my environment. Container might lead me in the wrong direction here looking at AWS ECS containers. And so I'm going to look for see advisor derived metrics, which I know has the machine memory utilization. So let's select that. And now we can see represented by these three stages, three different dashboard cards. So now let's go into editing each of these cards. And here we'll get to appreciate the visualization options. So now editing my data set table card. The first thing I want to do is edit the default stage name, which represents the title of this card on my dashboard. And so I'll name this something to represent what the data is showing. And now I can change how this data is being displayed. One of th
e things I might want to use is the table controls so I can hide all of the data or columns that I don't really care about. I might just want to look at my container logs in the most relevant infrastructure that is running that. And okay, now I have just the log message and some of the basic metadata here available. If I go back to my dashboard up in the left menu, I can see this change reflected in my dashboard card along with the title. But this is a little scrunched up. So I'm probably going to want to rearrange this to extend the full length of the dashboard. And then I can also rearrange these column sizes. And this will stick after saving the dashboard. So let's go now into the visualization card and start editing that again. I want to change the title. And then really I might not need to change anything else. If anything, I can change how this data is being visualized by changing the group by statement to show the number of logs per pod. And then I can see which services are emi
tting the most data. Now here to the right, you'll notice this visualization menu very similar to what we saw on worksheets gives me the ability to change what data is being visualized what visualization type I'm using. And anytime I make a change here, I'm going to want to click apply so to reflect it in the stage. Okay, and so I'm done with this card now and I could go back to my dashboard to then select the last card and start editing it. But I want to point out that we can navigate between different cards in a dashboard by using the editing card button up here in the top center. And so here I see the two other cards that are in my dashboard, which I can select and then immediately start editing. And here I have my Kubernetes node memory utilization and metrics within observe like the one here defined in this data set come with metadata that should help create the default visualization appropriately. And so here there's really nothing I need to change. I can go back to my dashboard 
and rearrange these two cards. And that will conclude how I can then create different visualization cards and edit them within a dashboard. And so we're nearly finished with introducing how to create a dashboard. The last thing to point out for this basic introduction is the objects that can be added to a dashboard. Again, filters and parameters will be described in a separate enablement video. And so for the objects, I can click a section and then title it. And a section provides a way of grouping different visualization cards so that they can be collapsed or re expanded. One cool fact to note is that if I collapse a section and then change my query, anything hidden within a section will not run the actual query, thus improving the performance of your dashboard. And so sections can be a great way of giving the option for displaying a lot of data while not having to query all of that data if you don't need it. And so now let's add another object this time a text object, which very much
 resembles a visualization card in how it can be rearranged and sized. But once edited, we just get a simple editing menu where we can use Markdown to display whatever we'd like. And then last but not least, you can add an image to your dashboard here, clicking on the edit icon will prompt you to select an image URL. And so if you have an image hosted on any cloud site, this could be S3, a public bucket. You could use that public URL to paste it here. One pro tip for using an image within a dashboard to help bring a custom feel is instead of hosting an image online and providing that hosted URL, you can go to Google, find whatever image you want to use and copy it. And then Google a tool for converting a PNG to a data you are just about any will do. From which you can paste your image, sometimes it'll prompt you for however it needs to get it, but then it provides you this really long string, which if copied and then I can use this really long string to somehow provide an image in my d
ashboard. And this resizing behavior can be used for situations in which the height or width doesn't exactly match up. And so that can help constrain it and display my image properly. And there we go, that concludes creating a dashboard here and observe all I need to do now is save. And then when I leave the editor or navigate to this dashboard from the dashboards page, this is exactly what I'll see. So in conclusion today, we've been able to go over how we might navigate observe to find dashboards and then how we'll use them in observe. Then we went into how we can create dashboards in different ways and all of the different things we can do to edit and customize our dashboard to our liking. And so with dashboards and observe, we believe they are the start, but never the end because being able to drill into the data opens the door for not only high level overviews, but also deep dives and investigations. Thank you for joining us today with observe enablement. We hope to see you again 
to learn more.
 Hello, I'm Denise Pearson, Chief Marketing Officer at Snowflake. The Snowflake Data Drivers Awards is a global awards program recognizing innovative individuals and teams that are transforming the organizations and the world around them using data. Every year we review hundreds of nominations from Snowflake customers around the world. Finally, the winners are show-sanned by a panel of industry experts and Snowflake leaders. Over the last year we've been transforming Snowflake from being the world's leading cloud data warehouse into the world's leading cloud data platform. Our goal is to grow a large ecosystem of companies who are building businesses on top of Snowflake. To mark this shift, we've introduced a new category to the Data Drivers Award called Best Data Application. And I'm delighted to announce that Observe has been selected the winner in 2020. Observes SaaS Observability Platform runs exclusively on Snowflake and takes advantage of many cutting-edge features
. We're looking forward to working with Observe to enable our joint customers to investigate their applications and infrastructure and order of magnitude faster and cheaper than ever before. As a permanent reminder of this win, we'll be sending over this Data Drivers Award's trophy over to the Observe team. Congratulations.
 And there you have it. To my perspective, what sets observe apart is the fact that they're talking about the pain that they solve for their customers without denigrating where their customers are on their incredible journey. They're doing it without having to spend years explaining it to people. And most importantly, they're meeting customers where they are with the problems and pains they're feeling today rather than talking about the far future. I think they're launching today at ObserveInk.com. I'd encourage you to go and sign up for early access, the first 100 people who do get something a little special. But then I want you to tell me whether it's good or whet
her it's crap. I have problems in case it wasn't blindly obvious, but they don't look like most other people's problems. So I'm curious to folks who are running these things in the wild. What do you think about this? I've already been paid. I just want to know what stories you're hearing from this. Is it as good as I think it is? Let me know. I'm Cloud Economist Corey Quinn and this has been The Launch of Observe.
 The focus of today's talk is quite simply the observability cloud. For the very first time, Observe is bringing together all the capabilities you've been asking for into one single integrated product, everything from log analytics to monitoring to dashboarding, distributed tracing and much, much more. We also built Observe on top of a modern cloud-native architecture, which delivers an order of magnitude improvement in economics. We believe that there's never been a better time to introduce the observability cloud. Recent research shows that 58% of businesses moving to the c
loud have lost visibility into their operations. At the same time, they're seeing a 40% increase in data volume, pushing costs through the roof. Let's start today with our unique approach. Unlike anyone else, we ingest all event data into a data lake. Then, also, unlike anyone else, we curate that event data into a data graph. Finally, we democratize access through data apps, enabling a broad range of users to get up and running in minutes. Let's drill into the data lake. Quite simply, we ingest anything and everything that looks like an event. That's logs, metrics, traces for sure, but also zendex tickets, Salesforce customer interactions, and Jenkins build information. And we don't use proprietary agents to collect your data. We rely 100% on open source. The data lake is based on AWS S3, and we compress data 10x on average, making it insanely cheap. Next, we make the magic happen. We curate the event data into a graph of connected data sets. No one has anything like this. Simply put,
 data sets are things that users want to ask questions about. That could be containers or S3 buckets, but it could also be customers or shopping carts. And then observe relates the data sets. This means that during an investigation, users can immediately navigate to related context. This is critical for investigating unknown issues, a common occurrence in today's world of microservice based apps and continuous delivery. Because data sets represent real things, we can track how the state of those things are changing over time. This is important because modern systems are ephemeral, they're constantly changing. And so it's essential to be able to reconstitute the state of the system at any point in time. Finally, let's talk about data apps, which promise to democratize access to event data. Everyone, from customer success to support, to DevOps and SRE, to engineering, should have access. Observe provides apps for the most common infrastructure components, such as Kubernetes, AWS, GCP, an
d Azure. And observe enables you to easily create equivalent apps to observe your own distributed applications. What's in an app? The short answer is everything you need to observe your distributed application. That means dashboards alert, search, and all of your data sets. Many of these components are available in competitive offerings, but in observe, they're much, much better. For example, almost everyone has dashboards. They pull together metrics to provide an overview of the health of an application. Observe can do that too. But because observe has better data, we deliver better dashboards. Observe dashboards are driven by a data set graph, which means that when we see a spike, we can drill down and retain context as we do it. Maybe we're only showing the customers that are experienced an error in the last hour. And if we suspect it's a problem with the infrastructure, we can drill into only see the health of the pods that emitted the error. And we can keep drilling to show maybe 
only the container logs emitted by those pods. Similarly, pretty much everyone has alerts. They fire when, for example, errors are detected in container logs. And observe has got alerts too. But because observe has better data, we're able to deliver better alerts. Instead of just showing a threshold condition has been exceeded, you can now show the impact of the blast radius of perhaps which customers have been affected. Observe can do this because observe alerts are based on the data set graph. Pretty much everyone has an ability to search events. This example, you've probably seen 100 times before diving in millions of container logs to look for errors. Observe can do that too. But because observe has better data, we're able to deliver better search. We give container log search results, but we also give you relevant contextual information, such as the pod names. And for more information about those pods, like the restart count, you're just to click away. Observe can do this because 
search is based on the data set graph, and it contains all the relevant contextual information. Finally, the observability cloud is based on a modern architecture. Observe separates storage and compute, which changes the economics of observability. For ingest, you simply pay for AWS S3, and don't forget that the data is compressed 10x. And you also pay for the compute you consume when you query observe. Computers are elastic, so you only pay when you're using the system, and it's built to the near a second. Architecture matters. Now, only does observe scale, the more it scales, the more you save. In our benchmarking, observe is 10x lower cost than leading competitors at scale. This example is from a typical host monitoring environment, but this one is from a Kubernetes logging environment. As you can see, the shape of the chart is exactly the same. At scale, observe has an order of magnitude cost advantage. What we've seen and what businesses really care about is how price scales with 
data volume. Most incumbent offerings scale linearly. That means that as their data volume increases 10x, the price increases 10x. Incumbent offerings have a legacy architecture, and if you want proof of that, just look at the price list. We believe that the observability cloud is going to improve observability for you by a factor of three. Imagine lowering MTTR3x, but we're also going to give you a 10x improvement in cost, depending on your use case. That's the observability cloud, and it's available today.
 We've been doubling down on providing out-of-box use cases for customers with the observables apps. First off, we made a bunch of improvement to our existing apps. We expanded AWS app to support Amazon CloudWatch synths settings. This allows you to monitor your endpoints and API, so you can continually verify your customer experience even if you don't have any customer traffic on your applications. We also made it easier to set up host monitoring app with the auto installation scr
ipt. This eliminates the needs to manually install and configure three separate agents. I'd like to also highlight a few of our new apps. We introduce a GCP app. It comes with auto box dataset dashboard and monitor for five popular GCP services, compute engine, cloud load balancing, cloud storage, cloud SQL and cloud functions. Support for BigQuery and GKE are coming soon. If you're already using Prometheus NodeExporter for host monitoring, you can now integrate with the Observe in minutes. All you have to do is install the NodeExporter apps and configure Prometheus to include Observe as a destination. You will get dashboard for CPU, memory and network disk out-of-box. We're committed to making it easy to observe your entire stack. That's why we're continuing to invest in CI-CD and code-level integrations with Jenkins app and GitHub app. This will help our customer quickly identify which PR calls a service audit or long response time, reducing the meantime to recover dramatically. But 
wait, there's more. Introducing the Open Telemetry app, a complete solution for tracing. It provides a prescriptive ingest guidance, visualizations and dataset for tracing use case. Beyond these apps, we are working on Azure, GitLab and many more in the future. Stay tuned.
 Hello and welcome to the Observability Trends webinar series. I'm Grant Swanson, your host, and today we will explore the topic of enhancing security through observability with the Observe Basic Threat Intel app. We'll kick off with a short presentation, followed by a live product demo of our observability cloud. After the product demo, we'll jump into questions and answers. Feel free to type your questions into the questions window at any time during the webinar. The State of Security Observability 2023 report was released yesterday. The inaugural report in collaboration with Site Research surveyed 500 security decision makers and practitioners to understand their current approach to security and how it intersects 
with the observability. Everyone on the webinar will receive a copy of the report and a link to the webinar recording. Now I'd like to introduce our guest speaker, Principal Solutions Engineer Keith Buzzwell. Welcome, Keith. Hey, thanks, Grant. Really appreciate it. Excited to be here. Yeah, I'd like to just go ahead and dive right in. So enhancing security through observability with the Observe Basic Threat Intel app. So what really we're going to be discussing today is a little bit about the Observe platform and then also how I'm leveraging our Threat Intel app just to really to kind of enhance my security posture inside of my environment. So getting started is the Observe approach. What are we doing differently here inside of the platform? So we've adopted the concept of a data lake for our environment and what we've done is we partnered with Snowflake to deliver that. So a lot of benefits that wrap around that. But one of the key ones being is that we can ingest incredible amount o
f data and make it very cost effective. This is different from some of the platforms I've used in the past just because that licensing from the ingest perspective all the way to the compute perspective were very tied together and made things very expensive. But Observe has been able to decouple storage from compute and at current rates right now and I think on average by us loading data into Snowflake tables, we're accomplishing right around the ballpark if I was to average out metrics and logs and trace it to kind of cross the board right around 10X compression by loading it into Snowflake tables. So if you think about basically every 10 terabytes that you bring inside of our platform compressed down to one terabyte at current S3 costs, we're looking at I think it's about $23 a month for every one terabyte that you want to store per month. So you figure about every 10 terabytes compressing down to one, you be looking about $23 to store the data for a month. So very cheap, very cost ef
fective to be able to bring in just an extraordinary amount of data, have it all at your fingertips and being very cost effective. Now the next thing that we do is that we reach inside of this data lake and start splitting out things that we call data sets. Now these are the things that really you want to interact with on a day to day basis. And so what I mean by that is if you come take a look over here for like AWS for example, as that data comes inside of there, you're going to have things like VPC flow logs and cloud trail and cloud watch and all the things AWS inside of there. But oftentimes you're just looking maybe very specifically at cloud trail or maybe cloud watch for example, you don't really want to interact with that huge data stream that's since you know coming inside of the data lake. So what we do is we split off and make these data sets. So as you see these data sets, what I'll do is I'll maybe create something very specific to specific just for the Lambda functions o
r just the cloud watch events that are inside of there, maybe cloud trail. And then that way you can just go query that data very similar if you're bringing in like your Kubernetes clusters as well. They could be hosted in AWS or maybe they're hosted somewhere completely differently. Really doesn't matter at the end of the day. But as we start building now, we're going to build a pod data set. As a matter of fact, it goes downstream from there as well. And we start creating container data sets, container logs all the way down to maybe if you're bringing in business level information like customer data that's feeding inside of there as well. And what becomes very powerful about this is not only the fact that we're creating these data sets, which is really nice. And you can kind of think that these as just the data sources of things that maybe you've leveraged in the past and other platforms. But the other thing that we're doing is we're creating a data graph on top of there as well. And
 what I mean by that is as you start interacting with container logs, what container logs are very, there's a very close relationship to the pods that they live inside of. And the pods live inside of a cluster. And then maybe the cluster might be running on EC2s, etc. So all of those have a relation inside of there. So what we've done is that we've really kind of paired all those things up. So if there's some of those relational fields like maybe a cluster ID or some type of a link or tag that you have inside the environment, we can take that information and we can create key value pairs inside of our environment to say, okay, at any point, if you're investigating something maybe like container logs and I start seeing errors inside of my applications, I could stay right where I'm at and I could go ask the pod data, be like, hey, are you seeing any restarts happening on the pods right now? Bring that right into my container logs as a brand new column inside of there and start interactin
g with it without ever leaving what I'm doing. And we could go all the way upstream as well and start bringing in information about the EC2 instances or anything that is upstream of there that has some type of relation to it. So it's a really powerful way to add context and do investigations as well. Now the way that all this is really being done for the most part is via our data apps. So as we go inside of the platform and I'll show you around inside of there here in just a moment is that in the data apps, what this is going to be is not only you're going to see these data sets and dashboards and things like that and I, matter of fact, out of the box alerts and monitor stuff like that, but also inside of there, it contains the shipping mechanisms to get the data in as well. And we've aligned with open source collectors across the board to be able to do this. So for your AWS environment, it's largely going to be scraping the environment, pushing things out via Lambda functions. But if 
you're hosting something on prem and it's more, and that type of focus, you can really use any open source and collector that you want. We've embedded our kind of own preferences inside of our apps and you don't have to do anything because at the end of the day, a lot of my customers aren't open source collector experts, but we are. So we've embedded the scrapers. We've embedded those agents right inside these apps and all of the scripts and things like that to get these things deployed. It's embedded right inside these. So very easy to get up and running and fuel all of this content inside of here. So for some very powerful stuff. But the reason we're here today is to talk about thread intel. So inside of my environment, as we, as we dive inside of there, you'll be able to see how I'm leveraging the open source thread intel app that we've embedded. But I just want to take a moment here to discuss a little bit about open source thread intel. So essentially what is open source thread in
tel? And it's exactly what it sounds like. So open source thread intel is a collection of openly available feeds on the internet. So think of just a community of people out there. Maybe they're just, you know, openly going to collect. It could be things like domain names or IP addresses or whatever. And they're taking their knowledge and what they're learning and they're putting it out into the community for those people who can consume those and be able to leverage them. So here's some of the things that you would expect to see in open source thread intel. Things like maybe a table of IP addresses or domain names or even process hashes and things like that. So there's a lot of, you know, really cool benefits that you can get from aligning with an open source thread intel collection. One of the, let's go ahead and dive in. So basically who uses it? Really everybody, right? We're using security teams use it compliance team DevOps SREs and companies of all sizes. Even the bad guys use it
 as well. So there's no real secrets inside of here. This is all openly available data. So just kind of FYI of that as you start to maybe leverage some of these open source thread intel feeds. If you even have a program of your own or if you're leveraging the app inside of here. Except no real secrets. You know, these things are out there. But that's just one thing just to kind of keep in mind. There is a lot of good context that comes with open source thread intel, but just kind of FYI. It's out there for everybody to use. So some of the pros that you get with open source thread intel. It's free. That's a big one, right? You don't have to pay for this. So as you, you know, maybe go and grab some of these feeds and you're bringing it in. You don't have to worry too much about the cost. If you're building your own threat intelligence platform or collection, some of the things that can kind of become cumbersome is that a lot of times there's duplication. You don't meet might see an IP or
 something like that on several different lists. So consolidating them down, curating the feeds, things like that. So there is some effort that goes into that. One thing that's nice about our app is we've done all that for you. So you don't have to worry about that. And then it also provides you context into your tax service and investigations. And I'm going to show you what this looks like inside of the app. But just kind of FYI, like if I can go and take a look at thread intel coming inside of my environment. And I can see it on some of my external facing items where, you know, people might be probing my environment to see how they might get in. This gives me an idea of what does it look like? What are the, you know, what kind of ports and what things are people hitting to go explore my environment to try and get in or maybe find type of a vulnerability? This gives you really good insight into that. So maybe if I find something that's a hit on there, I might want to use that to go ma
ybe wrap additional controls and visibility around and just really could invite it providing you that insight. So I think that's all I really wanted to talk about from a context perspective and what we're going to be doing today with that. Let's go ahead and dive into the environment and take a look around. So this is the observed platform. And really where I want to start right here is going into applications because this is really where a lot of that starts from. So as we dive in, you can see I have a few apps already installed inside. If you have my host monitoring app, this is really useful from just really down to the server level type of information. I was already talking about like hosting Kubernetes on EC2, and this is for example, so if I wanted to go get all of different type of metrics and logs and even kind of the state of a machine as well, like, hey, what at this given time? What was the memory and CPU usage and, you know, who was logged on and what connections were estab
lished? All of that information could be scraped with our host monitoring app. This can be ran on Windows, machines, Linux, machines alike. So really powerful stuff and being able to get all the way down to the host level. I'm also running Kubernetes inside of my environment. Matter of fact, I am running it on just kind of like bare metal inside of a around on some VMs inside of my environment. So what you're going to see is in that data set graph that I was showing you and talking about before how those links happen. As I go and deploy the Kubernetes app, it's going to be able to be aware and says, hey, I see that you also have the host monitoring app installed. And it's going to make those links for me. So as I start looking at container logs and maybe things about pods, for example, inside of there, I'm going to be able to go all the way back to the host level and really understand what did CPU look like on the host, for example, or maybe what did disk utilization look like on there
? So you're going to be able to very quickly do those things through the data set graph and have those links already established. I'm also just bringing in like a usage report, just kind of like, hey, you know, how do I leverage the observe platform? I'm also running a security on in my environment, mainly for leveraging Zeke. So I have that kind of looking at my East West traffic inside my environment. I also have a tap on my external facing firewall as well. So you're going to see a lot of that as we start to dive in. So I have a lot of things getting crossed across my security onion. There's already some thread until embedded inside of there as well. So I'll be able to tap into that on top of the thread until app that I'm leveraging, which in order to install these apps is very, very easy. So these are all apps that I've installed already, but do install brand new app, for example, I can come up here and click on install new, click on install, and the apps done. That's really it. An
d as I did these as well, it kind of walks you through, hey, you know, how do I deploy this app and how do I get all the collectors working to scrape that data? It's all embedded inside of these. But for my thread intel app, I'm going to come up here and click manage. And what that's going to do is it takes me and shows you all the out of the box content that's included inside of this app as well. We have all of our different versions as we start to update these. But right here, the important part is connections. So you can see all the different connections that we can establish down here to different thread intel and public IP ranges from some, some infrastructure service providers. So really, this is a collection of about 17 different providers ranging from process hashes to public facing IP ranges inside of infrastructure to the actual thread intel feeds themselves. And really to kind of get started with this is that I've already deployed the app. You've seen that. So for example, l
et's say that I'm deploying infrastructure inside of Azure, US, I can come over here and click on get started, hit create, and my job is done. That's really about it. So what you're going to see happen is you come over here and you find things of interest to you. So for maybe for example, emerging threats or Kyoto, as I go and click these and I get started, you can see down here at the bottom, it says here are your existing connections. So here's all of the things that I've already established inside of my environment and my job is done very easy to leverage this and start getting these data in. And you can even see inside of here, we even have one to pull in custom thread data as well. We have your own feed inside of your environment, we can start leveraging that as well. So very, very easy to get up and running with this now interacting with this data. What you're going to see right here is if I come into our data sets now data sets, remember all those little dots that we created, ju
st kind of all the different links from the big streaming data that's coming inside of my environment. But I'm going to come up here in the top. There's going to just say package equals threat intel. And here's all of that content that's being created. You're going to get account of how many entries are in each one of these data sets inside of here, basically what's the status is healthy. So just come to the top level metrics about any one of these. But as you go and interact with any one of these, what's this go take a mega you look at emerging threats block IP list. As you open these up, you're just going to do is it's going to take you to a tab called resources. And inside of here, what you're taking a look at is all of those different IPs that have come through the emerging threats block IP list. So it's very, very, you know, just kind of streamlined and very easily to access and go interact with. We can come up here and do searches across here as well if we really wanted to. But o
ne thing that we did is if I just kind of back up here for a second, there's the Fyodor block IT, there's emerging threats. You know, there's a few threat IP list that we have inside of here. So we wanted to do was make it really easy to consume all of them at the same time. So what we've done is we've come down here and created a unified IP for threat list. So for the most part, the way that I interact with this data is I'm typically interacting with these unified lists. So if you've unified URL list, the unified IP for list and things like that. But for any reason, if you wanted to interact directly with these, you could 100% do so. Because at the end of the day, every single one of these is just become some type of a data set or materialized view in a snowflake table that we can now start linking back and forth with other things inside of here. So one of the first ways that you can start interacting with this and really kind of consuming this app is we can go to something like dashb
oards, for example, and right here, I'm just going to go sort on my thread intel and let's go to thread until IP address threat list search. So a couple of things that you can do with this, it's going to start you with just a blank screen right here. Maybe you're doing investigations, somebody gave you an IP or something like that, for example. And what you might want to do is I just, I'm just kind of curious, does this IP live on any type of thread intel list? So I'm going to come inside of here to do, we just go drop in maybe an IP, for example, and hit apply. And what that's going to do is it's going to go look at all those thread intel feeds and this IP that I just put in, let me know. Yeah, we see that IP, in matter of fact, it lives on two different thread intel lists. It lives on the Fyodor list and the emerging threat list as well. We can see that the categories related to peekabot and then for the emerging threats, it's basically on the block list there. So that's just one ver
y simple way that you're going to be able to take maybe just an IP, just gather some context about what's going on with that IP and things like that. And then very similar as well, infrastructure as a service. So if this IP did live inside of Azure or AWS and I've configured those P or GCP, for example, it's going to let me know everything down here under the IAS information tab, things about that IP and which cloud vendor that it relates to. So once again, just adding context to an investigation, is this something that lives inside of, you know, one of the public clouds out there, this one happens to live inside of my environment. So just you're really kind of helping you out along those lines. The other way that you can interact with this data set, and this is one that I actually do quite a bit, is I'm actually going to go into our log explorer right now. And so just think of log explorer as just the way that all the different things that are coming inside of my environment. You saw 
that data stream, you know, coming in and we're splitting off and creating all those data sets. All of those data sets are going to live right here inside of your log explorer so that I could start immediately going interacting with my firewall logs or my Windows logs or metrics or traces. So just a really nice way that you can quickly go interact with those. So as you see right here, I have a PF sense firewall inside of my environment. So I actually don't have any thread until configured on my PF sense. Now there is some of that that you can add inside of there. It does put weight on the firewall as well. I kind of chose to streamline it and keep it running just kind of optimal and not do any of that. But that doesn't mean that I don't want to run that against some type of thread intel. So I'm going to leverage observe to do that for me. And then so one of the things I can start taking a look at it, I see my source IPs, I see you have desk IPs inside of here. So a lot of good info. Bu
t what I want to do is I want to go add some context to these. And one of the ways that we can do that is coming up to actions and going into a worksheet. Now what a worksheet is for observe, it's a bit of a playground. This is where I can start adding in contextual information, start linking other things inside of there. I can go publish this. I can create a URL right here so I can go share this with other people and show them maybe investigative workflows that I've been doing or just any particular things of interest. So it's a really nice way that we can just start taking things to the next level. And what I want to do is I want to go see is my environment sending any data. Am I interacting externally with anything that lives on any of my thread intel lists? So what I'm going to do is I'm going to come right over here to my destination IP addresses because I see out of my source IP and I know this is my public facing IP right here. So I'm going to go take a look at this desk IP and 
I'm going to go ask it to link to another data set. So what I'm doing right now is I'm going to go interact with this data set graph over here and I'm going to go create a brand new link inside of this data set graph and go and go ask a questions and bring back some information. So let's go into the data set graph. Let's go view more. And what I'm going to do is I have a full list of all of the thread intel hits inside of here. So it's asking me right now over here on the right. It's just a saying which field do you want to use to establish that link into the data set graph. So I'm just going to say, Hey, what's going to bring back my desk IP and what does call this something like tip look up, for example, and then we'll just say apply. And what that's going to do for us is now as I scroll over, it's turned green. So I know that it's established a link into my data set graph over here and now those two things are linked inside of there. So I'm going to go ask it to do his go bring me b
ack fields of interest. So I'm going to come over here and say ad related field and I'm going to say, Oh, okay, great. I'm going to now link to my thread intelligence look up chart. And I'm just going to say anything that has maybe a source because I know all those come from some source. I'm going to say if it has a source, bring me that information back, I'm going to hit apply. And what that does is right inside my PF sense firewall logs, it created a brand new column that never existed before. And as I scroll over here, I can see I have that new look up tip source field. And this is blank. Because for the most part, the data that is going outbound for my environment doesn't live on the thread intel list. And that's very good. Right. But I am kind of curious, are we interacting with anything that might live on the thread intel list? So what I'm going to do right here is I'm going to tell it, Hey, remove any of these cells that don't have anything. So remove empty cells. Let's just go 
look take a look in the past 15 minutes. Awesome. No hits in there. Exactly what I was expecting. Let's rewind the clock a little bit. Let's just say in the past 24 hours and see if there's any type of hits. And so this is going to scan all my firewall traffic. This is essentially leading outbound. And I do. So I had two hits almost at the exact same time this morning at a 6.04. And as I start to scroll over here, I can see that there was a thread intel hit on the Kyoto list. And this was the matching IP that was on that list. So this is something that actually is pretty useful to me where I can now come over here. And I can say, let's go take this table. And I can modify this a little bit further if I wanted to. Maybe I like it just the way it is. And this is helpful. I can say right here, let's go create a brand new dashboard. Or let's go add this to an existing dashboard that we already have inside of here. So a lot of really cool things that we can do from a visualization perspecti
ve. Just by, hey, I was able to narrow down on something that's a thread intel hit. Go put it on a brand new dashboard that I just created. We can take this a little bit further. I could say, let's go visualize this. And let's go maybe say, I want to go take a look at the source IP by the desk IP. And I want to do a drive that brand new field, the source as well. And let's turn that into a bar chart and apply. And over the past 24 hours. And let's go see what something like that would look like. And awesome. This is actually what I was expecting. So right now it says I have one section. They happen. There was two hits that happened at the exact same time from my public facing IP address to that desk IP. And it was on the Kyoto hit list. And I could once again come over here and say, let's go add that to that dashboard that we just created. And just like that, I would have two brand new visualizations and a brand new dashboard off of firewall data that I see heading outbound that lives 
on thread intel lists. So just a really nice way that you can start linking some of those things together and provide some insight. But what I want to do is I want to show you how I was able to really tie this whole thing together inside of my environment, leveraging the power of observe and the way that it's able to link all the different things together. Because it doesn't really matter if it's a log of trace of metric is that it's all going into loading into our snowflake tables and we can start to create those relationships across all those materialized views is what I'm going to do is I'm going to come over to dashboards. And I'm going to go narrow down on my thread intel overview dashboard. And let's go ahead and load that up here. And so in the past seven days. So I'm going to load seven days worth of data inside of this dashboard. And what what I kind of started out doing is exactly what I was just showing you as I said, take my outbound firewall traffic coming from my pf sense
 show me if there's any hits, give me the source and then I also linked into my show didn't account as well. Because I'm always about context. I want to have additional context what I'm looking at. I can never have enough information sometimes. So basically I'm saying, okay, I'm seeing that these hits happen. These were the destination IP addresses. And if you notice inside of my firewall data, it was giving me my public IP, my public facing IP address. So in here in just a second, I'm going to show you what I'm doing with that and how I'm able to get down a little bit further into the weeds with that. But for what I did right here is I basically said, hey, if you ever see an IP right here, just go drop in the IP at the end of this query. So I can come in here and double click on this. It'll take me to show in and it'll brought drop in that IP. And now I have some additional context about where my environment is interacting with. So some nice stuff that I'll be able to leverage here fo
r additional context inside of here as well. And then over here in the middle, I basically just saying in the last seven days, how many total connections have I hit. So just kind of a nice top level metric. But this is where on the next step, I'm leveraging my ZEEK traffic or my bro traffic inside of my environment because it's taking a look at that East West traffic happening inside of my environment as well as I had to tap into my firewall. So I can take a look what's happening East West and North and South inside my environment. And I went and told it, I was like, just go grab that IP address from a destination perspective. Drop it inside of my bro logs or my ZEEK traffic or my ZEEK logs inside of here. And then go bring me back the source IP of where you're seeing that traffic come from. So now I can go take a look and me expand this out just a little bit so that 127 connection that I just saw a second ago. I went back and it's able to tell me it's like, oh, that came from 196 or 1
 and 2682.83. And so now I'm all to get me able to get back to the original source IP of where that traffic came from. Also my PSense firewall is my DHCP provider as well inside of my particular environment. Really could be anything for the rest of you. But what I did is I created a DHCP lookup table that's automatically grabbing the host information and its IP at any given time and able to go do a lookup and it's telling me not only the IP, but I have the host name as well. So I know that's my security box inside of my environment. Here's the source IP. I also have some static hosts, so my domain controller is static inside of here. So I also have that information in case I see any hits on that as well. So I was able to take something that went external had no context whatsoever. I was able to bring back that context of not only the source IP, but the host that it was launched from. And I know that I also have some Windows data inside of my environment. And one thing that I traditiona
lly maybe not been able to log so much of is like the firewall connections from the Windows firewall itself. So the event ID 51 56s because they're really noisy and they create a lot of kind of chatter out there inside the environment. But since we're so cost effective, that's one of the things I've been able to onboard now. So what I did once again as I said, okay, now I have a host and I have a desk IP, go take those values and run it across my Windows logs, bring me back my event ID 51 56s that have correlated across these and give me the process that launched the connection of where these connections are being established to. So I could see on that security box, PowerShell was being leveraged to go launch that network connection. And I can see Chrome was to use a couple times Firefox is being used and PowerShell is being used twice. So some really powerful things being able to done from just observe as a platform, but all leading all the way just from a basic thread Intel, leveragi
ng open source thread Intel, leading all the way down to the core machine that launched it and the process itself that launched a network connection. So some pretty cool stuff that we're able to do with something like this. So I created a couple sections inside of here. This one is my outbound tip connections. I'm going to go ahead and close this because that's kind of what I'm doing from an outbound perspective. Now what I'm doing from an inbound perspective is that all of this traffic is actually being blocked by my firewall. Nothing that's actually on the threat intelligence list is making it through dirt on by my last investigation anyway. But if you had something that was open to the internet, Bash and Host or things like that, that might be maybe not behind a firewall being blocked by thread Intel, you can, you know, bring some really interesting insights. And even for my environment, even though this traffic's being blocked, I made a take a look at how my environment is being im
pacted. So I can see a lot of hits being happened to 80, 80, 8443 inside of here. So people probing to my hosting web servers, am I hosting a website inside of here and things like that. So these might be some things that I'm on. If I was hosting a web server, if I was hosting a website, I might want to take additional controls on those, go maybe lock it down a little bit, maybe investigate the access logs, a little bit closer on there and the connections that are being established. So there's some really nice context where I could take a look at these things and say, hey, you know, how is it that they're trying to access my environment? How are they probing my environment? As I scroll through here a little bit, I see some 3389 inside of here. So they're looking for RDP servers from my environment, some 23 looking for open telnet. So some really nice insights on a trending basis of basically how are people trying to access my environment externally? And then basically taking a look her
e at, you know, where does this thread intel live inside of my environment? Is it a block list, malware download, so the spam house dropped traffic and then basically the top providers. So that's how I'm leveraging the thread intel app inside of my environment and really the power of being able to bring that inside of the observed platform and some of the really cool things you can do. So I hope that was helpful and now I want to leave some time for some questions. Appreciate it. Thank you. Thank you, Keith, for the compelling demo and presentation. Feel free to type your questions in everyone in the audience and I'm taking a look there or have already been some questions that have come in. The first one is, can I bring my own thread intel? Yeah, yeah, absolutely. In the thread intel app, there was a section under connections and inside of there, there was a link that says go create your own custom connection inside of there. So that's one of the ways that you can do it and you can reu
se that multiple times. So if you have other feeds like it's more than just one, you can reuse that one multiple times to create additional connections. So that's one method that you can use. Then you can always basically send it in as a data source as well. So if you have something that you're already bringing it in, maybe it's like a CSV file or something along those lines, you can always just hit it into our environment using a collector or even pushing straight to our HTTP endpoint as well. So yeah, a couple of ways to do that. Excellent. Thank you Keith. The next question I see here is, do you have a list of current providers that are included in your thread intel feed? Yeah, absolutely. Yeah, you can find that on our documentation page. We'll see if we can get a link posted to that inside of here. But yeah, it'll list essentially how to use the app, all the current thread intel providers inside of there as well. So yeah, there's a pretty good list of content in there. Excellent. 
We'll give it just another minute here to see if any other questions come in. If not, our next webinar will be scaling observability, managing petabyte volumes of data in real time. This webinar will be on January 18th, 2024 at 10 a.m. Also keep an eye out for our new state of security observability report. And if anybody has any additional questions, you can respond to that follow-up email. And this concludes our webinar and I'd like to thank everyone for joining us today. Thank you. Thanks. Thanks, Keith.
 I'm gonna show you that observed apps are so easy to set up that even neat a six-grader can get up and running in seconds I love Kubernetes even though the staples at pods can't be up to in parallel. It's a real shame But hey, I'm just happy that they added an optional label selector to cube cuddle diff That brings up to parity with cube cuddle apply My friends I've been complaining about my Minecraft server going down recently, so I'm gonna set up observe And so on the app is easy
 First you go to the observed app browser and select Kubernetes. I can see here It's given me a token and some instructions and a cube cuddle command to run I'm gonna fire up my terminal and copy and paste the cube cuddle command After a few seconds our pods are up and running and the data will start flowing into observe Let's go back to observe and see what we've got Observe has provided a bunch of out-of-box content I get alerts for common issues and data sets for everything you might want pods containers workloads logo seed visor metrics ingresses and more now I can get back on my Minecraft server and find out what's going on with observe
 Hi, my name is Ethan Lilly. I'm a senior engineering manager here at Topgolf And I've been here for about four and a half years and I love it here It's a great place to work observability is very important to us at Topgolf Technology is is what drives the whole venue if if any one piece goes down It could affect the entire player experience from t
he games to getting into your bay to just making a reservation online So we have to make sure everything is up and running at all times So before we used to have to use multiple tools I used to have three monitors up in my desk with different tools open all at the same time trying to correlate This time stamp over here with this time stamp over here But with observe it's all in one place and you can see it all in one view And you don't have to jump around different monitors You can just easily click through the UI and find exactly what you're looking for Observe you knew right away that this this tool was different this tool is very helpful and giving us the data We need and correlating that data for us so observe as much easier to use another tooling And it definitely gets us up and running faster and keeps us that way One of our favorite games at Topgolf is Angry Birds Players will hit their ball and we'll use camera systems to track the ball flight And then overlay that onto the gam
e so that it tracks your bird flies exactly the same path as your ball We had an issue at one of our venues where some of the balls were not getting traced properly So using observe we were able to correlate the game log data with our hardware metrics and figure out that there was a Misconfiguration on the back end infrastructure After fixing it players were able to once again hit the balls and let the birds fly At Topgolf we continue to add new venues every year each new venue comes with a whole new set of data With observe we're able to keep our data for up to a year which allows us to monitor that data over an extended period of time And show us different trends that may be appearing that we may not have noticed before Before we used to have to pay per user for people to access our monitoring data With observe we can have as many users as we want Allowing us to give access to everyone that we would like to see our data and clean our support teams our development teams And anybody el
se that we think might be useful. We needed a solution that would scale with us and observe is that solution
 The focus of today's talk is quite simply the observability cloud. For the very first time, observe is bringing together all the capabilities you've been asking for into one single integrated product. Everything from log analytics to monitoring to dashboarding, distributed tracing and much, much more. We also build observe on top of a modern cloud native architecture, which delivers an order of magnitude improvement in economics. We believe that there's never been a better time to introduce the observability cloud. Recent research shows that 58% of businesses moving to the cloud have lost visibility into their operations. At the same time, they're seeing a 40% increase in data volume, pushing costs through the roof. Let's start today with our unique approach. Like anyone else, we ingest all event data into a data lake. Then also, unlike anyone else, we curate that event data in
to a data graph. Finally, we democratize access through data apps, enabling a broad range of users to get up and running in minutes. Let's drill into the data lake. Quite simply, we ingest anything and everything that looks like an event. That's log, metrics, traces for sure, but also zendex tickets, salesforce customer interactions, and Jenkins build information. And we don't use proprietary agents to collect your data. We rely 100% on open source. The data lake is based on AWS S3 and we compress data 10 X on average, making it insanely cheap. Next, we make the magic happen. We curate the event data into a graph of connected data sets. No one has anything like this. Simply put, data sets are things that users want to ask questions about that could be containers or S3 buckets, but it could also be customers or shopping carts. And then observe relates the data sets. This means that during an investigation, users can immediately navigate to related context. This is critical for investiga
ting unknown issues, a common occurrence in today's world of microservice based apps and continuous delivery. Because data sets represent real things, we can track how the state of those things are changing over time. This is important because modern systems are ephemeral. They're constantly changing. And so it's essential to be able to reconstitute the state of the system at any point in time. Finally, let's talk about data apps, which promise to democratize access to event data. Everyone from customer success to support, to DevOps and SRE, to engineering should have access. Observe provides apps for the most common infrastructure components, such as Kubernetes, AWS, GCP, and Azure. And observe enables you to easily create equivalent apps to observe your own distributed applications. What's in an app? The short answer is everything you need to observe your distributed application, that means dashboards alert, search, and all of your data sets. Many of these components are available in
 competitive offerings, but in observe, they're much, much better. For example, almost everyone has dashboards. They pull together metrics to provide an overview of the health of an application. Observe can do that too. But because observe has better data, we deliver better dashboards. Observe dashboards are driven by a data set graph, which means that when we see a spike, we can drill down and retain context as we do it. Maybe we're only showing the customers that are experienced an error in the last hour. And if we suspect it's a problem with the infrastructure, we can drill into only see the health of the pods that emitted the error. And we can keep drilling to show maybe only the container logs emitted by those pods. Similarly, pretty much everyone has alerts. They fire when, for example, errors are detected in container logs. And observe has got alerts too. But because observe has better data, we're able to deliver better alerts. Instead of just showing a threshold condition has b
een exceeded, you can now show the impact of the blast radius of perhaps which customers have been affected. Observe can do this because observe alerts are based on the data set graph. Pretty much everyone has an ability to search events. This example, you've probably seen a hundred times before, diving it in millions of container logs to look for errors. Observe can do that too. But because observe has better data, we're able to deliver better search. We give container log search results, but we also give you relevant contextual information, such as the pod names. And for more information about those pods, like the restart count, you're just to click away. Observe can do this because search is based on the data set graph, and it contains all the relevant contextual information. Finally, the observability cloud is based on a modern architecture. Observe separates storage and compute, which changes the economics of observability. For ingest, you simply pay for AWS S3, and don't forget t
hat the data is compressed 10x. And you also pay for the compute you consume when you query observe. Computers are elastic, so you only pay when you're using the system, and it's built to the near a second. Architecture matters. Now, only does observe scale, the more it scales, the more you save. In our benchmarking, observe is 10x lower cost than leading competitors at scale. This example is from a typical host monitoring environment, but this one is from a Kubernetes logging environment. As you can see, the shape of the chart is exactly the same. At scale, observe has an order of magnitude cost advantage. What we've seen and what businesses really care about is how price scales with data volume. Most incumbent offerings scale linearly. That means that as their data volume increases 10x, the price increases 10x. Incumbent offerings have a legacy architecture, and if you want proof of that, just look at the price list. We believe that the observability cloud is going to improve observa
bility for you by a factor of three. Imagine lowering MTTR3X. But we're also going to give you a 10x improvement in cost depending on your use case. That's the observability cloud, and it's available today. Hello and welcome. My name is Kelsey and I work on our sales engineering team here at Observe. And today, I'm going to walk through a demo of the observability cloud. Data is ingested into Observe using open source collectors and does not require certain schemas or structures, reducing operational overhead. Here, you can see high level stats of the data you have stored. There's about 70 or so terabytes in data in total, which includes various log data, permethiast metrics, hotel traces, and even contextual data like GitHub and Jenkins events. Of the data that is an Observe, about 30% is accelerated. Unlike other tools, you can access all your raw data at any time. However, acceleration is the process of curating data into data sets to facilitate rapid optimized queries. Let's take a
 look at Observe's data set graph, where you can see all the data sets built on top of the data light and the connections between them. At a high level, you can start to see the different out of the box apps we have deployed. For example, Kubernetes and AWS. If you want an observable system that enables you to reduce troubleshooting time and increase efficiency, then you need to be able to go from point A to point B without having to know the route. I.e., you don't want your teams to have to have tribal knowledge to do investigations. Similar to a GPS. This is what Observe does. If I click on Pod, you can see all the related data sets and all related traces that went through a set of pods. Or if I want to understand the Jenkins builds related to the pods that are restarted. No matter what questions you're trying to ask, graph link or these relationships between these out of the box data sets enables you to start anywhere and navigate to various layers of your tech stack quickly and sea
mlessly without needing tribal knowledge. Let's show this in practice. Let's use the Kubernetes out of the box app as an example. Opening the app will drop us into the out of the box, Kubernetes dashboard. This is an overall view of the health of your cluster. You are able to see high-level stats, like cluster size and some other key metrics. If I scroll down here, I can see that there's some unhealthy pods. Unlike a lot of other dashboards and other tools, Observes dashboards are actionable. Meaning you can use Graph link to investigate related data sets. Let's drill down into the related pods. Now you can see all the metrics for those unhealthy pods and the outlier metrics like high CPU and memory become more apparent. Over on the top and the activity tab, by default, you can see notifications which are out of the box alerts that have fired to specific teams via different channels like Slack or PagerDuty. But let's take this farther. Since metrics only tell us part of the story, ofte
n you want to take a look at other data like related logs. You can quickly pull up other related data, not just what is provided on the dashboard. Let's look at container logs. Notice that we didn't have to filter to the same time frame or the pods we were looking at at the dashboard view. It automatically keeps that context as you look at related data. Furthermore, I could seamlessly navigate to all the traces that went through these pods. By using Graph on this dashboard, I see I have traces sorted by response time. And there are some slow ones in here. So let's pick one and take a look. On the waterfall, I can see we started this trace in NGINX, then called down to a microservice container in Kubernetes, which made some elastic cache and SQL calls, which then invoked a Lambda. Oh, we can see the problem here. That Lambda has made lots and lots of SQL calls. Pretty quickly, we have come to the conclusion that the car rating Lambda, that is the problem. We have some sort of SQL call l
oop. But what is that SQL call? Let's open one of these up. Here we have the span attributes, and I can see specific SQLs that are being called. Further down this page, I get the tabular view of all the spans, and then below that, all the logs. We have a across this environment for this specific trace. Thank you. Since our launch, we've had customers using Observe to store, analyze, and alert on their open telemetry data. The feedback we received from these early adopters is a dicomore prescriptive out of the box solution for troubleshooting the microservice applications instrument with open telemetry. Today, I'm happy to announce the new Observe app for open telemetry, which does just that. We're firm believers in open telemetry's vision of commodity vendor neutral data collection. As a result, getting open telemetry data into Observe is a breeze. Observe has an endpoint for ingesting OTOP data, so all you have to do is configure open telemetry collector to export data at it. If you'r
e using our Kubernetes integration, we handle all of the setup for you. Once this data is in, you can use our out-of-the-box dashboards to understand the performance of all of the services your application depends on, and even drill into individual traces. With our new waterfall visualization, it's easy to diagnose problematic traces by seeing which spans per slow or had an error. The best part is that we treat tracing data like any other data to observe, which means that you can use our schema and demand and grappling capabilities to refine and link your open telemetry data to any other log metric or resource data in Observe. So what this mainstream aloevox experience is that your open telemetry data is linked to the Kubernetes AWS and host monitoring apps in Observe, so you can easily correlate application issues with infrastructure issues. Now, we ourselves use open telemetry to analyze the performance of Observe, and we're proud to now share this capability with you. We've been dou
bling down on providing out-of-bots use cases for customers with the Observe apps. First off, we made a bunch of the improvement to our existing apps. We expanded AWS app to support Amazon CloudWatch synthetics. This allows you to monitor your endpoints and API, so you can continually verify your customer experience even if you don't have any customer traffic on your applications. We also made it easier to set up host monitoring apps with the auto installation script. This eliminates the needs to manually install and configure three separate agents. I'd like to also highlight a few of our new apps. We introduce a GCP app. It comes with auto bots dataset dashboard and monitor for five popular GCP services, compute engine, cloud load balancing, cloud storage, class SQL, and cloud functions. Support for BigQuery and GKE are coming soon. If you're already using Prometheus Node Explorer for host monitoring, you can now integrate with Observe in minutes. All you have to do is install the Nod
e Explorer apps and configure Prometheus to include Observe as a destination. You will get dashboard for CPU, memory, and network disk out-of-box. We're committed to making it easy to observe your entire stack. That's why we're continuing to invest in CI-CD and code-level integrations with Jenkins app and GitHub app. This will help our customer quickly identify which PR calls a service audit or long response time, reducing the meantime to recover dramatically. But wait, there's more, introducing the Open Telemetry app, a complete solution for tracing. It provides a prescriptive ingest guidance, visualizations, and dataset for tracing use case. Beyond these apps, we are working on Azure, GitLab, and many more in the future. Stay tuned. Since releasing our dashboarding and metric expression builder features in May, we've been hard at work getting feedback from our users and making improvements. Let's review a few of the major ones. First off, the metric expression builder is now availabl
e in worksheets and dashboards. That means users can visualize complex, multi-metric expressions without using query language. We've also added support for parameters in metric expressions. So users can easily make their dashboards configurable. Power users will be happy to know that the Opel for their query is always a click away in the Script tab. On the dashboarding front, we've taken a look at the things that users struggle with when creating dashboards. Textboxes and drop downs are great to put on dashboard when you know exactly the sort of filter a user might want. But sometimes you just can't know that ahead of time. To solve this, we've introduced a new type of parameter that we call a filtered dataset. The filtered dataset parameter makes every field in a dataset available for filtering. It supports many different kinds of filtering, like inclusion and exclusion, wild cards, numeric ranges, and more. This leverages all of the powerful context that observe extracts from your da
ta. Lastly, we made a big improvement to our filter bar by adding first class JSON support. Previously, users had to extract JSON fields before filtering on them. With this latest improvement, users can access and filter any JSON field on the fly. This also extends to filtering and grouping in the metric expression builder. We're really looking forward to seeing the stories our users can now tell with these exciting new features. Thank you. One of the common concerns with usage-based pricing is how to keep spend within a desired budget. At Observe, we are obsessed with giving our customers full control of their tooling costs. I'm excited to tell you about three new features that are going to help with the same. First up, usage governor is a new feature that can help customers stay within a configurable budget. It works by tracking your average credit usage in a rolling time window. It will warn users when usage hits 80% of the configured budget or reject queries when usage exceeds the 
limit. The admin has the capability to define budgetary limits for both transforms and query to stay within their budget. Moving on to the next feature, Acceleration Consent, which is designed for when a user queries outside of the accelerator range of the data set. Observe will now ask users for their consent before kicking off an on-demand backwall. This will make sure user is aware and they're able to use their budget where they needed their most. Next up is our usage dashboard to which we've made a few improvements. Users can now drill down to understand their credit usage by data set by monitor, by package and by user. The dashboard will also have an overview of usage governor's behavior. This will include things like credits consumed against the configured limit analyst of bypass queries. We are confident that these features will empower our customer to be in the dry receipt and take advantage of a true usage based pricing model. I'm Tony Noons. I'm the North American and South A
merican support director for Project 44. Project 44 is a logistics king offering a great software to compile all your data and make end-to-end visibility for your shipments. And observability is a huge part of that for us. So to take something like observability and being able to dive into data, dive into every piece of data that we have is really what our business is about. We wanted to make things simpler, clearer, easier, friendlier. Tracking shipments, I know it seems really simple from the front end. You look on the website and you find out where your bathroom is or your Wonder Woman cake pan. But you're ultimately stuck with all this data that's happening behind the scenes. Project 44 is an organization. Compiles that data normalizes it for customers, but for us looking at it and support, it's not normalizing customized. We have to kind of do that on our own. One of the biggest benefits that we see is turnaround time. And resolve time is everything in support. Being able to find 
access to the pin pointed data that you need to understand is this problem with us. Is it with a carrier? Is it with the API? Is there a communication problem? To be able to locate that quickly is the difference between a happy customer and a sad customer. One of the new rollouts of the dashboards that that observed put together really helps staff starting from square one. We're able to build dashboards that say, here are the pieces you're going to need to know about this shipment. Here's our API log, here's your server request, here's your client request, here's all the data that you need to assess the shipment for the customer. The possibilities for observer are really endless, and it's what your imagination can picture with that data.
 Welcome to Observe EnableIt. Today's video will be an introduction to Observe for anyone brand new to the product or anyone looking to get a refresher on the fundamental concepts to observe. To introduce Observe today, we'll be going over three core t
opics that cover the majority of Observe at a high level. Those three topics are data ingests, data models, and data apps, which data apps are really how we put our data to use, and that includes dashboards, monitors, and data explorers. Let's start with data ingests. And our philosophy at Observe is that data ingests should be kept easy and simple, leaving the complexity to modeling and observe and leaving our source systems in their configuration as simple as possible. We'll talk more about storage considerations in respect to data ingests in a moment. Effectively, anything that can be sent via a network request can probably be ingested by Observe. This includes plain text, JSON, CSV data, or anything compressed in a zip file can be sent using our collection endpoint. To put data ingests in perspective, we can appreciate that there really are only four different methods of data ingests that apply to all the different sources that exist out there. And those four methods are at a high 
level, cloud services, collector agents, web hooks, and polar and cloud functions. Now, to go a little bit more detail on each of these different methods of ingests, to give some perspective on how you might ingest data from one of your sources, we'll look at a couple examples of each. Beginning with cloud services, this includes the main cloud providers like AWS, Azure, or GCP. And so with AWS, the most common example is a Kinesis fire hose that can be configured to send to the Observe collection URL. However, if your infrastructure is hosted on Microsoft Azure Cloud, then the more common example for you would be in Azure Event Hub. As you can see a few examples here, which are also configured to collect data from internal Azure services and send them on to observe. The next method of ingest is collector agents. Observe supports the open source collector community, and the three most prominently used at Observe are Fluentbit, Telegraph, and the Grafana agent. You can see the Fluentbit
 agent here, which is mainly used for collecting various logs. The Telegraph agent is used for collecting various metrics, making use of open source plugins for different sources of metrics. And then last but not least, the Grafana agent is commonly used because of Prometheus being such a common way of hosting and providing metrics in different environments. These common cloud services and collector agents comprise the majority of different sources that need to ingest data, especially the core infrastructure and application data. However, there are other sources of data like SAS providers, databases, and other APIs, which have data you would like to ingest to observe. However, they require a different means of extracting that data. Webhooks are a way of using the platform itself to export data automatically to a configured endpoint. And in this case, it would be observed. We can look to source control providers like GitHub, which provides Webhooks that can be created on their platform,
 configured to send certain events, which include merge requests, GitHub action events, and more, and then send that automatically to Observe. Another common example is with ticketing systems like JIRA. JIRA, of course, has a webhook mechanism that can new events, incidents, and tickets to observe automatically. And then last but not least, we have Polars and Functions, which are our last line of defense when trying to extract data from sources that don't support a nice interface for exporting that data. And so Polars can be thought of as simpler means of just using a Git request to extract data, often times without authentication needed. But if we do require secrets that should be stored and owned by the customer, or maybe we require additional logic to interface with the database or even an API, then we would implement a cloud function that could be run in AWS or Azure, periodically, and then extracts the data and sends it to Observe. A common example is using AWS Lambda to host a fu
nction like the one you see here called Observe Collection, which extracts data from S3 buckets and then sends it to Observe. And now we've appreciated the four different ways at a high level, at least, of ingesting data to observe, and so this should give you ideas on how you could ingest data from your different sources. Now, last but not least, we want to go over some ingest considerations. The two main ones that come to mind is storage concerns and also performance concerns with the load of ingest. However, you can imagine I'm going to say that there was really nothing to be concerned about, because within Observe, one of our core tenets of our architecture is separating compute from storage. And thus, you can store massive amounts of data for very cheap, and then only have to pay for the data you actually end up using in terms of queries. And so you can see in our Observe documentation, our concept of a data lake, where it provides a place to store all of your data, you can see th
at it's compressed ten times your data once stored, and then it's also stored ultimately in Amazon S3 for 13 months by default. Resulting in an inexpensive long-term storage. You can also see in further links, like blogs about using our storage in Snowflake, to get even more details on how economical it is to store massive amounts of data in Observe. And then going back to the other consideration in terms of performance, Observe has customers who ingest data at 100 terabytes per day, in addition to the many other customers, also ingesting terabytes of data per day. We absolutely boast supporting a scale of petabytes. And so now that we've talked about data ingest, let's start getting into what we can actually do with that data, and this all has to do with modeling data, because as it's ingested, it'll be at first in its raw form, and not as usable as we would like it to be. Before getting into modeling data itself, we should take a moment to appreciate the connection between ingesting 
data and data landing in Observe, and where data lands in Observe at first is a data stream. Data streams in Observe are the entry point for any and all data, before it gets modeled into any other forms. However, once the data has been made available in the data stream, we can filter it and model it into a subsequent data set. A data set is an important vocabulary term for Observe. It is a curated view of your data. We'll see in a moment what data sets look like in Observe and how that progresses from data streams. We'll also see how to create a data set at a very high level, but we'll leave it to later enablement videos to go into deeper detail. To create a data set, it requires data modeling, taking the raw data and ultimately applying filters and transformation, to produce some result. And this produces an additional data set that is a view, downstream from our data stream, and thus inherently creates a data pipeline. But in Observe terminology, we call this data lineage. Here you c
an see a snapshot of what data lineage actually looks like in Observe. The Kubernetes data stream accepts data, raw data from all the various different Kubernetes clusters, but from there we'll filter it out into specifically just API updates. And then one step further of all those different API updates, we can filter it down and model out API updates specific to deployments. And in this way, we've modemmed our data from its raw form, created two additional data sets, and also a data lineage. So now let's go through a very quick demo to appreciate how we can go from ingest to data streams, and then into data sets, and appreciate this data lineage. We can go back to our curl example that we showed earlier, and was used to send some basic data into an Observe instance. And so doing exactly this, I can jump from my terminal in which is the source that sent this data, and then go into Observe and see how it looks. Welcome to the Observe platform. This may be your first time seeing it. And 
it's so you may want to look into some additional Observe enablement content to help navigate the overall platform. However, for now we're going to stay focused on the data streams to appreciate ingest and basic data modeling. So here we can see we're looking at the list of data streams, which we can get to from the left navigation bar. And these are the logical entry points for data being ingested to Observe. We can create these that will and segment different sources of data ingest. And so I have my data stream here that I've created, and I can go in and see the data set for this data stream. Here we can see that this is a data set. And of course, going from a data stream to a data set is a very close connection because every data stream by default gets a data set created with it. And this data set has a default set of columns or fields to help encapsulate the data that wasn't just. So here I can see the data in its raw form. I can double click on each cell to expand it further, and 
I can also interface with this data. So I can begin manipulating and modeling this data right from the interface itself just by clicking on a field and then selecting extract. Now I can have this field available in my data set, changing its schema, but this also then gives me the ability to filter based on this field, something I couldn't have done before. In addition, if I want to look through metadata and filter to a specific value, I can also do that. And so now I've applied a filter and an extraction to my data. Now I might want to change the view of the columns by looking down here in the table controls. A somewhat hidden but very important feature and observe for helping visualize data exactly as you want. Here I can hide many different fields. I could also rearrange them if I'd like. This is also the opportunity where I can change the view of my data. And also how much data is being displayed in the current query window. Of course, there's not a lot of data in my present example
, but now I can see only the data I really care about. And this represents a new view of my data. As we mentioned, a data set is a curated view of your data. And so that's exactly what I would do is I would publish this as its own data set. To do so, I wouldn't need to go in and create a worksheet. Worksheets are a temporary scratch pad within observe that expose you to all of the power of modeling data. And so already you can see glimpses of the opal, the underlying processing language that drives these data manipulations. But worksheets will be covered in more detail in the intro to modeling data. That enablement video should be linked to this one. For now, we're going to skip over this because we already used the UI to apply our filter and to extract our data. We can maybe do one more step here and instead of just hiding these fields, we could actually delete them. By right clicking, I can delete these columns. And then right here from the right menu, you can see publish new data se
t. And I'll call this my view big set. So if we navigate to our new data set that we just created, we can see the view that we had just modeled out based off of the data stream data set. And now we have this new data set that we can reference without having to recreate those same filters and data transformations. Now last but not least, we can go into this related tab here. And this is going to help us appreciate how this data set relates to others. Namely the lineage tab here can be used to appreciate the data lineage that we mentioned earlier. Now we have our data stream data set with the raw data flowing into our new data set with the new modifications. So this concludes a very brief introduction into data models and tying together the pieces of ingest, data streams and data sets. Now let's move on to data apps because so far we've been able to get data ingested into observe and then get some basic modeling to make that data more useful. But ultimately our goal is to be able to crea
te these data models so that we can create dashboards monitors and be able to investigate that data further. And so to do that very quickly and easily within observe we have the concept of observe apps. Observe apps are a predefined collection of resources to help ingest the data but then also observe that data within observe. And these observed apps correlate to the observed integrations you'll find in our public documentation. Each of these comes with the following predefined dashboards monitors and then the data can be viewed through the log explorer and the metric explorer. And so these four mechanisms here are the main ways of using your data in observe. The one other one worth mentioning which we got a very brief glimpse of is worksheets. Of course we're going to cover that in more detail in future enablement videos. But for now we'll focus in on dashboards and monitors, the log explorer and metric explorer and how they relate to the content and data ingested through observe apps
. So starting with dashboards and monitors those are going to be tightly coupled to your observe integration or app. Let's go back into observe and appreciate observe apps and how they relate to dashboards monitors. Now back into the observe platform you can see right underneath data streams the apps 10 and this displays all of the apps installed in your environment. You can see a link to install more apps you can configure apps or you can view the content related to this. And so just as I mentioned an app or an integration like in this case for AWS comes with predefined data sets to model all of the raw data coming in for you. And you can see with the extent of AWS services and different types of data that could be ingested there is a large number of different data sets. We'll take a look at how we'll actually want to find data by using the log explorer here at just a moment. But for reference you can see these are all of the underlying data sets which you could use to look at specifi
c views of data. In addition we have monitors which underneath the templates tab for each individual app in this case the AWS app will provide template monitors that you can enable to cater to various different AWS monitoring use cases. And then last but not least we have dashboards from which you can see under the AWS package are various dashboards predefined to help monitor and observe the different AWS data being ingested. And this gives a clear idea of what is actually generated when an app is installed and observed. However to conclude today's introduction we can't leave without mentioning the log explorer and the metric explorer which will be your two main vehicles for investigating data from an integration or any other custom source. So we'll go into these next we'll go back into the observed platform. And then we can find the log explorer right under the investigate section in this logs tab. There will certainly be additional enabling content to help go into all of the differen
t features offered by the log explorer and also the metric explorer for today's video will give a very brief introduction but will leave it to those subsequent videos for more detail. So here in the log explorer we can see the actual logs themselves which by clicking on different cells we can interact with. Up above we have a filter bar which can be used to filter these logs or we could use the filter bar here to the left menu. And then if I want to switch between different sources of logs here looking at the app logs I can just click on a different logs data set and then quickly switch between I can also use this option here to visualize this data on the fly. Very similar to this view is the metric explorer which in a similar way allows me to select a metric I could search for a CPU metric and maybe I only want to look at CPU metrics related to my EC2 instances. And so now I can quickly find that metric and then work with it here in the metric visualize here I can use the filter bar t
o narrow my results. I can use this bar here to configure and change how my data is being aggregated or grouped by. And then lastly I can configure different visualization options here. And that concludes our introduction to observe today we went over data ingest all the different sources and ways we can ingest data to observe. And then we went over the basics of modeling data and observe to appreciate how we go from a raw ingested form into something that can be made more usable. And the way of making use of that data is through data apps which provide us dashboards monitors and the metric and log explorer to investigate our data. Many of the features and concepts you saw today will be described in greater detail and subsequent observe enablement videos. Please refer to those for more information. Thank you.
 We have tens of billions of dollars spent on machine analytics, but the problem is that the systems that were built were all built decades ago, you know, a decade or more ago. An
d they were built for a different world. And this was one of those things we had butterflies in our stomach. We weren't sure if we could actually build this thing. You talked to customers and they would tell you, you know, yeah, we have an APM tool, we have a log management tool, we have an alerting tool, maybe in some cases they have three or four of these things. And nevertheless, they still had no idea what was going on in their environment. So one of the common problems with lots of these tools is that they give you no structure. There's nothing to ask about. There's just a giant soup of data. But what people really want to do is they want to ask questions about things. So things can be users, they can be sessions, they can be hosts. There are lots and lots of things in any business data. That's what you want to ask questions about. So you open up one of these products and as a new user, all you see in front of you is like a blank search bar. And it's sort of like the system tellin
g you, it's like you figured out. These dashboards are all pointless. They're utterly useless. Two weeks into using them, users realize that the dashboards aren't telling them anything they need to know. They're generic and pointless. You know, universally, log management was seen as a very expensive proposition. You pay an awful lot of money just for the privilege of storing your log data. People just keep glomming tags onto data and you get this mass of tag soup. It's a complete mess. We wanted to provide an experience that was as flexible and general as a log management tool, but had strongly opinionated workflows like an APM tool. One of the things I wanted to solve is sort of like, let's bring it all together. Let's have one system that does it all and not a Frankenstein's monster where we're globbing different systems together. Observe stores all of the raw data with all of the timestamps when we saw it and also optionally timestamps that you put in when you saw the data. And bas
ed on this, we build up a model of what happened over time, what happened when to whom. It's a little bit like a crime scene investigation, right? At the end of the day, like, I don't really care about the logs per se. I don't really care about the metrics per se. What I care about is the user or the service or the thing that this telemetry tells a story about. Observe needs to figure out how to magically turn that raw stream of event data, which we call observations into these higher level resources and historic state of these resources. Now Observe actually has resources. We can actually do some really magical things with them. So one of these things is landing pages. So we can look at a resource. We can look at all of the data in a resource and we can reflect it and build dashboards automatically in a landing page. Time is really front and center if you're dealing with machine-borne data. The first technical challenge was, well, we have this theoretical underpinning of the time dime
nsion and the relational dimension. How the hell are we going to build something that users can understand on top of this? Algebra that there's only, you know, like five academic papers on. The challenge with time is that users, it's not just good enough to tell users what is the state of the system right now. People don't just want to see the latest state. They need to see the history of their system so they can ask questions such as, you know, what was the state last night when things went down? What was happening? It's not actually only solving this or two-dimensional relations and times. It's figuring out how do we present this to me, the user, in a way that I can easily use and get on with my life and solve the problem, get a very short mean time to clue rather than getting lost in all the technical details. So one of the things that resources and these connection between resources give us is the ability to produce of these magic moments in the UI. So one of these is the portal fe
ature that we implemented a few months ago. This allows you to start with one resource and get to another resource without knowing how the magic happens. From an end user perspective, it's phenomenal. Getting those kinds of insights out of data that you already have but have underutilized, that's an enormous potential of observe that once your systems put their data into observe, yes, this SRE is going to be the first person who uses Observer to figure out what's going on and how can we get better up time and how can we fast deliver our features. But the user product managers and the marketing team are going to be fast followers and build totally different applications that we haven't seen yet on top of our platform. When you do things that are really profound, if you knew how hard they were really at the start, you wouldn't do them. And so we're now proud to show the world something that they're going to be shocked by.
 This is a big day for Observe and our customers. This is the larg
est and most ambitious release to date. We're also announcing today our latest funding, $50 million in Series A3, led by Sutter Hill Ventures. This money will be predominantly used to expand our sales team. We've already hit our sales plan for this fiscal year and are seeing our growth accelerate. That growth is showing up in our usage statistics. Our platform is now ingesting almost 200 terabytes a day and we're issuing almost 55 million snowflake queries per day, representing over 1.5% of their daily query volume. Hubble introduces a 100% new user interface. Dozens of feedback sessions and months of usability testing have led us to this point. We've always had incredible power in our platform and with this release we've made sure that power users right the way down to the most junior users can be productive. Productivity is not just about our new user interface. We fully embraced generative AI in all of our troubleshooting workflows. With Hubble we don't just have a chatbot that can 
explain error messages and how to perform common tasks. It can write regular expressions. It can even act as an assistant during incidents. Reading alerts, navigating to relevant logs and metrics and summarizing current status, conclusions and timelines. This is just the start. We've scratched the surface of what can be achieved with generative AI and believe that rich and structured nature of our metadata gives us a competitive advantage. Now speaking of competitive advantage, let's talk about our platform. Hubble moves as beyond the magical one petabyte a day barrier in a single workspace. No matter what the size of the organization, observe can handle it. At the same time we've been working to reduce latency, introducing a new live capability in our explorers which allow the user to query data just 15 to 20 seconds after it was created. Finally, observe is opening up. We've introduced a new public API so that our platform can be accessed programmatically. We have a new command line 
interface and we can share data in and out using snowflake data sharing. With Hubble, observe has never been as productive, as able to ingest as much data, query in his little time and as easy to share data with other users and applications. It's our biggest and best release ever and it's available for you to try in our new free trial at Observing.com.
 Metrics are generated by machines, and with most tools, you feel like you need to be a machine to understand them. A few years ago, I watched an engineer try and build a Prometheus alert. It took them days to get it right, and the end result made little sense to other engineers on call. That's why I'm happy to announce our new metric-alerting experience. It's built by and for humans. We've introduced a Metrics expression builder. The builder makes it easy for anyone to alert on Metrics, even if they have to do more complex arithmetic. And for the power users out there, you can always dive into the opal and customize your query. Our expr
ession builder takes advantage of Graphlink. That means you aren't restricted to filtering or grouping by tags. You can bring in any related field in our system, like a customer or a user, and do so regardless of cardinality. At every step of the way, you can look at the preview, which will help you understand how the alert would have behaved in the past. We've also revamped the alert page, so it's easy to never define the critical information you need when responding to an incident.
 Hello and welcome to the Observability Trends webinar series. I'm Grant Swanson, your host, and today we will explore the topic of automated data correlation for root cause analysis with GraphLink. We will start with a short overview presentation that highlights this unique feature, followed by a live product demo. And on the webinar we'll receive an email with a link to the webinar recording. For those interested, we will be conducting a DevOps and SRE observability workshop on May 9th where attendees ca
n get hands-on technical training on how to use log explorer, trace explorer, and metric explorer to troubleshoot real-life scenarios. Now I'd like to introduce our guest speaker, Solutions Engineer, Chris Milton. Welcome, Chris. Thank you, Grant. Hi, everyone. Welcome to today's webinar. I'm excited to have you join us as we explore fascinating parallels between human cognitive processes and cutting-edge IT observability technology. Today, we'll delve into how our brains manage and interpret vast amounts of data to make decisions, such as Windows, Savely Cross, and Busy Street, and how these principles are mirrored and observes innovative platform with a special focus on its powerful grappling technology. Let's first talk about the intricacies of human data processing. Our brains are continuously overwhelmed with sensory information, and within mere milliseconds, our brains integrate this data, enabling us to comprehend and efficiently navigate our surroundings. This remarkable abilit
y to swiftly and precisely connect diverse pieces of data is crucial for our safety and enhances our interactions with the world. Building on our understanding of human data processing, let's explore how these cognitive mechanisms find their counterpart in the realm of IT observability. Within an observed platform, grappling plays a pivotal role akin to the brain's functions in our bodies. It adeptly manages the integration of diverse IT data, such as logs, metrics, and traces, allowing IT professionals to not only gain a comprehensive overview of their systems, but also to quickly troubleshoot issues and make well-informed decisions. This integration is crucial for enhancing system reliability and performance, mirroring how our brains enhance our ability to safely and effectively interact with the world. Throughout our discussion, we'll explore how leveraging advanced data correlation, like that powered by GraphLink, can significantly reduce troubleshooting times, predict potential sy
stem issues, and streamline IT operations. By the end of this webinar, you'll hopefully have a clearer understanding of how essential sophisticated data correlation is, both in human cognition and in managing modern IT infrastructure. So let's dive in and talk about how our brains process information and what lessons we can draw for enhancing IT observability with tools like GraphLink. So imagine you're about to cross a busy city street, a task that involves processing a tremendous amount of sensory information. Your eyes monitor the traffic, assessing the speed and distance of oncoming cars. Your ears pick up the distinct sounds of engines and horns, which help gauge the flow and urgency of traffic. Simultaneously, your brain integrates this visual and auditory data with spatial information, like the streets width and your walking speed, calculating whether you have enough time to cross safely. This complex data processing and decision making occur within seconds, showcasing the brain
's extraordinary capability to correlate diverse inputs for your safety and efficient navigation of the world around you. This seamless integration of information is managed through neural pathways, which correlate data from your different senses, analyze the risk and guide your actions. This process is incredibly fast and crucial for your safety. Now let's draw an interesting parallel with how observes GraphLink technology functions within an IT environment. GraphLink correlates desperate types of data from various IT sources, logs, metrics, or traces, much like how our brains correlate information from our senses. This technology integrates and visualizes this data to help IT professionals understand the state of their systems, detect anomalies, and troubleshoot with precision. To illustrate this, consider this diagram. This is our data graph view within Observe. Here, you see how different data points, representatives' nodes, are linked together much like neural connections in the b
rain. These links allow IT professionals to quickly trace the root cause of an issue across interconnected systems, similar to how you would assess multiple sensory cues to safely navigate through traffic. And just as our brain's ability to process and correlate information is vital for reacting to our environment, GraphLink's ability to correlate observability data is crucial for maintaining system health and performance. It enables rapid decision making and problem solving, which are essential in the fast-paced world of technology. In summary, by correlating data as our brain does with sensory inputs, GraphLink empowers organizations to enhance their operational intelligence, reduce downtime, and optimize performance, ensuring that their IT environments are as safe and efficient as navigating a busy city street as for us. So building an understanding of how the human brain processes complex sensory information to make decisions, let's shift our focus to do a deeper dive into GraphLin
k, which is a key technology with an Observe IT Observed Ability platform. Like the brain, GraphLink orchestrates the integration and sophisticated correlation of diverse data streams within IT environment, effectively managing and allowing for the analysis of information from multiple sources. So what is GraphLink? Well GraphLink is an advanced component of the Observe platform that functions by linking various types of IT data, such as logs, metrics, and traces. These data types are analogous to the senses in our human brain analogy, each providing critical insights into different aspects of IT system health and performance. Its core functionalities are one, data linking. GraphLink connects data points from different sources by recognizing and utilizing primary and foreign key relationships among the data entities, similar to how databases link tables. This method mirrors how our brain links related sensory inputs to form coherent interpretations of our environment. Two, temporal cor
relation. GraphLink also correlates data temporally, aligning events and metrics over time to provide a chronological narrative of system activities. This is akin to how our brain processes sequence of events to determine causality in context. Three, visualization. Through dynamic visualizations, GraphLink makes it easy to navigate complex data correlations, enhancing the clarity and speed of troubleshooting efforts. By intelligently correlating diverse data, GraphLink helps pinpoint the origins of issues rapidly. So how does it work? Well, consider a scenario where a server's error rate spikes unexpectedly. Observe and GraphLink technology employs its data correlation capabilities to link this anomaly to related changes captured in deployment logs and performance metrics that occurred around the same time. By establishing relationships based on primary and foreign keys and aligning these events in a temporal context, GraphLink provides a clear, actionable insight into the cause of the
 spike. GraphLink's ability to correlate data based on relational and temporal dimensions allows IT teams to not only respond to current issues, more effectively, but also to anticipate and mitigate future disruptions. This proactive approach is crucial for maintaining robust, efficient IT operations. In essence, GraphLink equips you with powerful data correlation capabilities to manage and secure your IT environment, much like how our brains help us navigate and interact safely with the world around us. So now let's take a look at GraphLink and Observe and Action. What we're looking at here as described before is our data set GraphView. Each one of these dots represents a data set coming into the Observe platform. This is all the data from our environment coming in. We have AWS, GCP, Kubernetes, as well as contextual data like GitHub, Jenkins, things like that. The lines that you're seeing between these actually show how these data sets correlate with one another. Now a lot of these d
ata correlations have been right out of the box, but if you have a custom application or something like that, you can create these links on your own using the same primary and foreign keys that we discussed earlier. Now let's pivot to our log explorer so we can take a look at how GraphLink might help us troubleshoot an issue. By moving over to logs explorer and we're looking at our container logs, we can see that all of our data is coming in here where we can view all our logs in a single space. I can begin filtering these logs easily on the left side, such as say name space default. I can filter up here as well by saying I want pods that start with cars or I can begin filtering in here. And like for instance, I'll say show this value only. Now as we're doing this, this is continuing to shape our logs using observe schema on demand. I can actually extract data from these log files. I'll go ahead and view this as JSON and I'll extract say level as well as message. And it pulls this righ
t into our schema. This applies historically as well as to future state of the logs. We can save these as custom application logs when we have them shaped exactly the way we want. Let's try and visualize this data. We'll go to visualize and we'll say I want to count of values of levels by container and by level. And let's go to a slightly larger data view of four hours. Let's also make this a stacked area chart. Now this view presents us with a lot of noise. We see all of our error messages or all of our log messages rather coming in here debug, error, info and all of those. So let's filter this down a little bit more by saying level contains error. Now we have a much easier view to read that pinpoints where the errors occur at what time and how many. We can easily save this to a dashboard. And I can drag this field across and size it to fit right on my dashboard like that. Now I'll go ahead and save my dashboard. We'll name it. Web it on. And I'll pivot back to my logs. Now the intere
sting thing about shaping logs with the visualization is that it continues to do so in your raw logs. So for example, now we can see we have the messages and only level error. Now where graph link comes into play in the log explorer is over here where we can see this text that's in green. Text in green means this is a join data set that they this information does not necessarily appear directly in our logs are the raw container logs that we're looking at. So for example, if I double click on this pod, this is going to give me all the information from this pod's data set view. I can see all of the metadata right here. I can even view metrics as well. The other thing I can do is I can extract fields from this pod's data set and bring those into my container logs. For instance, I'll say add related fields and I'll bring in something like pod restart count. And we can see that now over here. Now the interesting thing about bringing in pod restart count is that is generally infrastructure d
ata and these are app logs so they would normally never appear together. But through leveraging graph link technology, I can pull those all into one log view. I can also double click on this go down to our metrics and say let's look at container CPU usage in seconds, click this to open it in our metrics explorer. Notice it carries over the pod value as well as the time. And this shows us CPU spikes within that pod. Let's go ahead and add this to our dashboard as well. I'll drag this across like before. And here we can actually see something interesting in this dashboard. We're pulling together logs and metrics and we can see how some of these CPU spikes actually correlate with errors that are appear. I'll go ahead and save these changes. Now let's go ahead and pivot back to our log explorer. As we can see here, we're clearly identifying the error message and the level. So what if we wanted to turn this into a monitor that would generate alerts for us whenever we receive this error mess
age? Well, we could easily go to action and select create monitor. Now I've already created this monitor. So let's go take a look over there. I go into monitor and I go into errors for customers and logs and select view alert. We can actually see how these error messages are affecting our customers. These customers could actually represent servers, different environments or things like that. But the point is it shows us the blast radius of these errors and how it's impacting each customer. Now if I move to the logs for these particular error messages, this brings me into a worksheet. And it brings in those error messages already filtered for me while preserving the time stand. Now I could continue to build out these logs by bringing in additional fields from information we already have in here, such as the items marked in green. But instead, I'm going to leverage graph link down here to actually join this to a completely different data set to see if we can't get to the bottom of these 
error messages. So I'll click on graph link and I'll say I want to graph link from the linked pods. And I want to link those two are Jenkins builds. Now observe and the way correlation is handled with graph link allows me to pull information in about the build that was pushed specifically to these pods in this time frame. We can see that this build right here was pushed with the commit information of small change to cash code. Now just like before, I can continue to bring in more fields from our Jenkins build. For example, I can say let's add related fields and I'll bring in the GitHub URL. And that allows me to do actually something very interesting. I can double click on the URL and that's going to take me right to the GitHub commit page. We're here. We're showing what the change was made. Now that we found the cause of the error, let's turn this into an incident that I can share with my team. By going back into our worksheet here, let's rename some of these fields. I'll say build re
lated to errors. I'm going to call this error logs. And I'm going to name this worksheet incident X at Y time. And I can go ahead and save this worksheet. I could send this to somebody on my team to triage this further or I could pull this right into our dashboard as well. And now you can see that we have this information in our dashboard as well. Builds relate to errors as well as our error logs. Now that I have this information pulled into the dashboard, I can actually modify these fields as well. I can say, for instance, let's duplicate this list of builds related to errors. And in this duplicate, let's actually go in and turn this into a visualization. So now we're seeing exactly when these builds were pushed. I'll move back to our dashboard. And I will bring this up here and I will pull it over. And here we can see when the build was pushed along with the correlating it with the metrics. So we can see when these CPU spikes were occurring and when the errors in that pod occurred. S
o using GraphLink and correlating logs, metrics and other information, we can now visualize the entire incident from start to finish as well as the errors that were caused and who was affected. And I'll go ahead and save this dashboard as well. As we conclude today's webinar, let's briefly revisit our exploration of the human brain's data processing abilities and how these concepts apply to the advanced features of Observe's platform, particularly the GraphLink technology. Just as we rely on our senses to write us with real-time data about our environment, allowing us to make quick and form decisions such as when to safely cross a busy street, IT environments also require a sophisticated system to process vast amounts of diverse data. GraphLink serves as a crucial component within Observe, acting as the brain of IT observability by integrating and correlating logs, metrics and traces just as our brain integrates since re-imputs. We discussed how GraphLink links data based on primary an
d form key relationships, similar to how databases connect tables, and correlates information temporally, providing a detailed, chronological narrative of events. This mirrors the human cognitive process where timing and sequence play critical roles in how we perceive and react to the world around us. GraphLink's capabilities to intelligently correlate diverse data types significantly reduces troubleshooting times, predicts potential system issues and streamlines IT operations. In conclusion, just as our ability to process and correlate sensory data is essential for safely crossing a street, having an advanced observability platform like Observe powered by technologies such as GraphLink is indispensable in the fast-paced world of technology. That ensures IT environments are as secure and efficient as our interactions with busy streets, or daily lives, and by embracing technologies that enhance data correlation, organizations can not only maintain but also advance their operational inte
lligence, ensuring that they are prepared for whatever challenges lie ahead. Thank you for joining today's session. I hope you've left with a deeper appreciation for both the complexity of human cognition and the innovative capabilities of Observe and GraphLink in transforming IT observability. Back to you, Grant. Excellent. Thank you, Chris, for the presentation and demo. The links to our upcoming events and workshop have been posted in the chat window. Everyone who attends the workshop will be given an observability innovator certificate of completion. This concludes our session and thank you for your attention.
 In Hubble, observe scaled its core engine in data loading, query performance, and a data management to meet the enterprise requirements of today and tomorrow. First, data loading. Observe already loads hundreds of terabytes data a day for production users. To plan for their future expansion, we have added parallel inserts and time-partition tables to scale to over one petaby
te a day. With Hubble, we are also introducing live mode, which helps incidents on core immediately see the effect of a fix. We added this feature with an off-switches for each log and metric data stream. The production data freshness is now down to 20 seconds, or less. Our next update will take the number further down to below 5 seconds. Second, query performance. Hubble has added search indexes to speed up needle in haystack searches. Query set in the past took minutes. Now take a few seconds. In addition, the OPPO language have been expanded to include a new text search syntax for predicates and references to contactual data. Trend aligns and the field distribution of logs often point to the next investigation steps. We call these stats. Hubble's sampling technology makes stats queries an order of magnitude more efficient, which makes it easier to perform analytics on large data volumes. When serious incidents happen, hundreds of engineer-creel logs nonstop at the same time. Many ex
ist in two or fails this challenge. To solve this problem, Hubble supports more than 2,000 concurrent users. Furthermore, the engine dynamically utilized snowflake warehouse up to 6xl in total 512 machines in size as appropriate, third, data management, and price cloud environments are becoming more complex. In order to handle this, Hubble also adds fine-grand user access control, multi-side fail-safe, and settings to manage freshness of the data. We have seen many enterprise adopt a data lake strategy for low-cost long-term storage. We believe that observability data should be integrity part of it. To support this, Hubble enables data to be both directly loaded from parkway format files in a data lake and for enriched data to be shared back to data lake without copying. I'm very proud of the Hubble release. It is our largest and most impactful release to date and is now ready for prime time in supporting the most demanding workloads.
 Hello and welcome to this presentation on building
 a fully observable system. I'm Ross and I had a product here at Observe. In this presentation, we're going to first talk about how to form an observability strategy. What are the things you have to think about on day zero? We're going to dive briefly into Observe so that we can establish some context as we then move on to talk about how we use observability internally for troubleshooting and monitoring as well as for her whole slew of other use cases. Before I dive in, I'd like to briefly introduce myself. I am Ross Lazerwitz and I had a product here at Observe. I've been here for about two and a half years. Before this, I was working at a company called Spock. I've been working on very similar problems and I've sent most of my career really trying to make sense of machine data, whether it's been with I've been a security analyst and a security operations center, sis admin, wearing a pager, a pen tester working on front ends, and I just really like this problem. I think it's incredibl
y satisfying to take a morass of machine data that has very little to no structure and huge amounts of volume and make sense of it. So part one on forming an observability strategy. A lot of time ago in 2017, Observe was founded and in the early days of the company, we whiteboarded something very similar to this. This is an architecture diagram that we're going to revisit in a bit later. And the key things to take away here are we have a bunch of microservices that run on top of Kubernetes. We have a go back end. We talk to snowflake. We store data in Amazon S3 and Aurora. There's a Kafka queue for durable storage. So this is the kind of our forward looking architecture that we were kind of speccing out to build. And our big question was, how are we going to observe itself? What kinds of decisions do we need to make very early on to do this? And our kind of day zero strategy was really thinking about the data because it all starts with the data. So how do we admit log data? Do we struc
ture it? Is it not structured? Is it coming out of standard out? Do we have agents? For metrics, what kind of metrics format are we going to use? Where are we going to store it? Where are we going to query it? But we found ourselves falling back to reliable and well understood sources. So we ended up going with Prometheus because of its very tight support integration in communities. So we ended up going with Prometheus because of the fact that we were going to use it as a tool to create a new tool. And we started with the same tool. We started with the same tool. We found ourselves falling back to reliable and well understood sources. So we ended up going with Prometheus because of its very tight support integration in community with Kubernetes. And as we moved on to logging, similar exercise, where a go shop, so we started with G-log. And what we found was that the ghost standard logging libraries weren't great. And we wanted to do structured logging. So we rolled our own very lightwe
ight structured logging library that utilizes key value pairs. But there's tons of other great alternatives to go and do that. On the tracing side, in 2017, this was a relatively new thing. And there was no standardized library for doing this. There was Yeager and Zipkin and a bunch of very disconnected things. So we went ahead and we rolled our own library. Today, we recommend using open telemetry and we've begun work to transition over to that. And we recommend it only for tracing. We've evaluated for metrics and logs and it still seems that that isn't quite mature as the other. Best agreed solutions that we recommend. And I think the key takeaway from this kind of laser exercise looking back was that we became less opinionated over time. So early on, we were like super opinionated on what tools we would use. And this changed. For example, we wanted to do structured logging everywhere. But we found that that wasn't really tractable. So we used structured logging in some places with k
ey value pairs. Some others, Jason gets logged randomly and sometimes there's just no structured logging at all. And that's kind of okay. And as you adopt more technologies, you're ability to control your tooling really drops. So we thought we could get away with just doing Prometheus. Well, guess what? We have Amazon's. Now we have CloudWatch metrics and we have logs there as well. And we have audit data and we had, we spun up build infrastructure that we had some stuff as well we had to bring in. So, you know, this opinion really changed. And as our platform, like your own technology that we were using, our observability platform became more capable. The gravity of decision of what we'd use in a collection site, we're not as far reaching. And that's because our tool allows us to ingest all sorts of data, whether it be metric data, log data, trace data, and really stitch it together in a single place and make sense of it. So if you want to put structured logs in somewhere and you want
 to bring in CloudWatch and you want to do all the correlation, we can do that. So we don't really care that much anymore. So that's kind of how we formed our really early strategy. And I want to take a brief moment to explain what observance, so you have the context for understanding the kinds of use cases we have for our own observability. It all starts with data. So observe will take data from pretty much any data source. Could be Kubernetes, could be Amazon, could be a server, could be a phone, a browser. We don't really care as long as the thing you're sending to us has a timestamp. When possible, we try to utilize open source and commodity data collectors, you know, things like fluent bit, fluent de telegraph from ethyus, stats, the BTCO system. When we get into the cloud, like, you know, for Amazon, we've created a cloud formation template that installs a lamb that you can use, open source as well. We can pull things really any way that you want to be able to either send us data
 or have data or have us pull data, we'll go ahead and do that. And we'll take all of that data and we'll store it in observe is what we call observations, right. So like I said, it's not like we're separating out our logs to some sort of logging index and metric somewhere else. It's all being stored literally in the same place. So once we have all that data, it's, you know, centralized that searchable in one place. But when you try that troubleshoot, it's kind of like staring into the matrix, right. Historically, you know, people would go and they'd log in on one of these systems and they would go look at all these log lines, then be zillions of them, and maybe they can do filtering. But it's really difficult to troubleshoot. So what observed us is it takes all of his data and it shapes it in the things you can ask questions about. We call these data sets. So, you know, depending on your use case, like Amazon, these could be things like EC2 and Lambda, even your users navigating aroun
d in your console and cloud trail and Kubernetes. Those are pods and containers. They could be users or support tickets, really anything. And the goal is to give you as much as we possibly can out of the box for well understood technologies like infrastructure. And when we don't have some sort of out of the box data set, we provide a bunch of tooling to easily build these things so that you can capture what's custom and unique about your application. And once we have these data sets, we don't stop at just structuring them into things you can ask questions about. We actually link them together using something that we call graph link. So you can imagine we have this huge interconnected graph of all of our data that hopefully represents your environment. And we can do interesting things like, you know, if you have a bunch of incidents in Patreon and you want to navigate to the builds that were behind the image of the container and the pod that failed, observe will figure out how to go acr
oss this graph and really link everything together. Under the hood, it's just fancy joins using primary key relationships. And you know, one, now that we have that state and those relationships, we can actually replay what happened over time. So because we make it very cheap to store large amounts of data for pretty much as long as you want for the price of S3, you can go and wind back the clock and look at changes a day ago, a week ago, a month ago, a year ago, and really understand what's going on. And with this model that we have with your environment, we were able to generate user interfaces that are familiar to different personas. So SREs, Mike, Mike, want their classic dashboard that drills down into something lower level. They hand something off to engineers to get more of a worksheet experience where they can go and really navigate all of the raw data. They can reshape it on the fly because we have very flexible schemas and look at things at the lowest and highest levels. And t
hen for the customer support and success team, we're able to give a really like customer user-centric view into what's going on and really have this conversation in the tool, not over a bunch of random Slack messages. So that's observed. Let's talk about how we use observability for troubleshooting and monitoring. I'm going to return to that architecture diagram we had earlier. So once again, this is what our architecture looks like for our product. And there's really two main paths. There's the read path and the right path. The right path is in bold here. So data will come in from our open source agents from cloud formation through some whatever endpoint we have. It will hit an Amazon network load balancer, then forward on to a separate ingress controller and Kubernetes. It then goes into a Kafka instance. And we have that so that we have durable storage so that we can reliably effectively shovel data in the snowflake, which is our, you know, where we have our long term kind of data s
torage and querying. There's also a read path, which is the one underneath that. And there's two ways to enter that. You can have a user and a browser hit it. We have a UI that's built in TypeScript and react. Another entry point is also an alert action that might fire off to a third party system. And there's a separate ingress path for the read path that goes through. The UI will generate a bunch of queries in our language that we call opal. It's our observed query language. And that will then get compiled into SQL and then run and query inside of snowflake. So at a high level, that's our architecture. You know, the big important pieces here are Amazon, Kubernetes, snowflake, Kafka, as well as our UI. And we also have tracing instrumented throughout this whole process as I described earlier on in those days, zero decisions. But the question looking at this is, well, who watches all of this? Like who watches the watcher? Well, observed us. So we have something called O2. This is observ
ed on observe. And it's a completely distinct deployment of observe. It has a separate AWS account, snowflake account, Kubernetes cluster. It needs to have its own distinct failure domain. So that if something happens in Prague, we don't also bring down the environment we need to troubleshoot Prague. And our stock agents for Kubernetes and Amazon will feed data in the O2. We also have the deployments for these things, not happening simultaneously, but one fall in the others that we don't have the same change, hit these environments at the same time. And it's not just the proud environment that we watch. There's also a staging environment, as well as an engine environment, which is our bleeding edge acid test. Every single commit that happens gets deployed to edge as soon as it's ready so that we can go and test our changes. We also have a bunch of built-in infrastructure. We utilize Jenkins for our CI CD and Garrett for source code control management. And that's being monitored with fl
uently running on a Linux server, where we scrape any metric and log that we can from those services. This all goes to the O2, which interestingly enough also eats its own tail. So you can actually troubleshoot O2 inside of O2. I want to talk about the scale of the system briefly. So in terms of data, we're roughly doing 4.2 terabytes of data a day for this environment. And this is quite a bit, but it's definitely another largest observed customer, but it's definitely in the upper kind of quartile for that. We have 51 users of the system on a monthly basis. This includes our entire engineering team, our SRE team. We also have users and sales and data engineering that can come in to understand what's going on inside of their customer environments that they're servicing. Even our CEO will come in and check this. So we have a lot of people congregating in here to do their job. I spoke about data sets earlier. We have 671. You can think of each data set as representing a very discrete use 
case and observe. So we've continued to build a lot of these. And you know, different teams definitely have different kinds of views on top of that data. There's a lot of monitoring going on as well. We have 68 monitors. I'm going to show you some examples of some of those that are able to use this data. If you can observe your system and it's entire you can also monitor your system because all you're doing is just having a question pre-computed on data. You can get that for free. We have a lot of worksheets. So worksheets are effectively like interactive spreadsheets or notebooks for investigations. So we're accumulating a lot of institutional knowledge that would otherwise just be a bunch of browsers have for someone. Whenever you have an investigation or an incident or some sudden performance review, we save it as a worksheet and we can go back. And because we're keeping all this data, you know, historically, we're able to go back and kind of use these things as a system of record. 
In terms of data sources, we have a few generic data sources. We have, you know, Kubernetes. We're getting container logs, ingresses. We're getting the entire state of the Kubernetes cluster. We're getting, you know, metrics from the Prometheus and about right endpoint being funneled into the system. In Amazon, we have cloud watch logs, cloud watch metrics. We get cloud trail data. We have VPC flow data as well as all of the state changes for everything in our entire environment, whether it's RDS or Lambda or Nest3 bucket, we can represent it. And we also have some observed, some observed specific sources as well. So we have a very lightweight JavaScript library in our UI that will send every exception and every UI kind of click event that goes in. We have a bunch of open tracing spans from all of our backend services, which we use to actually build and construct more context, which we'll show you. We're big users of Snowflake. And because that's our query layer, we need to know what's
 going on in there. So Snowflake has a schema called account usage. And we're actually using the Snowflake data sharing feature to get all of the query information and warehouse information in the O2 so that we can correlate it with everything else. We have very observed specific application objects. So, you know, in the UI, you might interact with a monitor or you might be a user, you might create a data set. We're able to take all of that data from a relational database, bring it into the O2 so that we have context. So if I'm troubleshooting and I have an identifier that represents an object in the system, I can pull in all of the information about that configuration object. And lastly, you know, as I said earlier, we pull in all of our CISD information. So we have all of our Jenkins build events. We have all of our source code management events coming from Garrett. So we can see every commit that's going into the system where it's being built and the code that eventually goes on to 
run in production. So now that I have all of this data, we can do lots of stuff. I mean, the most basic stuff you can do is basic infrastructure, you know, troubleshooting and monitoring. So in the left here, you're looking at a dashboard. We've automatically generated that showing me the health of all of the containers running on a subset of pods. And I can navigate over and go look at logs and, you know, this is pretty basic stuff once you have all the data. It really comes out in the wash. We can do this for pretty much any service that we're using all over Amazon, as well as the stuff that we've built. And this is kind of what you'd expect to get in a more like monitoring tool. And that's just coming for free with all of this data. It's really cool. We have a big use case around monitoring our UI. So we have a very small JavaScript library that we've written that will send events directly to our collection endpoint. It spits out some JSON blobs. And every single thing that you do i
n the UI, whether you click on something or we change the page, or you get an error, it's being collected and sent to us in no two. And our big use case for us is really error monitoring. So when we have a fatal exception in the front end, we generate what's called a white screen of death. And these used to happen really frequently so much show that one of our engineers went and created a monitor that will fork off any of these exceptions. Into a Slack channel and we would then go and triage it. And what's really interesting is, you know, since creating this alert, we haven't had one of these exceptions in months. We used to get them all the time. So it's really important to ensure the engineers can get feedback directly on the code that they're running in production so they can know what to fix and make it better. As far as open tracing goes, I mentioned earlier that we have a custom open tracing, you know, back in that we've had to build out, but we have pretty good coverage pretty m
uch all of our microservices are instrumented and we're able to use that data not just to look at like the performance of a span or a trace and know what's slow, but actually build additional context using that data. So I spoke about kind of the barebone stuff that you get for free that you can do with observability, but I think what's really amazing is what is with all this data, the other things you can do once you can really make sense of it. So we have a lot of tracing derived data sets, meaning if we think about trace data historically, everything kind of looks the same right there's a span that had some time frame that belongs to a trace. But that's not very like generic or specific to an application right it's a big abstraction. We're able to take all of that trace data and turn them in a data set. So our graph QL requests, which is a subset of our spans, we can rip out more information and make those more interesting right we can pull out the end point we can pull in the object
s that are modifying all of our queries that are running in snowflake we can pull in the snowflake data. So we've used the trace data to derive additional data. We also have lots of PM use cases. So I'm able to go into the product and using our trace data and the event data coming out of the front end. I can understand things like how many users we have on a monthly daily or weekly basis. I can look at how much time people are spending in the product. And I can look at things like feature uptake and really you know drill in. For example, a few weeks ago, I wanted to know whether or not people were creating monitors against Kubernetes specific data. So I was able to go and observe. Find all the monitor objects that are being dumped out from the system and then look up whether or not those monitors touched Kubernetes specific data sets. And then from that I could get adoption numbers. So it's really, really useful for the product team as well. Stoflake is a really important tool for us. 
And we're able to get all of the query history warehouse information and storage use imported directly into O2 using the snowflake data sharing feature. So what you're looking at here are all the tables that we've pulled in from the snowflake and just exposed new observe, which is one of the things that we can do because we're running directly on top of snowflake. And this lets us do really interesting things like we can go look here at the performance of all the queries that are being run over this time frame and I can see the execution time those queries versus the compilation time. And what our engineers are able to do are drill in this data and ask more questions. So if I notice that hey, you know, some of those queries are spending way too much time compiling what are they. So I can drill down directly into the individual statements that were generated and look at the sequel itself. And how to figure out what's going on and look at the kind of warehouse that that sequel was run on
 in the snowflake world. Observe has application specific objects. So, you know, every app has a bunch of configuration stuff that resides in a database somewhere in observe. We dump those things out on a periodic basis. So all of the things in the UI that you see, you know, data sets, users, monitors, you know, worksheets, your single sign on. And this is represented inside of the product, which allows us to at any point in time join that data in to more typical machine data. So if I identify her for an observed user, I can pull in what that user's name is and what account they belong to and what they're doing in the system and really hop across all those different data sets. And also use it to debug data sets. So a lot of times someone might call us and say, hey, you know, something happened on my data set, we can go and look at all the changes that the user made throughout time, try to kind of narrow down the root cause. What exactly happened with any of those objects. And you know,
 we use this to do a few things like we use them to add additional context or troubleshooting, you know, if there's an error, what were the steps that we're leading up to error? What happened in the system? So some should we reach out to someone should our data engineering team look at the user that hit that thing and go find them and let them know they had a problem and go and help them fix it proactively. We also have lots of routine support scenarios, right? Someone deleted the data set, well, we can go into to and grab their configuration and restore it from that. Another one that happens quite frequently is people asking us if we're receiving their events, right? Is there issue on the collection side or is it actually being dropped at our, you know, at our endpoints at our ingress side so we can go and for any customer, you know, pull up their access logs or ingress logs and demonstrate that, hey, you know, we are getting events or no, we're hitting a bunch of, you know, 400 or 50
0 errors and then drilling the what's inside those requests that's really causing it. So I'm going to tie this together with a really fun scenario. So at some point, it customaryed out to us and said they got an error from one of their data sets and the error wasn't particularly useful, but the TLDR is their data set was not usable. So we were able to go and start with that data set because it's an object in the system and we asked observe, hey, can you figure out how to get from the user's data set that was defined down to the actual SQL queries that are being run in snowflake. And you can kind of see on the right hand side that observe figured out how to portal across all of those different things. So it said, hey, you know what, a data set belongs to a task run and the platform, which goes on in the scheduler, which is a lot of things that are going to be done. So it says, you know, run on a warehouse and then eventually in snowflake and the user troubleshooting doesn't really need 
to understand what's really going on in this flow, just that we can get there by using all the links along the way and we can flip this around too. So another scenario might be noticing at the lowest levels that a bunch of queries are having issues, we can actually then ask observe, hey, can you go from these queries and bubble back up and show us the data sets that customers are having issues so we can go and help them. And that's that bottoms up flow. It's the reverse of the portal. And CI CD. We're ingesting all of our build information so we can go under, we can understand when builds are really slow. We can trace the builds back to specific get commits as well as who reviewed and accepted those and look at all their comments, all in a single place. And this is really great when you do things like this where hey, we want to we run these a bunch of benchmark reports we can see over time whether or not any of our performance is regressing across the platform. And you know the use cas
es don't just stop there. We have lots of other use cases. For example, whenever we're filing bugs, we usually attach a worksheet that captures the information that the sign is going to need the troubleshoot. We also have web hooks that automatically open up geartickets for back and errors automatically. When we're doing post mortems, you'll frequently see links to worksheets or screenshots from the tool itself that let users really tell a story as to what happened in a timeline type of view. We always share these things in Slack, you know, when we're in some sort of, you know, word room or don't insert response. We'll go and we'll share links back and forth to add to each other's worksheets to really build up this investigation. A favorite use case of mine is actually for our SOC2 audit. So we became SOC2 type to certified last year. And one of the requirements was being able to audit all of the admin operations that were done in the system. So because we're early locking this informa
tion, we very quickly built a data set that allows us to go and review all of the changes to the system. And because we are getting all of the cloud trail information as well, we can do the exact same thing for every configuration change to everything in Amazon. So if something changed on a server, someone detached a volume, we can actually go back in time and see the specific user and whether they're logging into that point in time, who made that change. I hope that you've enjoyed this presentation. If you're having issues with reliability, the time it takes you to investigate issues and the cost of your tooling, observe maybe able to help. You feel free to check out our website at www.observeink.com and if you feel so inclined, requesting a personalized demo session. Thank you.
 AI is a beautiful thing because there's no better way to express the sentiment. I hear you have VC money and I would like some of it please. That's really where it starts and stops. Now, I've long said that a
ny keynote, webinar or presentation about a product is going to be shitty if it doesn't feature customers telling stories about what they actually use the thing for. You'll see this sometimes at various conferences. Damn world. Sorry. So, in that light, let's talk to a few pre-release observed customers about their experience with the product and why it's not a complete clown show.
 -" Осuplease." -"rete and wind." -"H Norway." -"S Zoom out to the Ms. Aquinas!" -"Paris!" -" Hi! -" Romeo. -"NEW gestoch�d. -"NEW."
 concrete you
 1.0-1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1 1.0-1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1 2.0-1.1.1.1.1.1.1.1.1
.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1
 Welcome back to DIM or deeply innovative metrics, the 1961 edition. In part one, we covered walk-a-byte scale databases and algebraically correct query languages. In this show, we'll cover many more must-haves for every Rockstar X-Full-Stack engineer. Next up today, let's cover dashboards. That is a UI or user interface. Remember, you can never have enough dashboards. If you see one you like, you just copy it and make it your own. Contrary to popular belief, dashboards are state-of-the-art, incredibly useful, and never, ever break. Boops. Boops. Next up, tagging, lots and lots of tagging. Packed asian, you're it. Ah! Jason, do you know what a DIM tag is? And last but not least, we need to cover alerts. And there's one now. Let's walk over to our alert operator, Ms. Phyllis Stein, to find
 out what's going on. Phyllis, what is that particular alert for? Is an application down? Phyllis? Is the CPU being plotted by server instead of by process? Is that pizza here yet? Oh. Looks like Phyllis has a bad case of alert fatigue. Sorry about that. We'll address that in our next show. And speaking of our next show, tuning next week when we'll take an in-depth look at logs. We'll show you how to determine the true age of any log by examining timestamps in our cross section. But that's for next time, so don't miss it.
 Take a gentlemen Jack! Taht sleeping delivery back to the station Gem footage same as the battery Daa s
 So, that's all well and good. But what does this actually mean in practice? One of the biggest problems we're having right now during this global pandemic is that you don't have that moment of frustration watching someone else drive a computer and it's not the way you would drive the computer. Here to help bring that office moment back to you is Belcha, to demonst
rate exactly how observe works.
 1.0.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5 Hello and welcome to the observability trends webinar series. I'm Grant Swanson your host and today we will explore the topic of scaling observability, managing petabyte volumes of data in real time. We will start with a short overview presentation that highlights our unique architecture. Followed by a live product demo, everybody on the webinar will receive an email with a link to the webinar recording shortly after the session. For those interested, we will be conducting a DevOps and SRE observability workshop on February 8, where attendees can get hands-on technical training on how to use log explorer, trace explorer and metrics explorer to troubleshoot real-life scenarios. We will be doing the registration page available in the c
hat window. Now I would like to introduce our guest speaker, Principal Solutions Engineer, Matt Ray. Welcome Matt. Thanks Grant. Let's go ahead and hop in here. The first thing that we want to talk about with the observability is the architecture. This is what ultimately allows us to manage these hundreds of petabytes of petabytes scale range amounts of data. While still being able to provide a good user experience to the end users that are coming in and querying and working with the data. Our approach can really be broken down into a few different parts. The first part is that we're built on a data lake. The whole idea behind this is that we can take data from any source within your environment, whether that be your cloud environment, your Kubernetes, open telemetry from a tracing perspective, as well as business-level data, things like your GitHub commits and your tickets coming in from a Zendask. We take all of this data and put it into one common data lake. From there, we store eve
rything on it, compressed and decompressed on the fly by default for 13 months. Once that data is in the data lake, we start to build out the schema and the relationships within that data and what we refer to as the data graph. What this is doing is applying schema and structure to the data that you've sent into the data lake and allowing us to link and correlate related entities within the information so that as your troubleshooting, you have the ability to automatically understand and move across the related entities and resources without needing to switch screens, switch tools, or manually try to stitch together information. The last piece to our architecture is really these data applications. All of these are out of the box content that helps to get you up and running quickly. Things like dashboards, alerts, data sets, that have a predefined understanding of common entities like AWS, Kubernetes, so on and so forth, so that you can get up and going and don't have to worry about manu
ally creating or building out this data. You can really focus on getting started quickly. From there, there's really two areas that I think are important to dig deeper into, especially when we're talking about how do we manage large scale and large volumes of data. So the first one is on the less the city side. Really, the reason that we're able to do this is because observe has utilized this data lake concept in order to separate storage and compute and be able to provide high performance of both. So if we think about the way that a traditional monitoring tool is built out, there's these concepts of hot storage and warm storage and cold storage. And then if your data is too old, it has to be dehydrated and brought back into hot storage so that you can query it. And what we see for our customers is a that creates a lot of time and effort needed in order to troubleshoot, but it also can be complex and determining how much do we retain and where do we retain it. So observe stores everyth
ing in cloud storage. It's always hot and compressed and decompressed on the fly. So there's no need to worry about what data is living within what tier. And you can always go through and query any of the data within the system. The other piece is that we've separated out the storage from the compute. And what that means is that rather than going and defining a static amount of compute based on what your average load is or what you're expected high load is. Having to worry about the performance of the system when you go to run a large query, we've actually created a multi tenant data warehouse architecture utilizing snowflake where we understand the queries that you're running and the amount of data that you're processing and then assign an amount of compute, and then compute relevant to that query and what you're processing in order to deliver high performance to you. And so what that means is that when you're using the system heavily, you get a lot of compute and high performance. An
d when you're not using the system heavily, you aren't burning money and having compute standing there running that nobody is taking advantage of. And so it ends up providing a much more economical way to manage and utilize high volumes of data without breaking the bank from from a budget perspective. And so going into that a little bit more, the way that that compute scheduling works is we have a scheduler. And it'll understand what is the query that's being around. So if you're running a bunch of small queries, say loading up a dashboard, well, we can spread a lot of small queries over a lot of small compute instances in order to load that dashboard quickly. So if you're running a large query, over say a long time period, well, then we can utilize large compute instances and split out the query across multiple instances in order to process large amounts of data in an efficient amount of time. That's really where that underlying data lake architecture helps us to be able to match the 
processing power of the system with the questions that you're trying to answer so that we can drive performance, even when you're looking over large quality data. Now the other piece that plays into the performance of the overall system and being able to manage very large amounts of data is this concept of acceleration. And what we mean by that is basically taking an understanding the data coming in so that you can query it faster on the back end. If we look at the architecture, one of the paths that we've been talking about is at how queries, what am I going to query right now, what am I looking at, and those run directly through the scheduler in order to determine what is the best amount of compute. But for transforms that are run on a consistent basis say I always want my container logs to have a certain set of fields associated with them. We can batch and transform those so that when somebody comes in and needs to troubleshoot, they're always available and quick to query. So really
 the way that we can think of this is as a path where we have some observations coming in and as we move down the path, we do these transforms or accelerations where we're materializing out the data that is relevant to us. And as we move further, further down the data becomes more specific faster and query. So we'll go into that a little bit more in the demo, but that's a high level overview of the concepts of elasticity and acceleration that are helping us to drive high levels of performance for our customers. And with that, let's go ahead and hop over to the demo. Okay, so here we are in observe and the first thing that I want to do is just pop down to the data streams. And in this environment, you can see that I have a few different things coming in here. So I have my AWS data. I have some physical hopes that I'm monitoring Kubernetes. I'm bringing in open telemetry. I also have things like my GitHub logs and Zendesk data that's all coming into this data lake. Again, without me need
ing to apply a structure or necessarily understand the format that the data is coming in before it gets here. Once we have the data coming into the data lake, we can go and look at it. And if I hop into this this Kubernetes data stream here, you'll see that the raw data that we have coming in is fairly basic, right? It's basically just a JSON blog that we're getting out of our Kubernetes logs along with some extra metadata type of information. But as we go through that process of acceleration, we start to build out the things that we care about and want to look at from a Kubernetes perspective. So if I come in zoom in here, you can see that there's a few different things that we're building out of Kubernetes, things like API updates and it's shaped container logs. And as I come into any one of these nodes along the acceleration path, I see that as we move forward, we get more and more specific with the types of resources and the types of entities that we're looking at. And so this beco
mes really powerful as we're talking about looking through large amounts of data to where I can go and query container logs to go into in a minute and have a good set of information. But then I can also understand how a container log transfers to an app log for a specific type of container. And I can start to build resources for things that I want to look at within the data. So things like my users and my manufacturers associated with the data in order to give me a point of view for troubleshooting that's defined by a resource or a thing I want to ask questions about versus just searching through raw events and raw logs with that. Let's go ahead and hop into the log explorer for those container logs that we were looking at earlier. Now the log explorer is really designed to be a first stop place for being able to go and explore your data and understand what's going on. So you can see here that I have a number of log data sets within my environment. I've chosen the Kubernetes log right 
now. I also have a number of filters that I can easily come and click through and choose a filter that updates my search bar on top. And it also changes the context of the logs that I'm looking at over here. And with these container logs, we can see that they started to be shaped into things that we care about. So we still have a log line message that's based on the log coming out of the container. But we've also started to build out an understanding of things like containers, pods and clusters that are relevant to to Kubernetes logs that we would want to look into. So with this, right, we can see that there is some relevant data, but there's maybe more that we want to do in this log. I see that there's quite a bit of information stored within the actual line here. So one of the things that I can do is I can double click and start to grab data that's relevant to me and extract them out of fields. So maybe I'll grab the span ID, the trace ID, and the level for right now. As I pull those
 fields out, I'm then able to immediately see these columns directly within my log search here. We refer to this as schema on demand. It gives me the ability to without necessarily knowing what I want to ask about as I'm creating the data set. To come and on demand to find what are the things that I want to search on? What are the things that I want to filter on? So as an example here, we just pulled out this level. One of the things I can do is I can come back and say maybe instead of 15 minutes, I want to look at four hours of performance history. So I'll come back and do four hours. And then I can come to this level and say, let's go ahead and filter this. And instead of looking at everything, I just want to see the air logs over the last four hours. And immediately search over the fields that I've just added into the log data set here. And so now I'm pulling in only the errors associated with this app server. And I didn't have to do anything extra to build or maintain a new schema 
coming in. But if I'm going to be in here looking at this app server, right, extracting these fields all the time, that might be something that I do want to come out and say accelerate, right, have this available as a data set for any user that wants to come in and utilize the system. And so from that, all I would do is come up and save the data set and publish it as a new log data set. And that's essentially what we've done here with these application logs. So you can see that we have the same message and level that we were looking at earlier, along with that stack trace that you saw. And the relevant information about things like the container, the pod cluster. And we've also pulled in some other pieces of data things like our span and trace and user session. The way we've done this is utilizing our graph link, which is essentially a sequel join across primary foreign key pairs. So now I've tied the trace that is coming out of my Kubernetes logs with the trace that's coming out of OP
EL. So I can easily make connections between those two points. And so if I come in here to say trace, I can add related fields. And maybe into this container log data set, I want to pull in things like the duration to this data set. So I can easily not just pull in information that's relevant or extracted from the logs that I'm looking at here, but actually create relationships across data sets and pull in relevant information. As I need it, as I'm investigating issues within within my environment. And then just like we did earlier, we could always come through and sort by or filter this associated information. And so that's really the power that observe offers, right, is the ability to to go through and understand and work with your data without needing to worry about having that pre applied schema or maintaining an index. You can easily leverage the power of the data warehouse in order to pull in and build on demand, what you're looking at and create links between data that tradition
ally people have had to link via engineers remembering timestamps and UU IDs as they move across different tools. And so that's really how we enable our customers to leverage and utilize large amounts of data. Now, some of the other pieces here, we can always come through and further dive into those links. So for instance, we have the screen link here on pod. If I go and click this, our graph link allows us to now see all of the data that's relevant to this pod, things like the namespace and cluster, but we can also get the IP address and the status of the overall pod. We can also hop over to the metrics and similar to the way that we were looking at the log data, we can immediately see all of the metrics that are relevant to this pod. And if I wanted to, I could jump over and move from my log explorer to my metric explorer to understand what's going on with, with the in this case garbage collection size for this pod. I can also back out and say, maybe I don't want to look at just this
 pod, but maybe I want to look at all the pods. And I can even add cardinality by coming in here and saying, let's also get an understanding of which cluster they're living in and which namespace and start to build out these really high cardinality type of use cases where we have multiple points all leveraging the data warehouse behind observe and the ability to utilize the SQL joins. And so what we see with our customers is that this becomes really impactful because now we can do things like tie metrics to individual transactions or individual customers or user sessions. There's really high cardinality type of use cases that you're not necessarily able to to obtain with other tools. So that's just a brief dive into some of the metric side of the world as well. But to wrap things up, what I want to do is actually hop back over to the log explorer and take a look at these container logs. So I know we're really supposed to be talking about scale during during this webinar. And a lot of t
he pieces that we've touched on, right, the elasticity, the acceleration, the ability to build relationships on demand, given the architecture that observe is built on, are really the underlying or underpinning things that allow us to manage these really high volumes data. But just to bring that to life, one of the things that we can do is we can come back and say, what's instead of looking at the last 15 minutes, jump back, maybe a few months and go ahead and apply that. Now we're looking at our container logs across a few billion points of data that we have loaded up in here. So one use case that I was working on with a customer recently was understanding all of the hits they've had on a particular IP address across a large period of time. And so one of the things that I can do is just come in here and say, I'm going to search the logs for anything that contains this particular IP address and port. And go ahead and run this query. And what we'll see is that within a matter of a coupl
e seconds, right, we're able to query across billions of lines and bring back millions of results, all utilizing that the power of the data warehouse that that we have running behind the scenes. And so that concludes everything that I was hoping to show today.
 We keep refining our query language, OPPO, that powers all the data processing and observe. And here are a few highlights. I sometimes want to merge loglines to combine JavaScript traces into a single event, or debounds noisy sensory data. We added a powerful verb called merge events, which lets me do just that. And with Schemaun Reed, I can always go back and change it later and see it update my view of the past. Once I find the event I'm looking for, I can use the surrounding feature to find things that happen right around the time of that event in that context. When working with intermediate results in the console, I cannot directly type in sub queries without having to make cards in the UI. This helped us build our in-produc
t usage reporting, which is a product feature built entirely as an application on the Observe platform itself. These are some examples of the 20 new verbs and features we built to improve productivity and capability. And you can read more about all of them at our newly upgraded documentation site at docs.observing.com.
 Welcome. Deeply innovative metrics. That's DIM for short. The metrics you never expected to see until at least 60 years in the future. If you're a crack-adjac technologist, then you rely on metrics on a daily basis. Any modern metrics you've got, you can't just go to the market and buy them. The tech-based metrics you've got are the key to your knowledge. You rely on metrics on a daily basis. Any modern metrics platform needs these key components. My capable technical assistant, young Jason here, will help me illustrate these for you on our giant, easy to comprehend dashboard. The DIM platform includes all the key requirements you crave. Must have for every Rockstar 10X
 FullStack Engineer. First, you'll need a dedicated, wonkybite, scale in memory database. You'll need real-time processing because the bank company's too silly. So make sure you have wonkybites of memory on hand. Remember, it's VC money you're spending so cost should never be an object. To analyze your metrics, we don't use a modern, popular language. There are nowhere near expressive enough. So we invented a simpler language just for you. DIMQL. How about that? Algebraically correct and so simple. A fifth grader can use it. Well, that's all the time we have for now. But tune into our next show when we'll talk about dashboards, complex tagging systems, and alerts, more and more DIF, deeply innovative metrics. Hello. My name is Annette Mulaney and I'm a software engineer. I know I don't look it, but that's because they paid me to shower. I'm just kidding. We're all very cool and hygienic. I'm here today to talk to you about observability. What are my qualifications? Am I a 10X Rockstar 
Ninja Engineer? God, no. No. Absolutely not. Not even a little. Honestly, I'm more of a 1X. 1.5 on a good day. 0.7 if I'm a little hungover. Work hard, play hard. Point is, I'm a remarkably average and deeply adequate engineer, like most employees, but that's exactly who needs to troubleshoot apps. Normal humans. Ideally, unfortunately, the ability to glean insight from whatever Hodgepodge at stack of various products and in-house things is usually very concentrated. I'm talking relying on one person to solve the tricky issues, which is terrifying. The power can really go to their head. I've already promised a firstborn to get help before, and that's just not a scalable solution. Plus, what if that person leaves or dies? I mean, I'm sure it'll be fine. I'm sure our stack is completely and thoroughly documented and easily grasped by whoever needs to fill in. What is observability? Other than a buzzword I throw around in meetings and attempt to sound smart, open to limitry. High cardinal
ity. Cloud native. Tail sampling. Are you even gonna put those into sentences? No. Observability is ultimately about answering simple questions. What's happening and why? Or, as we might phrase it during an incident call, what the hell is happening? It's 2 a.m. Dear God, why? In my experience, I usually try to answer these questions by desperately parsing too many logs, because I never actually got good at the proprietary query language of our service. And the logs themselves are deeply unhelpful. The tags return everything. I'm like, who set this up? Oh, I didn't. Huh. I should make that better. What if I added more tags, more logs? You know, I should probably add metrics and tracing to those logs. And you know what? More tags to those metrics. Okay, now I definitely need more dashboards and more alerts. But are there enough tags? And not to brag. But all this is costing the company a lot to ingest. Even though I don't even need most of this info anymore. But like, what am I gonna do?
 Remove it? I mean, who does that? Oh, excuse me. Don't worry, it's just another alert. I get them all the time. They'll be more of something's really going wrong. Plus, I'd have to wait for everyone to hop on the call before we can make any progress. I have time. Okay. I know it's 2AM. Let's get this over with so we can go back to bed. Okay, what's the issue? Bob has joined the meeting. Hi, Bob. Welcome. Mary has joined the meeting. Thanks for joining us, Mary. Bob, you're muted. Bob. Bob, you're muted. Bob, no one can hear you. Plus, everyone has their own tools, and so everyone has their own data. Since our tools don't speak to each other, we have to. And if there's one thing I love, it's stressed out troubleshooting. Although actually, it has been one of my more reliable forms of socializing during some of the bleaker parts of the last year. Oh, restart the note. Can't believe it and think of that. This is the third time this is happening this week and that. Crazy, right? This didn
't need to be a call. Sure, Bob. Wasn't it kind of nice, but it was? Okay. Well, just try restarting it next time before hitting me up. HOTELY. Why don't you get a dog? Bye. Bye. I also tried to fill the void by bettering myself. A year ago, when I stopped commuting and my climbing gym closed, I resolved to dive deep into Kubernetes and finally understand it instead of silently praying to myself. And within a few weeks, I had completely given up on that resolution. But others have fared better. Observe, for example, has some pretty exciting updates to share.
 The SaaS applications these days, they've gotten really complex with all the multi-tenancy on the network and infrastructure that the data traverses through and so many different applications and monitoring tools, but you know, you can scale all of that with just people. The technology standpoint, we deal with massive amount of data and we also have the challenge to deliver personalized experience in real time. We recently transit
ioned our software development efforts to use modern tools such as containers, clustering, and orchestration, which provided us with considerable operational efficiency, but that costs visibility. The root cause analysis is just spread around different log files and consoles and monitoring systems and stock traces and new name. So for us to be able to identify what's going on is the challenge that our learning systems are noisy. Most of my team could legitimately ignore 50 to 60% of their emails and that number is higher for me. Right now we have to go through multiple systems, log into multiple things. We have a dashboard separate, we have a log monitoring that is separate, and then we have an API monitoring that is separate. So you have to be able to connect the dots across all of these systems. Application logs are spread across multiple compute nodes. We rarely know what node is servicing a client when there's an error. It's really an aggregation problem. We need a system to gather
 data from all of our application containers and store it in one place, preferably in the cloud so we don't have to maintain anything on site. With the tool like observe, all of them come into one place. We get into a dashboard. My infrastructure is mapped and I can go from a trouble ticket to solving the issue in one place. Observe takes the idea of event relationships to the next level. It allows us to drill down from a big picture view into more granular data, but it also provides us visibility into related event streams. In addition to providing the standard searching and filtering tools that we've relied on for years. It really helps us with root cause analysis. We have a lot of different systems to track log files, our uptime, we have monitoring and learning system, and for us to reach the root cause analysis is just trying to match up those different sources and observe helps us really identify call it the data and put it in one place. Their whole philosophy is that the clients 
shouldn't have to write queries or regular expressions. That makes me really happy. The functionality is available, but observe would prefer that the clients didn't ever use it. Generally, we don't have a lot of time to write queries and to parse and organize log data that we only use when an application is broken. When an application is broken, we definitely don't have time to try to figure out queries or regular expressions. How well the user experience has been thought up? As a very junior support person, customer success or a product person, I can jump into the UI, look at the dashboard, see where the problem is, and I can almost get to the root cause of the problem, and being able to provide all of this good troubleshooting information to the engineer, and the engineer can solve the problem quickly. We believe that the focus that the product has on event correlation and relationships will go a long way to helping us generate useful relevant alerts. That helps us deliver better cus
tomer experience in the end.
 Welcome. Deeply innovative metrics. That's DIM for short. The metrics you never expected to see until at least a few years ago. The first time I've ever seen a new one, I've never seen a new one. I've never seen a new one. I've never seen a new one. Welcome. Deeply innovative metrics. That's DIM for short. The metrics you never expected to see until at least 60 years in the future. If you're a crack-adjac technologist, then you rely on metrics on a daily basis. Any modern metrics platform needs these key components. My capable technical assistant, Young Jason here, will help me illustrate these for you on our giant. Easy to comprehend, dashboard. The DIM platform includes all the key requirements you crave. Must have for every route you've been to. The DIM platform includes all the key requirements you crave. Must have for every rockstar 10X full stack engineer. First, you'll need a dedicated, WonkaBite scale in memory database. You'll need real-time proce
ssing because the main company's too silly. So make sure you have wonkaBites of memory on hand. Remember, it's VC money you're spending so cost should never be an object. To analyze your metrics, we don't use a modern, popular language. There are no where near, expensive enough. So we invented a simpler language just for you. DIMQL. How about that? How to break the correct end so simple? A fifth grader can use it. Well, that's all the time we have for now. But tune into our next show when we'll talk about dashboards. Complex tagging systems. And alerts more and more DIM. Deeply innovative metrics. Hello. My name is Annette Mulaney and I'm a software engineer. I know I don't look it, but that's because they paid me to shower. I'm just kidding. We're all very cool and hygienic. I'm here today to talk to you about observability. What are my qualifications? Am I a 10X rockstar ninja engineer? God no. No. Absolutely not. Not even a little. Honestly, I'm more of a 1X. 1.5 on a good day. 0.7 
if I'm a little hungover. Work hard. Play hard. Point is, I'm a remarkably average and deeply adequate engineer. Like most employees. But that's exactly who needs to troubleshoot apps. Normal humans. Ideally. The first stack of various products and in-house things is usually very concentrated. I'm talking relying on one person to solve the tricky issues. Which is terrifying. The power can really go to their head. I've already promised a firstborn to get help before. And that's just not a scalable solution. Plus, what if that person leaves or dies? I mean, I'm sure I'll be fine. I'm sure our stack is completely and thoroughly documented and easily grasped by whoever needs to fill in. What is observability? Other than a buzzword I throw around in meetings and attempt to sound smart? Open telemetry. High cardinality. Cloud native. Tail sampling. Are you even going to put those into sentences? No. Observability is ultimately about answering simple questions. What's happening and why? Or, a
s we might phrase it during an incident call? What the hell is happening? It's 2 a.m. Why? In my experience, I usually try to answer these questions by desperately parsing too many logs. Because I never actually got good at the proprietary query language of our service. And the logs themselves are deeply unhelpful. The tags return everything. I'm like, who set this up? Oh. I didn't. Huh. I should make that better. What if I added more tags, more logs? I probably had metrics and tracing to those logs. And you know what? More tags to those metrics. Okay, now I definitely need more dashboards and more alerts. But are there enough tags? And not to brag. But all this is costing the company a lot to ingest. Even though I don't even need most of this info anymore. But like, what am I going to do? Remove it? I mean, who does that? Oh. Excuse me. Don't worry. It's just another alert. I'm going to get them all the time. There'll be more of something's really going wrong. Plus, I'd have to wait f
or everyone to hop on the call before we can make any progress. I have time. Okay. I know it's 2AM. Let's get this over what swing go back to bed. Okay, what's the issue? Bob has joined the meeting. Hi, Bob. Welcome. Mary has joined the meeting. Thanks for joining us, Mary. Bob, you're muted. Bob. Bob, you're muted. Bob, no one can hear you. Plus, everyone has their own tools, and so everyone has their own data. Since our tools don't speak to each other, we have to. And if there's one thing I love, it's stressed out troubleshooting. Although actually, it has been one of my more reliable forms of socializing during some of the bleaker parts of the last year. Oh, restart the note. Can't believe it and think of that. This is the third time this is happening this week and that. Crazy, right? This didn't need to be a call. Sure, Bob. Wasn't it kind of nice, but it was? Okay. Well, just try restarting it next time before hitting me up. Totally. Don't you get a dog? Bye. Bye. I also tried to 
fill the void by bettering myself. A year ago, when I stopped commuting and my climbing gym closed, I resolved to dive deep into Kubernetes and finally understand it instead of silently praying to myself. And within a few weeks, I had completely given up on that resolution. But others have fared better. Observe, for example, has some pretty exciting updates to share. Thanks for watching the Observe launch. Now, this is our second launch, and we're going to focus a little bit less on the company today, and more on new product features and some of our amazing customers. If you're new to Observe, the first thing that you should know is that we're taking a very different approach. Recent research by the 451 group shows that organizations use seven tools to troubleshoot and monitor their applications. They've got many tools because they have fragmented data. We believe that observability is fundamentally a data problem. If we can solve that, the tools problem will take care of itself. Now, 
you'll often hear vendors talk about observability as three pillars, logs, metrics, and traces. When we look at the customer experience in using these tools, we find that the DevOps team is spending all of their time doing ops, tasks like tagging and archiving data. There appears to be not much dev in DevOps these days. Too much effort is going below what I call the value line, and the SRE and engineering teams don't see enough value. There's got to be a different approach. With Observe, we flip the iceberg. We use a commercial database snowflake as our underlying data store, so a lot of those operational tasks just fundamentally disappear. Our engineering team focuses their effort like above the value line. This enables more users, including roles such as customer success, to do what they really want to do, which is to understand and analyze their application. Now, what's our approach to delivering all of this value? The most fundamental step is putting all of the telemetry data in on
e place. After all, it's 2021, and cloud storage is practically free. If your data isn't in one place, it becomes really hard to relate the piece parts, which is critical to providing context, which is critical to observability. Now, the magic in the system is that the raw data, it's messy, and generated, it's gobbledygook, it's not understandable by humans. Conventional wisdom is to provide users with a search bar and have them go looking for breadcrumbs. We think that that's a terrible starting point, and so we transform the machine data into something called the Observe Data Universe. Event data is curated into things called resources that users understand. These are things like customers and shopping carts and pods and containers. No other product has this abstraction layer, and it allows us to do something pretty amazing. Most importantly, Observe establishes relationships between these resources. This allows users to quickly locate additional contextual information for the proble
m that they're looking at. The user can navigate the graph using our grappling feature without knowing the exact path between the nodes. Let's now talk about some of the new features in Observe. When we looked at the state of the art in systems that analyze metrics, we found that to become fusing, often presenting hundreds or thousands of metrics and tags to the user. If you knew the metric that you wanted to look at, you were all set, but if you didn't, you were never going to find it. Users get lost in their own tag soup. We wanted to change the game, Observe curates metrics, so that users only see the metrics that are relevant to the part of the system that they're looking at. In this example, a view of the customer was seeing average response times, error counts, and the number of tickets that they've raised. If we want to add something more to the dashboard, we just drag and drop it from a curated list. And because the user only sees relevant metrics, they're not overwhelmed. Let'
s turn our attention now to alerts. Alerts aren't new in concept, but it's amazing how unwieldy they are to deal with, even today. Too many inboxes are filled with too many alerts that contain too little information. As systems become more complex, something has to change. Like metrics, Observe's implementation is a game changer. Our alerts feature relevant contextual information, so the user knows exactly where to start their investigation. Now we can do this because of the data universe, the graph of connected data sets that exist behind the scenes. In this example, we're alerting on errors in application logs, but we also know exactly which customers are affected. Why? Because the customer's resource is linked to the application logs. Now, speaking of customers, we've made great strides with our early customers since last October, and pleased to announce that we have over 20 paying customers that use Observe on a daily basis. They're all part of our founding customer program and are
 helping us define the product roadmap. I want to say a big word of thanks to all of those customers for their trust in Observe at this early stage. And finally, a word about our pricing model, which I believe again is another game changer. Our pricing is usage-based, so customers only pay us when they're using the system. Even better, we itemize bills right down to individual data sets. Imagine if you got an electrical bill, and it itemized the top 10 appliances that were consuming electricity in your house. That would be a beautiful thing. That's exactly what we do. We believe in usage-based pricing, but we also believe in being fully transparent with where the money is going. Thanks so much for your time today. I really appreciate you tuning into the update. After this break, we'll be back to hear from the Observe founders on what they were thinking when they were implementing many of these new features. Welcome back to DIM, or deeply innovative metrics, the 1961 edition. In part on
e, we covered walk-a-byte scale databases and algebraicly correct query languages. In this show, we'll cover many more must-haves for every Rockstar 10X full-stack engineer. First up today, let's cover dashboards. That is a UI or user interface. Remember, you can never have enough dashboards. If you see one you like, you just copy it and make it your own. Contrary to popular belief, dashboards are state-of-the-art, incredibly useful, and never ever break. Oops. Next up, tagging, lots and lots of tagging. Pack, Jason, you're it. Jason, do you know what a DIM tag is? And last but not least, we need to cover alerts. And there's one now. Let's walk over to our alert operator, Ms. Phyllis Stein, to find out what's going on. Phyllis, what is that particular alert for? Is it an application down? Phyllis? Is the CPU being plotted by server instead of by process? Is that pizza here yet? Oh. Looks like Phyllis has a bad case of alert fatigue. Sorry about that. We'll address that in our next show
. And speaking of our next show, tune in next week when we'll take an in-depth look at logs. We'll show you how to determine the true age of any log by examining timestamps in our cross-section. But that's for next time, so don't miss it. In most metrics products, you're dropped into a sea of often cryptically named metrics, with little knowledge of how to get the right metric for the situation you're in, or even how to get that metric drawn correctly. All right, so typical situation. There's a problem with X. What metrics can I find that might tell me about X? You're scrolling through pages and pages of dashboards trying to find something as relevant. You don't find it now browsing through thousands and thousands of raw metrics of how you're looking for. The metric naming is all over the place. Taxory consistent, it's a mess. The way observed stores and processes metrics data takes advantage of the unique capabilities of snowflake, and is radically different from existing systems. In 
observe, the way you find metrics is dramatically different. When you look at a resource, we can find and surface the right metrics about that thing without any need to search. So we're trying to flip this on its head. So instead of paging through all those metrics and dashboards, it just goes straight to our resource page, and we'll show you any metrics that are relevant. We'll even auto-generate dashboards based on those metrics. This metrics experience is almost disturbingly simple compared to similar products where you weigh through thousands of pointless tags and metric names. Our perspective, I guess largely, is that you don't even need those tags anyway. If you can just correlate data sets together, if you can follow these transitive relationships in the data, you can answer these super nuanced questions. One thing I'm particularly excited about is our solution to the cardinality problem. Cardinality is the number of unique combinations of metric names, such as CPU utilization, 
memory utilization, et cetera, and tags, such as the application name, Kubernetes pod name, and so forth. Existing systems require users to carefully plan and watch the cardinality of their metrics data to find the right balance between cardinality, cost, and performance. Observe does a way with all of that. In observed cardinality is largely irrelevant for cost and performance. Applications may emit as many metrics and unique tags as they please, with no need for users to play with tuning knobs such as indexes. Metrics data can be retained and queried potentially forever at full fidelity and with high performance. So when we were thinking about what to do with our learning feature, we wanted to make sure that we did more than just like fire and forget a learning. So at first it was some servers on fire, and then it was these servers on fire. And now we need to know what services run on those servers, and what customers are using those services. Alerts about things or resources complet
ely changes not only the experience of configuring alerts and monitors, but also how to make use of them when they finally trigger. The alert you want to see as a business operator is a customer is not having a good time. Oh no! Because we do both time and relations, we can deliver that. We put a lot of effort into making sure there's somewhere meaningful to go when you click on our alert generated by observe. So instead of looking at random log entries or metric chart or whatever, you quickly see a list of like which users do this impact. How often has it happened in the past? What other issues might these users be experiencing? If you have a roster of things you care about, and those things relate to each other, you would be fooling out to use observe. You should use observe. Now that all sounds pretty sick, but what does it look like in practice? Let's have Belgium, a better engineer than I, walk us through how freaking simple it can be when all your traces, logs, and now metrics ar
e in one place, mapped on to concrete concepts that a human can navigate. But don't worry, even if you are that rockstar engineer that can stare into an abyss of logs and see the matrix, there's still a view with those raw logs that you can use to scare the interns. Hi, my name is Belgium, and I'm one of the engineers here at Observe. Today we're going to take a walk through the Observe solution and focus on some recent updates to the platform, particularly with respect to alerts and metrics. To begin, I'm actually not going to dive right into Observe. Instead, let's take a look at this Slack channel where I get alerts from Observe. Recently, an alert fired, telling me that some of our customers are facing errors. One of the key properties of alerts in Observe is that they leverage GraphLink to provide context around the notification. And in this case, even though we don't have a lot of errors, we seem to have a subset of customers who are experiencing a high error rate. And this could
 lead to some poor customer satisfaction. So let's dive in. Now I even observe. Here, I get an overview of that notification. I can see that this is still an active incident. And for us, like any other SaaS company, when issues occur, a key question is who was impacted. This page helps me answer this question without diving further, so that I can evaluate the impact of the problem. For example, it looks like a few more customers have been added to the impacted list since we received the alert. Let's pause and think about what the root cause may be. This customer impacting problem could easily be pods stuck in pending states, maybe no memory usage over 90%, perhaps repeated fail logins through database, or really numerous other root causes. And in all of these cases, we want to get specific answers out of our investigation to understand the impact of the problem. By answering questions like which databases affected for which application in which AWS region? To do so, let's go take a loo
k at the data that fired this alert. Now I can see the log lines that cause my alert to fire. I immediately noticed I have a lot of out of memory errors, which is a little alarming. If I scroll across, I can see more information. Like here, I have the stack trace for that error. But right now, I'm still not really sure if this is a code issue or resource contention at the infrastructure level. I need to do a little more digging to figure that out. Because these datasets are linked together and observed in a relationship graph, I can jump to the Kubernetes pods that generated these log lines. Here, I see the pods where those error messages came from. These pods are currently active. And using the time scrubber at the top, I can travel back to an earlier time when these pods came alive first, which seems to be shortly before my alert fired. Now, what I'm really curious to look at is the metrics for these pods. If I scroll down, I automatically get in context metrics for these pods. And n
otice that I didn't have to hunt around following tags, carrying around names or IDs for my logs platform to my metric platform. Because I observe is a one-stop shop for my logs, metrics, and other technical or business data, I can pull together metrics for my resources with ease. So because I was getting memory errors, it's really the memory metrics that I'm interested in. Scrolling down, I see a chart with my CPU usage metrics, and I can open it to see a more detailed view. And, aha, I see the signature so-to shape that is indicative of a memory leak. This is starting to look more like a code issue. The final piece of validation to check is whether these pods are restarting. To do that, I'm going to look at the notifications for these pods. So, observe alerts serve a larger function than just not finding you, be a slack or pager duty. They can also be used in context of an investigation. Here I see notifications about pods restarting frequently, which is the last piece of the puzzle.
 So, I quickly got to a good spot in my investigation. I know which deployment is failing. I can open a ticket for the relevant team with this link, so they can see the problem in context and fix it. But, I can take this one more step further. In my environment, I'm using continuous integration and deployment. My CI-CD data also comes into observe. By using Graphlink, I can ask Observe to find build events from Jenkins to help me figure out the exact change that introduces code issue. Let's click navigate to here and select Jenkins builds. Before we take a look at the build data, let's see how we got here. At the top of the screen, we have the breadcrumbs. We started out by looking at the error logs, then jumped to the problematic pods, and then Observe seamlessly took me from pods to the relevant Jenkins builds. There are actually several hops in our relationship graph we had to go through. But as a user, I didn't need to know what path to take. Observe knew that pods are composed of 
containers that are running images that are built by Jenkins. Okay, let's go back to our Jenkins build data. Here, I see the exact change that calls the issue. Apparently, Tom made a small change to cache code, which calls a memory leak. I can now tell Tom that his recent change is impacting our customers and needs to be fixed. Well, there's more more things. As you remember, we started this investigation with an alert telling us that our users were experiencing errors. Well, I wonder if some of them actually open support tickets related to this. Using Observe, I can actually find the Zendesk tickets that are potentially related to this bad code change. Again, leverage and graph link, I can jump to my customer tickets without worrying about the underlying path. Here, Observe is showing me all the support tickets opened by users who interacted with that problematic build. I can let a support team know that we identified root cause of the issue and we can communicate their customers that
 the issue will be resolved shortly. Damn, girl, that was both impressive and given the ability to navigate so quickly to an offending line of code, quite terrifying. Makes for some great, blamful problem-solving. Now, if you're like me, this information isn't quite enough to be convincing because there's not a human demonstrating social proof. Well, you're in luck. Let's hear from some actual customers who are using the product. We've got two customer videos. The first is from LineData. There are financial services company that is rapidly moving everything to AWS and has security and compliance challenges. So this is like key for them because they need their customers to trust them. My name is Andre Butsar. I am the director of Cloud and DevOps at LineData. I went to college for computer engineering. I actually started my career in help desk, moved on to system engineering. I play a lot with Raspberry Pi, Arduino, and automating things around the house. LineData is a FinTech company. 
We provide front-middle and back-office products for hedge funds, wealth managers, and banks. One of my responsibilities is to manage the Cloud Platform team, which is responsible for building as-code frameworks to be reused by the various business units and engineering teams, as well as develop DevSecOps and GitHub's processes to improve developer velocity and quality of life. A lot of organizations, they might have monitoring, but monitoring is only a piece of observability. Our ability is what brings all of the different pieces together, tracing metrics, monitoring, logging. Security is always top of mind for me. We have dozens and dozens of AWS accounts. But with Observe, we can create linkages not only across resources but across accounts, which makes it very easy for our engineers to isolate issues, especially when you have dependencies on different services from different accounts. So we can troubleshoot an issue in our multi-account AWS architecture in Observe without ever havi
ng to log into an actual AWS account. It's not just security of the application, but it's also the security of the infrastructure, it's the compliance, the auditing. When we developed our GitHub's process, we had SOC 2 and Mon. We're able to link CloudTrail to the pull request information in Observe so that every single CloudTrail event has links back to a pull request. So in the Observe UI, I can look at a CloudTrail event and I will see who created the pull request, what the comment is exactly what's happening, who opened it, who merged it, who closed it, what had happened. Observe stood out to me for the simple fact that you can link disparate data sets together. You no longer have to copy and paste and remember what it is that you're looking for because one data set can lead you to the next and the next like a trail of breadcrumbs until you find your way to the issue. The impact on line data has been tremendous, providing our engineering team all of the same resources that our SREs
 have in order to manage their engineering environments. That has allowed us to develop more secure applications and release better quality code into production. And then we've got TopGolf. They have over 50 locations in the US and the last thing they want when people are drinking beer and playing Angry Birds. Yes, with a golf ball is a problem that they have to wait for their IT guys to sort out. My name's Ethan Lilly and the engineering manager over here at TopGolf. I've been at TopGolf for two and a half years now. When I was growing up, the stars was the 90s. They were just making it big here in Dallas and they ended up winning the cup in 99 and that's when I really got into hockey. I used to just play out in the street with some of my friends. It's one of the few sports I'm actually decent at. I mean, I always grew up around computers. I only remember the first computer we had. So I've been heavy into technology my whole life thanks to my dad that we're going to rate things on and
 be into it himself. Our main mission is creating moments that matter. People do things like gender real parties or they do proposals at TopGolf. We need a matter of big life event. We also have top-tracer technology which traces the flight of the ball and we can use that to do all kinds of things. This year we put out an Angry Birds game. You can actually play Angry Birds with a golf ball which is pretty crazy. Things that I've been working on since I joined is moving things to Kubernetes, more modern Docker orchestration engine. When I first started programming I was on. We were maybe pushing updates to production six months to a year. That was pretty much our cadence and whenever we did it it was very painful and then over time I've seen ships do quicker and quicker really cycles. Smaller and smaller chunks getting pushed out to here at TopGolf were now on a weekly cadence. We push updates every single week. The four we're using mostly elastic search for centralizing a lot of our lo
gging information. We're limited to kind of what Kavanaugh would allow us to do which they have some capabilities with dashboards and visualizations but we mainly use it just to dig through logging. Kavanaugh uses a different search syntax called Lucene which is very annoying to use. It's like regular expressions but it's not which is really annoying for people. I would never really advertise it to anybody. Observe helps us to monitor the game system and the integrations with our POS system for handling the checks. We send a lot of data and we send over 1.25 billion events a day and all of that data is anything from a ball going into a target or a pendant light changing for a day or a new reservation coming in. What we've really gained with Observe is the ability to link our data from different data sources and ways that we never could before. Being able to link the data better between just our microservices alone and including different things like infrastructure with service now, wit
h all these different platforms that we use that we wouldn't have even thought of linking before. We've also been exploring different ways to get more data into the platform before where previously we were trying to figure out ways to stop sending data because it was costing so much money. It's saving us time, it's allowing us to more quickly resolve escalation which is better for our guests. Nobody wants to be sitting around waiting for some IT guide to fix your problem. Before it was like a cat, we had this cat that had its own mind and wanted to do what it wanted to do and no matter what I wanted it to do, it didn't matter. But now it's like having a dog because your buddy will help you out when you ask him and he's not too complicated. I love simple and dog-like. That's why I date men. Well, that's been the Observe product update. I hope you enjoyed it moderately more than the average tech product show. We'd love to hear from you, so thank you so much for tuning in. You know, at th
e end of the day, we're all just humans, standing before a monitor, asking it to please give us something meaningful. I'm not going to be a cat. I'm not going to be a cat. I'm not going to be a cat. I'm not going to be a cat. I'm not going to be a cat. I'm not going to be a cat. I'm not going to be a cat. I'm not going to be a cat. I'm not going to be a cat. You
 Over the past six months, we've become obsessed with ease of use in the new user experience and observe. We've conducted dozens of user interviews and live usability tests that have made major improvements to observe UI based on this feedback. Like to share a few of these improvements with you now. First, I'm excited to announce our new log explorer UI. The log explorer is intended to be a one-stop destination to browse, search, and analyze all of your log data. When you come to the log explorer, you can quickly flip between all of the log sources being sent to observe. You can easily search for errors and other keywords, and 
you can visualize the results of your search all without leaving this one page. Because all data and observe is linked together, you can also drill in on these log messages to see additional context, like the configuration of related resources or even related metrics. After finding the log data you're looking for, you can share a link with a colleague, create a monitor to watch for errors, or add your visualization to a dashboard. The log explorer UI is driven by a new query builder, which was designed specifically for working with log data. This means that you can access all of this functionality without learning or using a new query language. Along with the log explorer, we've also introduced a new metric explorer UI. The metric explorer is intended to be a first-class destination to find, plot, and analyze all of your metrics data. You can quickly search and browse all of your metrics with a metric picker. After picking a metric, you can use the metric expression builder to configur
e filters and aggregations for plotting your metrics. You can then drill in on these charts and quickly pivot to any related logs and dashboards. Once you're satisfied with your chart, you can share with others, you can create a monitor, or you can add it to a dashboard. Like the log explorer, all of these actions can be accomplished without using a query language, so new users can settle in quickly. Now for both of these explorers, we had two goals in mind. We wanted to streamline the adoption of Observe for users migrating from existing open source or commercial monitoring tools. These new experiences should feel familiar and intuitive to new users no matter where you're coming from. Second, we wanted to make it even easier to take advantage of Observe's unique ability to link and correlate all of your data together through connected resources. That is, anytime you see a user, a service, or a container, and observe, it should be as simple as clicking on it to navigate to related logs
, metrics, traces, and dashboards. Now in addition to the new explorers, we've also revamped the navigation bar in the product to make dashboards, monitors, and ingest configuration easier to access. We've introduced a customizable homepage to help you organize content for your team. We've updated dashboard editor with a new focus editing mode and much, much more. I'm excited for you to try the new Hubble UI.
 Hi there. I'm Cloud Economist, Cory Quinn at the Duckbill Group. You may have no idea what a Cloud Economist is, but that's okay. I made the term up. When you define what you do, that gives you a false position of authority, from which you can insist you're right no matter what the topic is. Speaking of making up a field in which to master, let's talk about observability. What is observability? Ask five people get seven different answers. I mean to me, it's hipster monitoring. For better or worse, that definition is not yet caught on because thought leadership is harder than we t
hought. The big problem in the space though is that we look at so much data coming in through so many different tools. It becomes one giant painful of glass and there's no good way to disambiguate signal from all of that noise. That is the fundamental tenet of why observability is important and why we're not all just stuck with Nagio still trying to patch it a little bit further like we were 15 years ago. Backing it up a step further. If we look at what observability really means, it means it's time once again for you to find one of the vanishing number of monarchies in the world and kidnap a princess for ransom because you're going to need to do that in order to pay the ridiculous fee. Now what is it going to cost you? Well, that's a fun story. The reason that they're so good at extracting signal from all of this noise is there are so many different pricing dimensions you won't know. In fact, it requires the energy output of a small coal plant in order to figure out the bill for these
 things alone. But I digress. That's not really what observability is. Instead, let's get observability to find for us by professionals, namely industry analysts. Instead of being paid to sell you things, they have a much more objective perspective because they're paid by people who are paid to sell you things. Let's begin. CEOs realize that their customer engagement models have changed with COVID-19 and some have changed forever. As organizations have migrated from a traditional technology stack to an application and infrastructure environment that includes cloud, multi-cloud, containers, Kubernetes, microservices, this environment is so much more complex and so much more dynamic than a traditional technology stack. When you have an issue, it can be rather like a murder mystery trying to find out what the issue is. So organizations that are doing microservices, they think about how can we pull together all of the information in context to better understand problems if and when they oc
cur. And it has a bunch of new and different requirements in terms of trying to be able to figure out what's going on. It requires a new approach to monitoring. One of the key tenets of observability is really building systems that are designed to be observed and managed accordingly. And the way I like to think about it is that it's really a love letter to your future self or perhaps to someone on your team. It's really thinking about wait a second. Let's try and ensure that the outputs are going to be those that can help us to solve problems. If you want to be an observability vendor, I think there's a couple of key things that you're going to end up having to offer in order to meet demand from customers. So one is scale because especially as end users are in these cognitive environments, they're collecting a much bigger volume of operations data. So you have to be able to scale to be able to affordably collect that volume of data. And a tool has to collect different types of data as 
well because they're increasing recognition that there's value in metrics and distributed traces and events and logs and errors. So you have to be able to collect all these different types of data within a single system. You've also got to have pretty sophisticated analytics. Then correlating that of course with business metrics, other kinds of metrics so that the people that matter are the people that you can help from a customer service perspective. Or a couple of things from a product standpoint. First and foremost, full stack visibility. Really having an opportunity to collect metrics, logs, traces, external and internal information, really bring it all together in context of an application service or a system. We've also found that this whole theme of it's not just about operational data, but collecting business data, understanding the nuances of the dependencies of different data components of a system. Got observability natives and others that are probably doing some observabili
ty washing. So certainly if we look at vendors across a number of spaces, certainly distributed tracing, logging and metrics, all of those vendors are repositioning around this observability notion. You have all of these different kinds of tools, different categories of tools out there. That creates some problems for end users in terms of being able to correlate data that's collected and siloed within these different tools. Then you've got a new set of play that have come in and they're really thinking about observability in a different kind of way. They're coming out of the notion as I say of troubleshooting and they're coming out of observability very much in terms of understanding the behavior of a system from its outputs and dealing with organizations and helping them to have a new way of working. Why are executives thinking about this? Well, the business drivers is really about reliability and customer experience. When we start telling together some of that infrastructure informat
ion and the telemetry around the infrastructure or applications are running, with stuff that we're seeing in places like GitHub, where we could take the social coding information and really begin to understand who checks something in, when they check something in, and you can begin to correlate system information with human telemetry. I think that is really interesting one of the things I want to start seeing from observability then is going forward because that's really where the puck is going, I think. Observability is a love letter to your future self. That's incredibly poetic by analyst terms, in sconce, within a love letter to their future client. Now, that future client, Jeremy Burton, CEO of Observe, is going to talk somehow for five minutes or less about what observability means to him. Ideally, at the end of that video, he'll be accepted to college. I want to take just five minutes of your time to tell you a little bit about what Observe does and the customer problems we solve
. As most of you are aware, we live in a digital economy. With your product company, services companies, or with COVID, a government agency like Education, you're doing everything online. Every company has to become a technology company. Key to delivering a digital product or service is software, and we build software differently. We use microservices, and we deliver new features every day using continuous delivery. There's never been more change going into production on a daily basis. And when something goes wrong, the customer notices. The stakes are high here because we know that when a consumer has a bad experience online, in 76% of cases, they don't go back. Now when something does go wrong, the scene resembles a murder mystery movie. The smartest guy in the room trying to figure out the answer to two questions. What happened, which is the easy bit, but why did it happen, which is a nightmare? And it's hard to find out why something happened because the date is fragmented. No one,
 even the smartest person in the room, has access to all of the information. They have to piece it together using their intuition and knowledge, which is not sustainable. But the worst bit is that it costs a lot of money to know nothing about your organization and what's going on in your systems. Legacy vendors charge or volume of data ingested on metric points that preventing you from ingesting the very data you need to troubleshoot your problems. And this is why we created Observe. The current log analytics and metrics monitoring and APM vendors, they're not getting the job done. New problems need a new approach and we firmly believe that observability is that approach. Now when we think about observability, we think about it holistically. We don't just think about cloud infrastructure or database infrastructure. We don't just think about the business applications or the service desk or maybe the CICD infrastructure. We think about everything. Why? Because everything is related. When
 the custom has an issue with the application, they're going to raise a service desk ticket. The issue may be in the database or it could be in the underlying Kubernetes infrastructure or it could be with the AWS infrastructure. You don't know. So you have to have this wide angle field of vision. We wanted to take a different approach with Observe. We didn't want to become a tools company. The custom has enough tools. We fundamentally believe that observability is a data problem. An Observe is a data company. Now key to everything here is us being able to ingest all of your data. We don't subscribe to this three pillars of observability view, logs, metrics and traces. We treat everything as an event and we start all data together. What we do subscribe to is cloud storage economics. We think you should be able to ingest as much data as you want and keep it for as long as you want. Once we've ingest the data, we shape the data. Why is this important? Well, we didn't want to give users a 
search box and have you go looking for breadcrumbs. We curate the data and turn it into users and sessions and shopping carts and pods and containers. It provides a much more logical starting point. Most importantly, after we've shaped the data, we relate the data. Behind the scenes, we create a graph of connected data sets and we don't do it using tags. Tags are a nightmare. It's not a sustainable way to join data that become un-maintainable. We think that by connecting data sets, the user can navigate data more quickly and bring all of the context they need to bear on the problem that they're looking at. And finally, we keep track of time. Modern systems are a femoral. Different things run in different places at different times. The question maybe isn't what happened. It's what happened at 2 a.m last Tuesday when the user received an error on the website. Observe keeps track of state of all the components so you can wind back the system to any point in time. You may be thinking at th
is point, well, observe is this some kind of data tool for data scientists? The answers know we built the interface for the SRE team and we realized that the SRE team may have junior members that are on call and are trying to triage problems in the middle of the night. So we wanted an intuitive visual dashboard approach to allow those users to use the product. But we also recognize that when you're investigating an issue, some of these issues are complex and hairy. You've got to get in and deal with the data directly. So whether you're a junior member of the team or a season member of the team, you can work with Observe. Taking a step back, what Observe does is it allows you to find problems in order of magnitude faster. And we do this because we present things at a familiar and we allow you to quickly navigate to additional contextual information which will help you solve the problem. The example on the screen shows a navigation from help desk tickets right the way through logs, right
 the way through to even Jenkins builds. And the best news is that we have usage-based pricing. We separate storage and compute so you can ingest data at the cost of S3 and you only pay when you use it. Getting started with Observe is simple. In fact, you can do what you're doing today. But as you drive towards this goal of a fully observable environment, Observe will allow you to keep on ingesting the different data types that you think you need to find the problems that you see. Thank you very much and thanks for listening today. Ugh, missed it by about 10% for the timing, which by observability metrics is pretty decent or so I'm told. We'll be right back after this brief message about a company we're legally not allowed to mention by name. So that's all well and good. But what does this actually mean in practice? One of the biggest problems we're having right now during this global pandemic is that you don't have that moment of frustration watching someone else drive a computer and 
it's not the way you would drive the computer. Here to help bring that office moment back to you is Belcha to demonstrate exactly how Observe works. Hi, my name is Vagy and I'm one of the engineers here at Observe. Today we're going to take a walk through the Observe solution. For my demo environment, I have a microservices SaaS application running on Kubernetes and Amazon EC2. Right now, I'm an Observe and I'm looking at the different resources I have to start any investigation. Notice these aren't all technical or infrastructure terms. Some of them are in fact related to concepts like customers, tickets, and so on. Well, let's take a deeper look at the customers resource. Here, I can see the different customers that are using my SaaS application. They're all represented on the honeycomb and some more data about them further down the page. One thing that jumps out at me here is that I have some notifications related to these customers. So notification service of items in the data that
 are of interest, which indicate that there might be an issue. So let's take a closer look at that. I see that I have a number of items here that are coming from Zendesk. So when a user opens a ticket in Zendesk, that also gets sent into Observe where we can tie this back to the customers and the rest of the observability data. It looks like a number of customers are seeing issues. So let's dive into that. The first thing I'm going to do is use this related navigation to drill down to Zendesk tickets themselves. I see I have quite a few tickets here and what I'm going to do now is to use the filters on the right to just look at the high priority tickets. This way I can focus on what looks like my real burning issue. Well, I have some tickets here and I can see which users opened them. What if I could jump to my logs and my application just for these users who have opened these tickets? Well, I can. Because Observe understands relationships between the data it consumes, I can click Navi
gate 2 and just ask Observe to take me to the logs using teleports. Let me pick App logs here. Now we're looking at the application logs for just those users who opened high priority tickets. But how did that actually happen? Well, Observe figured out that to get from customer tickets to App logs, I had to go through the user and user session data sets. As a user, I didn't need to worry about the path I had to take to get to a destination. Observe simply to care for me. Well, I still have a lot of log lines here though and it is kind of hard to figure out the exact problem. But as we have shaped this data, I have a number of filters available. I'm going to filter on just those log lines that are level severe. Well, now that the filters are applied, I can start to understand what the problem is. It looks like we're running out of memory, which indicates a memory leak somewhere in my application. To validate this hypothesis, I want to look at the pods that generate these messages and see
 if there's a problem here. I suspect that this might be related to pod restarts. So next, I'm going to use this related navigation again, click on pods to see just the pods that generate these errors. Here, I see that pods that may be causing our problem. I can use this time scover to see when these pods were and were not alive. And it looks like something has changed recently. So again, I see that I have some notifications of interest. Let's take a look. And aha, there's a problem and it does appear that these pods are continually restarting. Great. I think I have something that I can pass on. I know which pods and deployments have a memory leak. And with that, I can tie back to a dev team and pass my work onto them. But I also have my continuous integration data coming from Jenkins into observe. What if I could use that to get right to the problematic commit and build that introduces issue? Well, let's give that a try too. Again, we hit navigate to and use teleport to find the relev
ant builds. Here we are looking at the relevant Jenkins builds. Notice we have just one event here, because there was one built that introduces problem. I can see that it was Tom who made a change to cache code and it looks like this is our problem. So now I can go right to Tom and say, hey, bud, that cache change commit you did introduced a memory leak that is impacting our customers. I can save where I am in my investigation and share my work with Tom to give him a place to start any further investigation he might want to do. So let's summarize what we just went through. I can see the whole path of my investigation here on the right side on their all applied filters. I started by looking at my customer and then the tickets they opened. I filtered on high priority tickets and then jumped using teleport to the logs for those users. When I filtered on severe messages, I saw a suspected memory leak. So I looked at the pods throwing those errors and saw that they were restarting. Well, to
 close the loop, I did teleport again and got to the build and commit that actually introduced the problem. This was all the direct correlation in a single tool and that's the video of observe. Tags. Yeah, I fix AWS builds for a living. Let me tell you, tags are the only way to get meaningful signal out of that entire mess of nonsense. Who tags appropriately? Absolutely freaking nobody does because that's a job for computers who set the computers to tag appropriately. That's right. No one has. So it means that you're trying to wind up sifting through this giant universe that's constantly changing and then yelling at people because the tags aren't correct. It's a terrific way to wind up making observability something everyone can hate if you're basing it all on top of tags because it's not only expensive and demeaning, it also doesn't work. Tags. So in case you couldn't tell, tags really torque me off. Now, let's talk to the observed founders about what torque them off so severely in th
eir previous corporate lives that they felt they had no choice but to inflict another observability product on the rest of us. We have tens of billions of dollars spent on machine analytics, but the problem is that the systems that were built were all built decades ago, you know, a decade or more ago and they were built for a different world. And this was one of those things we had butterflies in our stomach. We weren't sure if we could actually build this thing. You talked to customers and they would tell you, you know, yeah, we have an APM tool, we have a log management tool, we have an alerting tool, maybe in some cases they have three or four of these things and nevertheless they still had no idea what was going on in their environment. So one of the common problems with lots of these tools is that they give you no structure. There's nothing to ask about. There's just a giant soup of data. But what people really want to do is they want to ask questions about things. So things can b
e users, they can be sessions, they can be hosts. There are lots and lots of things in any business data. And that's what you want to ask questions about. So you open up one of these products and as a new user all you see in front of you is like a blank search bar and it's sort of like the system telling you it's like you figured out these dashboards are all pointless. They're utterly useless. Two weeks into using them uses realize that the dashboards aren't telling them anything they need to know. They're generic and pointless. You know universally log management was seen as a very expensive proposition. You pay an awful lot of money just for the privilege of storing your log data. People just keep glomming tags onto data and you get this mass of tag soup. It's a complete mess. We wanted to provide an experience that was as flexible and general as a log management tool but had strongly opinionated workflows like an APM tool. One of the things I wanted to solve is sort of like let's br
ing it all together. Let's have one system that does it all and not a Frankenstein's monster where we're globbing different systems together. Observe stores all of the raw data with all of the timestamps when we saw it and also optionally timestamps that you put in when you saw the data and based on this we build up a model of what happened over time. What happened when to whom it's a little bit like a crime scene investigation right at the end of the day like I don't really care about the logs per se. I don't really care about the metrics per se. What I care about is the user or the service or the thing that this telemetry tells a story about. Observe needs to figure out how to magically turn that raw stream of event data which we call observations into these higher level resources and historic state of these resources. Now Observe actually has resources we can actually do some really magical things with them. So one of these things is landing pages so we can look at a resource we can
 look at all of the data in a resource and we can reflect it and build dashboards automatically in a landing page. Time is really front and center if you're dealing with machine boring data. The first technical challenge was well we have this theoretical underpinning of you know the time dimension and the relational dimension how the hell are we going to build something that users can understand on top of this algorithm that there's only like five academic papers on. The challenge with time is that users it's not just good enough to tell users what is the state of the system right now people and don't just want to see the latest state they need to see the history of their system so they can ask questions such as you know what was the state last night when things went down what was happening it's not actually only solving this or two dimensional relations and times it's figuring out how do we present this to me the user in a way that I can easily use and get on with my life and solve th
e problem get a very short mean time to clue rather than getting lost in all the technical details. So one of the things that resources and these connection between resources give us is the ability to produce some of these magic moments in the UI one of these is the portal feature that we implemented a few months ago this allows you to start with one resource and get to another resource without knowing how the magic happens from an end user perspective it's phenomenal. Getting those kinds of insights out of data that you already have but have underutilized that's an enormous potential of observe that once your systems put their data into observe yes this SRE is going to be the first person who uses observe to figure out what's going on and how can we get better uptime and how can we fast deliver our features but the the user product managers and the marketing team are going to be fast followers and build totally different applications that we haven't seen yet on top of our platform. Wh
en you do things that are really profound if you knew how hard they were really at the start you went do them and so we're now proud to show the world something that they're going to be shocked by. AI is a beautiful thing because there's no better way to express the sentiment I hear you have VC money and I would like some of it please that's really where it starts and stops. Now I've long said that any keynote webinar or presentation about a product is going to be shitty if it doesn't feature customers telling stories about what they actually use the thing for. You'll see this sometimes at various conferences the am world sorry so in that right let's talk to a few pre-release observed customers about their experience with the product and why it's not a complete clown show. The SaaS applications these days they've gotten really complex with all the multi-tenancy on the network and the infrastructure that the data traverses through and so many different applications and monitoring tools 
but you know you can't scale all of that with just people. From the technology standpoint we deal with massive amount of data and we also have the challenge to deliver personalized experience in real time. We recently transitioned our software development efforts to use modern tools such as containers clustering and orchestration which provided us with considerable operational efficiency but that costs visibility. The root cause analysis is just spread around different log files and consoles and monitoring systems and stock traces and new name it so for us to be able to identify what's going on is the challenge that our alerting systems are noisy. Most of my team could legitimately ignore 50 to 60% of their emails and that number is higher for me. Right now we have to go through multiple systems log into multiple things we have a dashboard separate we have a log monitoring that is separate and then we have an API monitoring that is separate right so you have to be able to connect the d
ots across all of these systems. Application logs are spread across multiple compute nodes we rarely know what node is servicing a client when there's an error. It's really an aggregation problem we need a system to gather data from all of our application containers and store it in one place preferably in the cloud so we don't have to maintain anything on site. With the tool like observe all of them come into one place we get into a dashboard my infrastructure is mapped and I can go from a trouble ticket to solving the issue in one place. Observe takes the idea of event relationships to the next level it allows us to drill down from a big picture view into more granular data but it also provides us visibility in related event streams in addition to providing the standard searching and filtering tools that we've relied on for years. It really helps us with root cause analysis we have a lot of different systems to track log files our uptime we have monitoring and learning system and for 
us to reach the root cause analysis is just trying to match up those different sources and observe helps us really identify call it the data input in one place. Their whole philosophy is that the clients shouldn't have to write queries or regular expressions that makes me really happy the functionality is available but observe would prefer that the clients didn't ever use it. Generally we don't have a lot of time to write queries and to parse and organize log data that we only use when an application is broken. When an application is broken we definitely don't have time to try to figure out queries or regular expressions. How well they use an experience as being thought up as a very junior support person customer success or a product person I can jump into the UI look at the dashboard see where the problem is and I can almost you know get to the root cause of the problem and being able to provide all of this good you know troubleshooting information to the engineer and the engineer can
 solve the problem quickly. We believe that the focus that the product has on event correlation and relationships will go a long way to helping us generate useful relevant alerts. That helps us deliver better customer experience in the end. Hello I'm Denise Pearson, Chief Marketing Officer at Snowflake. The Snowflake Data Driver's Awards is a global awards program recognizing innovative individuals and teams that are transforming the organizations and the world around them using data. Every year we review hundreds of nominations from Snowflake customers around the world. Finally, the winners are show sound by a panel of industry experts and Snowflake leaders. Over the last year we've been transforming Snowflake from being the world's leading cloud data warehouse into the world's leading cloud data platform. Our goal is to grow a large ecosystem of companies who are building businesses on top of Snowflake. To mark this shift we've introduced a new category to the Data Driver's Award cal
led Best Data Application and I'm delighted to announce that Observe has been selected the winner in 2020. Observe's SaaS Observability Platform runs exclusively on Snowflake and takes advantage of many cutting-edge features. We're looking forward to working with Observe to enable our joint customers to investigate their applications and infrastructure and order of magnitude faster and cheaper than ever before. As a permanent reminder of this win, we'll be sending over this Data Driver's Award's trophy over to the Observe team. Congratulations. And there you have it. To my perspective, what Sets Observe apart is the fact that they're talking about the pain that they solve for their customers without denigrating where their customers are on their incredible journey. They're doing it without having to spend years explaining it to people and most importantly, they're meeting customers where they are with the problems and pains they're feeling today rather than talking about the far future
. I think they're launching today at ObserveInk.com. I'd encourage you to go and sign up for early access, the first 100 people who do get something a little special. But then I want you to tell me whether it's good or whether it's crap. I have problems in case it wasn't blinded in the obvious, but they don't look like most other people's problems. So I'm curious to folks who are running these things in the wild. What do you think about this? I've already been paid. I just want to know what stories you're hearing from this. Is it as good as I think it is? Let me know. I'm Cloud Economist Corey Quinn, and this has been the launch of Observe.
 Կְִִִִִִִִִִִևְִִִ֍ִցִֵֹֿ֮־ւֻ, aocracy jetz everu  AD vard
 Jong Raymond You don't have to. You want to leave now? Give him a give! From this region, and village may be registered
 Hello and welcome to the Observability Trends webinar series. I'm Grant Swanson your host and today we will dive into the topic of navigating regulatory compliance with 
observability. Now I'd like to introduce our guest speaker, Senior Sales Engineer Keith Buzwell. Welcome Keith. Hey Grant, thanks a lot for having me today. Yeah so just getting right into it, navigating regulatory compliance with observability, a modern approach. So, you meet it here, there's three things I really want to dissect just from the title of this webinar today. So, regulatory compliance, what is that? You know, what does that even mean? Set a little of a baseline there. Just make sure you know we're all on the same page what we're talking about but then doing it with observability and a modern approach. You know, so what are all those things mean inside of here? So, just getting into the first one is that, you know, what is regulatory compliance? And really these are the rules that a company should be following to make sure they're taking care of their employees data, their customers data, partners data, and it varies. You know, so if you're in banking for example, there's 
going to be some differences on some of the things that you should be doing there. For example, in things like HIPAA, NERC-SIP and with critical infrastructure. So, it really depends by the industry that you're in and what type of regulatory compliance is and kind of measures that you're going to have. And it covers much more things outside of just logging and you know where workload should be running and things like that. But it even goes down to like HR type of policies and stuff like that. But it goes very, very in depth. But for the purposes of today, I really want to discuss what it means, you know, to do it with observability. And it can even depend on geography. So, for example, if you're living in the EU and GDPR is going to be important to you if you're living Australia or even California. And you know, there's going to be different laws that you would need to adhere to on what type of data, how long you keep it, what you do, and you know, how are you securing that data as wel
l. So, there's a lot of variations there and which kind of leads me into this next portion is what makes it so difficult. And I just, I've been in the logging realm, I would say for around 10 years now a little bit over. But in every time compliance comes up, it's always, you know, it seems like it's a bit of a difficult thing. And some of the reasons are is that it varies so much, you know, depending on your industry, you know, what type of things should we bring in about your customers. Matter of fact, your own customers themselves can have an impact on what regulatory compliance is that you should be adhering with. You could have multiple regulations, you know, you could have a different type of a tool stack in your product stack could mean that's like, hey, the products we're offering means that we have to have multiple, we align with multiple regulations because of the type of customers that we serve to. And then of course, location, but another thing that makes it difficult is th
at traditionally it's been, it's been pretty expensive to adhere to as well. And some of the reasons for that being is things like long retention, you know, I think, you know, like PCI for example, you know, like 90 days readily retrievable, but one month or one year of, you know, kind of backup or an archive. And then so you know, just saying 90 days readily retrievable, and that can be achieved in various ways, but essentially it's saying, you know, at any point, I need to be able to go ask this question about the day that is, you know, in the PCI scope and make sure I could treat, you know, retrieve that data quickly. And that, you know, anything with that first 90 days, but then a year after that, you could have it in something a bit of a colder state. So that's complexity as well, because you know, for example, just like that is that well under the first 90 days is here after that need to go over here. And how do I interact with that data? Do I query it the same? Does everybody kn
ow how to access that? So there's some, some complexity that gets added to some of those things. And sometimes it can be a lot of data, depending on how large your environment is or what your business is, and the services that you offer up, it can be very expensive and to do some of these things. And then sometimes, you know, I've even seen like hey, we're just hosting something internal to just logo this data. So, and that's fine. However, now you have infrastructure costs. Somebody needs to maintain that environment. You got to make sure it's up to date. And if that has data coming into it that has maybe PCI or HIPAA related data as well. Now that's in scope as well. And so it has to adhere to the same kind of things that we're, you know, we're trying to log from in the first place. And then if you're not doing things right, sometimes there's some very large fines that are wrapped around this as well. So just some, just some things that I would say that have come up over the past 10 
years that I've seen that have just made this kind of a difficult problem to solve. And then it even comes down to give a, hey, I know my industry, I know my customers, I know the regulations. And then really kind of getting done. Okay. So now, you know, exactly what data is it that I need to be collecting. How am I going to get that data in and, you know, where am I going to store it? How am I going to store it? So there's a lot of questions to ask there as well around some of these steps. So some of the things that I've seen as options of that have tackled this problem in the past, you know, so one of them is just using your existing logging or observability tools. So I just I have a logging background myself. So I've seen this attempted, but I would just say previously, you know, so I was at some of the pros around this is that, you know, a lot of the different platforms that I leveraged had their own agents. There was an existing pipeline going into that logging platform. And you c
ould just kind of piggyback off of that, you know, more often than not, that data was already being collected. Maybe there was some additional things that you needed to bring in or remove some filters to allow that type of data that meets your regulations to be to come in. And then often, you know, you had to have it some type of option to archive that data. You know, I could say that you see this data go archive over here and, you know, we don't we're not going to use it. And if we do need to use it, maybe it's like a rehydrate or re index type of function that you would use. So there was some of those pros on the cons is that most of the platforms that I've leveraged in the past. In some fashion, the ingest that the data that you were bringing in that was tied to directly your licensing cost. You know, said, it's like, okay, you're bringing in, you know, say 100 gigs of data per day. There's a cost to that. So your costs would go up to bring in this type of data. And it could be very
 expensive. Sometimes it just bringing data. It's like, hey, I just simply need this for compliance. You know, I don't feel like it should be that expensive. I don't even really plan on interacting with it that much. So it created some some very expensive scenarios, depending on what you were trying to do. And then also like if, you know, the data was archived off, you know, which was sometimes an option. It would become cumbersome to use it is, okay, where is that data? What's the process of going about interacting with that data? Can I just query it where it's at? And it's slower. Do I have to re-injust that data and put it into an index and then make it available inside of there. And then sometimes even bringing that data out of the mechanism that it was in became very expensive as well. So it's just like with the complexity and so it's never been a very easy problem. Another option that I would see is kind of one of the things I already mentioned was building a appliance logging pi
peline. So it's like, hey, you know, whatever team has their own logging observability tools, we're going to go build our own thing. That's just for compliance on make it super cheap storage. It could be, you know, obviously more cost effective than commercial off the shelf. You're not locked into vendor collection pipelines and stuff like that. So you have something that's very, you know, repurposed and kind of easy to, you know, take from one scenario, one environment to another. But some of the cons, you know, once again, now we have somebody that has to kind of maintain this environment. Some of the upfront costs and building a pipeline, you know, a lot of times I would see it something like backed up into like S3 buckets, for example, or just building some type of routing pipeline that would, you know, dump into some type of blob storage. But there's an upfront investment to that because you know, some of the things that you have to understand is, okay, how are we going to collect
 that data? What tools are available to grab that data? How are we going to build the pipeline? How do we route it? And you know, there's also costs associated with that as well. And a lot of times that I was interacting with those type of scenarios is like, hey, we're just going to go put it in like this S3 thing and then, you know, maybe use Athena or, you know, some other mechanism that will allow me to search my blob storage. And you know, a lot of times it was slow. You know, I think the query language more often that wasn't terrible. It was kind of SQL based on a lot of times, which I think is a very common query language and a lot of us know it. But also it wasn't very intuitive, I would say. It was just kind of is what it is and you just kind of dealt with it. And then one of the big downsides for me is that, you know, when that data was collected and it went into somewhere else, you just really didn't have an opportunity to correlate that data with anything else anymore. It li
ved on an island. It was in its own silo. And for the most parts, that was okay because maybe the data didn't have a ton of value and correlation capabilities inside of there. But, you know, there was everyone's thoughts like, it would be interesting to see how this is tying into the things that are in my observability or my logging platform. But really just didn't have an opportunity to do that because it just kind of lived on living on an island there. So those are some of the things that I've seen. Just, you know, kind of throw my 10-year with just logging and trying to, you know, see these different compliance and regulatory use cases in some of the different ways we went about trying to solve it. So, you know, taking a bit of a modern approach to this. Now, what does that mean? So some of the things that I'm looking for nowadays when I think about, hey, I have all this data. I want to bring it in. But, you know, there's lots of scenarios where I want to leverage cloud native archi
tecture. There's a lot of economies of scale being able to dump stuff, like I mentioned, like into S3 storage and stuff like that. And have stuff that's very elastic compute. You know, if I want to go interact with it, if I'm interacting with a lot, be able to scale up, be able to scale down, have unlimited storage, be able to have very cheap storage as well. Also, I don't really want to think too much on how I want to ingest that data. And so, schema agnostic, what I mean by that is, you know, for a lot of examples, some of the tools that I've leveraged is like, oh, well, you know, as that data comes in, it needs to pass through this pipeline, it needs to hit these parsers, these parses need to extract these fields. And if you don't do that, then really it's difficult to interact with the data once it's inside the environment. Sometimes you just, you can't even really interact with it all. So, it's like, I just don't really want to think about that too much. I really, I just kind of w
ant to throw the data in. And if I do need to create some type of schema or extract some values later, I want to do something with it. I could do that at any point I want. So, I think this is just kind of a more of a nice to have just considering some of the things that I've leveraged in the past. A flexible data collection pipeline. I don't really love being married to proprietary agents. You know, some, the agents typically work well, I would say. But the problem is, is that if for whatever reason you need to, you kind of break up with that vendor. And then now you're left in a scenario, it's like, okay, now we need to go redo that work for putting on new agents and configurations. And so, I really want something that's flexible that can leverage in existing pipeline that I'm building. So, for example, if I'm leveraging like FluentBit agents or Telegraph or Grafone agents, kind of really, you know, anything in that open source realm, I really want to platform that can just consume th
e feed from those agents. Just like I would be doing if I was building my own open source pipeline and putting it into there. So, I would, I want to be able to reuse the pipeline that I built. And then of course, cost effective. You know, that's a big one, right? That's one of the biggest reasons that I've, or one of the biggest things that I've seen and a lot of the architectures is that, hey, you kind of bring in this data, it's attached to ingest. And it's going to be very expensive. So, something that's cost effective to do this as well. Now, what does this mean for Observe? And doing it inside of Observeability Platform, such as Observe. So, really, one of the key components that we've been able to make things very, very cost effective, but also very performant, is the concept of separating storage from compute. You know, we really, we're reusing both when we're interacting with data and we're ingesting data. But really, they're kind of serving two different purposes. And I, in on
e really shouldn't necessarily from a licensing perspective, be associated with the other in some scenarios. And I think compliance logging is a great use case for that. Because if you think about it, I have all this data that I'm bringing in. I really probably not going to interact it with a whole bunch. So, I'm not going to be using a whole lot of compute. But I might be using a lot of storage. Well, I really maybe only just want to get it charged, you know, mainly for the storage and not so much the compute. So, being able to decouple those two things from each other is one of the key architectural components that Observe did when we design the solution. So, what does that look like? So, starting with compute. Now, on the left, this is kind of what I'm used to in the past. So, data is going to come into your platform. It's going to get put into these storage tiers, for example. We're going to talk a little bit about storage here in a second. But everything was tied together. So, if 
you, if you brought in more data, the compute, the storage, it all scaled at the exact same, essentially the same size. And that build, the license that you got charged, it was all tied together. And then sometimes there would be tiered storage, for example, something to reduce some of those costs. But now, it also created complexity. How do I queer that data? Do I need to do it in a different way? Does the query language, even the same, do I have the same capabilities? I'm going to dive in that a little bit more here in a second. But we really wanted to decouple those two. So, starting with compute is Observe is hosting several data warehouses on the background. So, when you go run your queries and one of the key architectural components is that we partnered with Snowflake to deliver the back end of our product. And so, with that, we have really low cost storage. And we also have very elastic compute in the fashion of these data warehouses that we're all running in the background for 
you. So, now, when you go run a query, we can look at the data that you're interacting with and go pick the appropriate size data warehouse and a sliver of that to go run your query against. And as we're hosting these resources, if some of these resources aren't being used, we can scale them down. Or if we say, oh, yeah, there's a lot of queries happening. And maybe I'll only have one Excel large available right now. I might want to go back it up with another one. That way that everybody's getting the exact compute they need at any given time. But we can scale those resources up and we can scale them down. But we don't need to tie the compute and the storage together and also be very performance anytime you go run a query as well by doing this. Now, what's, you know, so the way that we're achieving this is that we have adopted the concept of a data lake. Now, that data lake for us is essentially going to be S3 storage that's going to be feeding into Snowflake tables. And I'm going to g
o, we're going to show that a little bit in the UI here. But one of the, one of the key foundational things that we've done is we've aligned with an open source collection pipeline. So as you go deploy your telegraph agents and your beats agents or whatever it is, you can keep using those. You can repurpose those pipelines that you've already built or partners with curvil as well. Great tool to be able to pipe data into us. So it's a, it's a, it's just a really flexible platform to be able to get that data. And then also with things like AWS and as you could deploy our apps in our apps, not only is it going to have dashboards and things like that free to use, but the entire collection pipeline is also built into our apps. So when you deploy the app, you're also going to get all the tools to get in that data as well. So it becomes very easy. It removes that burden of having to think about, hey, how am I going to get this data into the platform? And you know, kind of how do I interact wi
th that data? So a lot of nice things baked into that. But as that all that data comes inside of here, it's going to go live inside essentially S3 buckets in our platform. And it doesn't matter what it is if it's a log, a metric, a trace, everything business contextual data, all lives inside of that data lake. So it reduces some of that complexity that you might have like, okay, well this, I have to put this type of data in this type of index after put this type of data in this type of index or it goes into this type of back in database. So all of our data lives in the exact same spot. Also all of our customers get 13 months of retention by default. So just out of the gate, that's what we give everybody. And a lot of times like, oh, wow, that sounds expensive. But the way that we can decouple storage and compute and since we've partnered with snowflake to deliver our back end, one of the things that is really nice about our platform is that we average about a 10x data compression. So i
f we're talking in terabytes, if you figure for every one terabyte that comes into our platform, we're going to compress that data 10x. So a matter of fact, if we even leverage like a number like 10 terabytes because you kind of use a nice number that we know will compress nicely. If we use a 10, if we have 10 terabytes of data coming into our platform and we compress that data at 10x, now this is where our monthly storage charge would come from. And this is just piggybacking off a publicly listed S3 storage rate. So 23 cents a gig per month. So 23 dollars for every terabyte that you bring in. So you could figure for every 10 terabytes I bring into the platform after compression, I'd be paying about 23 dollars a month for that data. So very cost effective to bring in a lot of data and just store it inside of here. Matter of fact, this is even more cost effective than I've been able to do it with my own S3. Like I have a guy I don't want to put it in maybe to another vendor's platform. 
I can do it cheaper. I haven't found that to be to be true is that I can that you know, since we charge post compression on the storage. It makes this very, very cost effective and very appealing to be able to store the data inside of here and on a hot ready to search state all the time. There's no tiered storage. You don't have to think about where it's at or anything like that makes it very, very, very compelling. So with that storage, you know, some of the some of the different things that I've seen in the past, obviously it's like you're going to have your hot storage and maybe you'd say what's called seven days. We keep data in there seven days and now it goes into warm storage stays in there for 30 days and then everything after that maybe goes into cold. And this is kind of what I've been used to doing. But once again that complexity comes in how do I query that data? You know where where does it live to even can I even dashboard off of that? You know, or like what capabilities 
do I have when it's at these states. So a lot of that complexity came there, but it from us it's just cloud storage. It's all hot. It's all really tribable. So if you have 13 months, if you go interact with that data 13 months from now and you interact with that very first data that you brought into the platform. It's going to behave and it's going to respond the exact same way if the data that it was ingested this morning. And that's one of the big foundational concepts is that we want to make it really easy. We want to make a very performant and also cost effective. And these are some of the ways that we've been able to achieve that. So what I would like to do now, just considering kind of logging and like just compliance and regulatory. It can be a little bit difficult I would say to demo that. But there's a couple key foundation things that I brought up inside of here that I would like to go interact with and do inside of the platform for you. Just to kind of show you like, hey, th
is is what I mean by like there's no tears and everything works exactly the same with each other. So what I do is I'm going to back out of here. And we're going to go inside of one of our observed tenants inside of here. And I think this is a little bit small. So I'm going to open this up and what I want to what I want to start with is over here on our applications. So one of the things I mentioned is just hitting the easy button. It's like, hey, you know what, I have this regulatory compliance. And you know, maybe for whatever reason, like me or me data that lives inside of your containers, work loads that are running in Kubernetes. I need to make sure that I'm retaining that data for a long amount of time. So when you go deploy our apps, not only is it going to have dashboards and the different modules to go create data sets from the environment. And you know, go create all the specific things that you want to interact with. But all of the collection mechanism as well or built inside
 of there. So, you know, I mentioned that we aligned completely with open source agents. You know, at the end of the day, you might not be open source collector experts. And that's okay. You know, because we are. And so what we've done is that we've baked open source agents inside of our modules. And, as a matter of fact, every module in here has the agent baked inside of it. So when you go deploy the AWS app, for example, all of the things to get that data in are baked inside of the app. And that's true for all of our apps inside of here. So that's a really nice way just to really get up and running really fast. And you just don't have to think about how to get that data. So, you know, what mechanisms and the configs and things like that. It's all done for you. And now as you deploy these apps and you start getting the data and this is that data lake. So all of your raw data is coming inside of the environment right here. And you can see like, for example, I had the AWS app deployed. 
I had the Azure app deployed. You know, some of these other ones inside of here. So at any point, I could just come in here and go take a look at AWS, for example. And I could go open this data set and look at the raw data. So if you, you know, a lot of compliance is going to say, hey, I want to be able to, you need to be able to prove that you have the raw on a modified data for X amount of time. And as I go open up these raw logs and I start kind of just listening through these. This is the exact feed of data coming from our AWS account and it's raw format. So you're very easily going to be able to access that. And I'm going to come back to this in here just a second. But I want to show that you're always going to have the raw data available to you inside of here. The other thing that we're doing is that we're building a navigational guide of your environment as well. So if I go open up the data set graph, for example, and I say, okay, well, you know, all of that Kubernetes data that
 I was bringing in, we create this GPS guide essentially inside of here. And you can kind of see how everything gets related and it's linked together. And for compliance purposes, there might not be a ton of use cases around this for you. However, it is a really nice thing to add because the other thing is, you know, if you wanted to build just strictly a compliance pipeline, I want to dump data into some platform and keep it very cheap. I'm never really going to interact with it. Observe can be fantastic for that. However, there also is a snare. It's like, hey, you know what, my team, we also want to kind of really, we want to democratize our data. We want to, you know, do all of the regulatory compliance things and we want to put that somewhere, but it needs to be in the exact same spot that we're already collecting all of our other data. You know, so for like our developers and maybe the security team and we want to own one platform. And then that's where when you start looking at e
verything live together inside of here, not only is it super cost effective. You can also do some really cool things around that data once it's inside of here for your developers and your engineering teams. And we have some other awesome webinars that really go into depth about all the cool things that you can do. So I definitely recommend taking a look at some of those. But going back to compliance. I just wanted to show like how, you know, not only is the data going to come in here and it's not just doing nothing is that we also kind of create this navigational guide of your environment as well. So a couple things that I like to talk about, especially just, you know, just from a, a compliance standpoint is that there's going to be some really nifty things that you can do. So if I come into our log explorer right here, one of the options. So just kind of think of like like payment logs. Maybe I'm in an industry where we have payment logs. I can see I have clear text, credit card numbe
rs listed right here. The C, the C, C, B listed here as well. Now for all intents and purposes, more often than not, I don't even want to bring in this data to begin with. So a lot of things that we would do at the collection level is like, hey, what's going to identify these patterns? Let's just drop that from coming in. And that's perfectly viable. There's some scenarios that's like, well, I didn't even really know that data existed inside of there and observe can help you with trying to remove this data from the platform as well. But there's also a scenario where it's like, well, you know what, I need that data. I need those logs because I start to scroll over here. I also have like this nested payload inside of here that talks a lot about like, hey, what's going on with that transaction? And my team might need that to troubleshoot. But what they don't need is they don't need these credit card numbers. And this is where I brought up that flexibility that I mentioned earlier. You hav
e the ultimate flexibility to really kind of do anything that you want with the data inside of here. And I just don't really want to think too much about the schema. So one really nice thing that you can do inside of here and you can do at any point you want is that. So for example, but well, I'm seeing this credit card number inside of here. I want to publish this data set in a way that my team can use it so they can troubleshoot what's going on with these transactions inside of here. But they just don't need this. And so one of the things I can do is, for example, I'm going to come up here and go to actions. I'm going to say, let's go open up a worksheet and a worksheet for observe is a bit of a Swiss Army knife. You can create runbooks inside of here, troubleshooting workflows. It's a bit of a playground where I can kind of start doing adding additional data sets and making correlations. So it's just kind of a really nice area to do some of the things. But also I can I can start cus
tomizing this a bit more as well. And in the data right now looks exactly like I was looking at inside of log explorer, except for I can come in here and you don't give this worksheet a title if I wanted to. But the other thing that I could do is, for example, if I come over here and go to opal and this is the observed processing and analytics language. And it's going to bring up our query window here is now I could come in here and do something like drop column and I'll start typing credit card. And so say, okay, you want to drop the credit card number, but yes, but I also want to drop the CBV number. And now when I go run this, what it's going to do is it's going to go grab all that data. But now I don't have a credit card number. I don't have the CBV number, but I have all of the data that my engineering team or developers need to troubleshoot the issues inside of here. And all of the raw telemetry is listed here for me. And one really cool thing I can do is I can now come over here
. I could say, go publish this as a brand new data set. And we'll just say logs for developers or whatever we can create this naming doesn't really matter too much. You can name this whatever you want. And now when I hit publish, what's going to happen is that I can now go lock down my original data set payment logs. And we could say that maybe there is a compliance team and maybe they need this information and they can come in here and access it. But the other team comes in here and accesses the developer logs so they can do their job. But it creates just a super flexible way that I can drop columns and I can manipulate the data and I can do things like that at any point I want. And I don't have to think about re-indexing. I don't have to think about schema. I can apply schema on demand whenever I want inside the platform and use roll based access control to go lock down those data sets. So that's one thing that I thought was really nice when I was interacting with this. And then we a
lso have the flexibility. For example, I could come inside of here instead of doing this. I'm like, well, I really don't want to drop the column. Maybe we just want to redact the payment logs inside of here. And so one option that we chose in this avenue is instead of dropping the column is doing more of a hash. So just go hash that credit card. And so now inside of here, I can see it's just kind of a random number that I'm taking a look at. And so that's a nice option here for as well. It's like, okay, I know that there's a credit card number inside of here. I can't see the credit card inside of here. And also maybe you want to do something more like this where we're just more doing like a pattern replace. Like, okay, that CVB number just go put stars over top it. I know it's a three digit character. I know it's going to be some combination of numbers. Just go find that and put stars over top of it and then go publish that as you brand new data set. So just really flexible and being a
ble to do those things inside of here. So, you know, for compliance use cases, those are just some common ones that come up. It could be, you know, HIPAA related data. It could be credit card related data. You can kind of really think about all the different scenarios. But it's like, okay, I really want to drop this column. And here you guys can go use this or I want to redact something. All of those capabilities are inside of here. They're all built in very easy to use and be able to publish these new data sets for whatever use cases might be. The last thing that I wanted to show for you today was the fact that I mentioned, okay, we don't have tiered storage. It's all hot. It's all ready to go for you. And it all interacts the exact same way. So what I want to do is go back into the data streams that I showed just a little bit ago. And if we come inside of here, I'm looking at so the first time I received data from some of these four months ago, three months ago, I want to find so six
 months ago. So my Kubernetes data set, there's data stream that's coming inside of here. We started ingesting that data about six months ago. And if I open this up, it looks like that I get about 5.2 million data points per hour inside of my Kubernetes data. So a pretty decent amount of data. And what I want to do is I'm going to say, hey, I want to go bring back all of it all six months of that data. And I want to go interact with it because maybe something came up or an auditor is asking me a question. And it's like, hey, just show me that you're collecting this data. I want to go maybe do some certain things with it. So what I'm going to do is I'm going to just say, let's go open up that data set. And it's going to open it up into something that's a very resembles the worksheets that we're just interacting with a second ago. And by default, you can see it's like, okay, well, here's your past 15 minutes of data. And if I look down here at the bottom, that brought back 1.2 million lo
gs and one and a half seconds. So what I'm going to do is I'm going to say, well, you told me that the data goes back, you know, it's six months ago. So what I'm going to do is I'm going to come inside of here and date range. And today is actually it's going to so with today's the 14th. So what I'm going to do is I'm going to say, well, okay, let's go back out. Let's go back this out and let's go to 23 and let's go to December 1st. And let's go see what data is available all the way back there. And if it's not available, there's just no data in it. It'll still return. But I'm going to say bring back everything. Let's go do that. And so now what it's going to go do, okay, so here's our results. So we're already done. 12.2 billion logs and 2.6 seconds. And I can see down here, you know, minus 21 weeks ago, minus 17 weeks ago, I can kind of see a little bit of a time series chart inside of here. But what I'm going to do is I'm going to say, I don't believe you. I don't believe that I have
 that data and you brought it back that quick. And what I'm going to do is I'm going to come over here and go sort earlier to later. And once I come inside of here, so December 1st at midnight is that we can see if I go open up my raw data right here, just like I showed before. And I start tabbing my way through here. This is the full fidelity raw data as it was sent inside of here. So it's not just metadata. We're able to interact with very, very old data just as effective as it came in this morning. And the other thing too is I have all of the same capabilities as I did with any of the other data. If I wanted to say extract from Json, for example, I want to go grab some fields from here. It's going to drop me into that playground again. That Swiss Army knife, the worksheet. And it's going to say, well, what do you want? Maybe let's go grab that container ID, the container name. Let's go extract some fields from the object, maybe like the API version, whatever. Which is go pick some r
andom ones here and hit apply. And now it went and built that queer language I was talking about. And all of those fields that I extracted are now listed right here. So container names. I can come in here, start doing filters off of this. I only want to look things that related to Kafka, maybe. And this is all interacting off of data that has been in here for five plus months. And it's in its raw format and it's doing it the exact same way as data that showed up this morning. So I hope you found this informative and maybe shed some light on some of the ways that we can, you know, maybe help you improve your regulatory compliance logging. Make it easy for you. Make it cost effective as well. But then also be able to interact with it in a much easier fashion than, you know, just dumping it into something like an S3 bucket. So, hey, appreciate it Grant. And back to you. Thank you Keith for the excellent presentation and demo. We encourage everyone on the webinar to go to observing.com and
 request a demo or sign up for free trial. You're also welcome to join us at any of these upcoming events as we will be at the DevOps con in San Diego. We also have a technical hands on observability workshop on 530. And don't miss us at the snowflake summit for our breakout session on building a modern observability platform on snowflake. This concludes our session and thank you for your attention.
 Like any leader and being a leader in this organization, when you have a bunch of engineers that have to use a tool, the easier it is to use that tool saves the company really a ton of money. And we move so fast here. Like it's a rocket ship over here. We're moving so fast. It's crazy. And we didn't have time to have to devote really learning observe. It was really easy to learn right out of the box. Another thing you guys have incorporated into your observability tool is AI. I can ask a question. And instead of it giving me a link to a document, it takes the steps from the document and ju
st prints them out. I thought that was brilliant. We want to do that. Troubleshooting is great. It's fast and easy. And it doesn't matter what part of the organization is doing the troubleshooting. Everybody has learned how to do it in a very similar way. It's really nice. One thing that did happen, we did have an accidental data spike. Like our data went from two or three terabytes to data, 10 terabytes a day. And the system didn't even hiccup. It just kept running. It always had. The speed was there. Everything was there. But one of the things that is the most special part about this product I have to say is your customer support is beyond stellar. It's amazing. But my big fear was going to be the cost. Because we had shifted to Kubernetes. I know the logs and Kubernetes are just huge. When you go from bare metal to Kubernetes, your logs triple and quadruple. And when we looked at the price, I really couldn't believe it. It was a fair price. It would easily ramp up our ability to do 
observability. And it gave us some really great tools. All of these things have really married me to this product. I can't see using another observability tool. It gives us everything we need. The support is just stellar. It is first class.
 Tax. Yeah, I fix AWS bills for a living. Let me tell you, tags are the only way to get meaningful signal out of that entire mess of nonsense. Who tags appropriately? Absolutely freaking nobody does. Because that's a job for computers. Who set the computers to tag appropriately? That's right. No one has. So it means that you're trying to wind up sifting through this giant universe that's constantly changing and then yelling at people because the tags aren't correct. It's a terrific way to wind up making observability something everyone can hate if you're basing it all on top of tags because it's not only expensive and demeaning, it also doesn't work. Tags. So in case you couldn't tell, tags really torque me off. Now, let's talk to the observed found
ers about what torqued them off so severely in their previous corporate lives that they felt they had no choice but to inflict another observability product on the rest of us.
 You just want to show your life and no one else can stop you You want your life, it's your life You enjoy your life, so it's gonna stay the way It's your life, we'll try to enjoy our lives And you ain't different baby You want your life You can change it As long as you can stand there It's the same girl When no one else can hurt you It's your chance now Before you find it still, let's see The heart we dumped is stopped, just can stop you Between them, like a rash, just can stop your breath I can't have a track, only to see you The heart we dumped is stopped, just can stop you Between them, like a rash, just can stop you They're gonna have a track, only to see you Welcome to the Observability Trends webinar series I'm Grant Swanson, your host And today we will explore the topic of real-time incident detection and
 resolution In Kubernetes environments We'll kick off with a presentation that will challenge your perspective Followed by a live product demo of our observability cloud After the product demo, we will jump into questions and answers Feel free to type your questions into the questions window And anytime during the webinar We welcome everybody on the session today to visit our website And sign up for a free trial Upon signing up, our data engineering team will grant access And remain available for any assistance needed Whether it be related to data ingestion or addressing specific use cases you aim to solve Now I'd like to introduce our guest speaker, principal solutions engineer, Brian Fisher Welcome Brian Thanks Grant Before we jump into a demonstration of troubleshooting in a Kubernetes environment, Let's quickly review the challenges of observability in these environments I think that most would agree that the move from physical servers to virtual machines to containerization Has bo
th changed the paradigm of computing and exacerbated the observability problem The hyper scale, ephemeral nature of containers And just the challenges of getting toometry from containerized environments like Kubernetes is well enough And that's before you add in additional complexities of cloud-based environments And the new serverless computing paradigms that are provided there Sure, it was difficult before having metrics logs and traces and separate tools But that becomes even more difficult as you now add the hyper scale and ephemeral nature of Kubernetes environments Which you really need is a unified approach to observability Which leads up to what a complete observability solution should look like in Kubernetes environments First and foremost, an ideal observability solution will provide all of your observability data in one place in one data store At full fidelity to address well those feature unknowns Sampling and aggregation just causes problematic issues as you're trying to t
ake a look at data and identify unknown issues that are coming up It would also do this with long retention rights so you can leverage the data for things like seasonal analysis, baseline, and trend analysis Having data for two or three weeks is really not enough to accomplish these types of tasks either Second, an observability solution should provide flexible, extensible ingest and schema on demand Now why? Well it's because this provides you an ability to enhance your insights on the data even after it's ingested With the hyper scale of containerized environments and while the variety of development teams were usually involved in developing microservices architectures It could be difficult to enforce perfect standards for logging or telemetry You often need to address these inconsistencies even after ingest so that you don't lose the visibility It's also important to bring in contextual data which can provide key information about your containerized environments such as events from 
your CICD pipeline, Jenkins builds or GitHub information or information from your service desk This helps provide key context when doing correlation and analysis Correlation and automatic linking of your data is next With containerization especially in an environment like Kubernetes where things can be very ephemeral It's critical to be able to correlate observability data both temporally so you can see what's happening at specific times to specific containers But also relationally so that you understand what containers are running for each pod on which nodes All this needs to also happen automatically in real time on the fly so you can troubleshoot an incident quickly and in context This will allow you to resolve an incident much faster Lastly, as you're choosing solution in this space, it's important to look at solutions that provide out of the box opinionated content That can provide insights into your Kubernetes infrastructure The reason is that really this type of out of box conte
nt is as important as the flexibility of the platform And that's because you want to accelerate your visibility into the Kubernetes infrastructure so you can get the value from the solution, frankly, day one You also want a platform that provides actionable insights and alerts that aid in your root cause analysis The last thing you want is your observability platform to overwhelm you with information It should provide you with key insights about your Kubernetes environments and then allow you to leverage this discovered relationships that are there to aid you in the root cause analysis and really prioritization of the issues Well, with all those concepts reviewed, let's dive into observe and look at how this can be accomplished in a live environment So as we get into the demo portion of the session today, I wanted to take a minute to talk about some of the key concepts around the observer platform And really to discuss how they address some of the requirements that I talked about on th
e earlier slide, which is presented Let me switch over to our data lake you very quickly before we talk about some of these concepts We believe that observability is a data problem. In fact, we've based our solution from the ground up on a data lake, in this case, snowflake And what this provides is really an ability to separate storage from compute. Of course, what does that mean to you is you're doing your observability on your Kubernetes clusters? Well, what it means is that you can send us all of your observability data, your metrics events, your logs, your traces, and store all that data at incredibly low cost Observe will ingest that data, compress that data, and store it into S3 buckets full visibility, full fidelity, and do so at up to 13 months for data retention or frankly more if you wish Of course, by separating storage and compute, it also allows us to optimize the compute that's required to store, analyze, and frankly present that data to you So that we can provide you wi
th tremendous performance, even with petabytes of data coming in per day, and still keep that cost of analyzing and presenting that data to you low Now you might be wondering, well, how does observe capture data from your Kubernetes clusters? Well, observe is leveraging all open standards based collection techniques such as permethias, open telemetry, fluent bit, et cetera, et cetera, to capture your information from your Kubernetes environment. These are all easily deployed by home charts and deployed as Damon sets in your environment. Now once that data comes in, observe will go ahead and take the stream of data coming in and curate that into a variety of important data sets and more critically add correlations and relationships for you automatically This object view that we have here, in fact, when we focus on more of like a lineage view and then take a look at Kubernetes gives you an ability to kind of visualize this in an easy way. We can see as that data comes in, we're going to 
take that raw data and break it up into the different pod metrics, container logs, API updates, et cetera, et cetera, and then not just create these different data sets, but also create the different relationships. So for instance, let's take something like a container here is we focus on container, we can take a look at the variety of different relationships that container has with other parts of your Kubernetes environment. This is nothing that you need to be aware of. Observe will automatically take your speed stream of data in and then correlate it into the variety of different data sets and create these relationships. You can even follow these relationships and we can do so really quickly here if we wanted to go from container to pod and then pod to something like node, you can see that not only are we keeping track of the different relationships for each of the different components, but as we follow the different links in the environment, we're also following the bread crumbs as 
we go from container to pod pod to node, et cetera, et cetera, understanding those relationships exist. Now as we get deeper into the actual demo itself, you'll see how these things relate to doing troubleshooting in analysis and your environment. Now there's one set of concepts that did want to also address before we get into the actual troubleshooting and scenario within the environment. And that's really to talk about how observe presents data. We've got two different areas in terms of how we present data applications, which are those opinionated views on data that allow you to get insights automatically in an opinionated way and then our investigate views are explorers, which allow you to take a look at your metrics logs and traces and do so in an unapunited way. As you're exploring your telemetry information. Now, of course, we want to start off with the applications you so I'm going to switch over to applications here. And as you can see observe has a variety of different prebuil
t applications that are within observe and of course a whole set of a catalog of additional applications that you can install based upon the environment that you have in this environment. We can see that we are running Kubernetes within AWS. We have a number of EC to virtual machines so we have host monitor during install as well. And we're also monitoring with open telemetry. Now as we take a look at something like Kubernetes, if we were a cyber reliability engineer, that might be a place that we live and we might want to take a look more at that content. Before we jump into the actual content itself, let's view the variety of content that's actually provided. So for Kubernetes, we provide a number of different prebuilt monitors and alerts to alert you on any of the different issues that are occurring within your environment. And also again, curate the data that coming in the from Kubernetes into different data sets and provide a variety of different prebuilt dashboards and opinionate
d views for you. If we go ahead to open the application, we could of course open the application directly. It will take us to that landing page for Kubernetes, which of course is the high level view of the clusters year environment. In this case, we just have one cluster in this environment. It's running a simple application we call our cars application that's providing effectively an e-commerce site for you to access and order cars from online in the environment. Let's take a step back here and take a look at this environment and imagine we're a site reliability engineer. As an SRE may be responsible for Kubernetes, we would want to get critical information about the Kubernetes infrastructure and also be made aware when issues are occurring. Of course, this landing page is providing just that. It's showing you the number of nodes and pods and deployments and namespaces in the environment, as well as the high level metric information, the CPU memory utilization, the number of throttle 
pods, unhealthy nodes, etc. And of course, it's going to identify when things are unhealthy as well, because this is a pin and opinionated view. It's telling us here that we have a number of unhealthy pods in the environment, and we might want to drill down and identify what's going on with the SRE and healthy pods. So, if we were to have a summary that's responsible for this infrastructure, that might be a key thing that we would do within our day to day, although I wouldn't necessarily expect them to be staring at this dashboard 24 by seven, but we can talk about alerts and events in a minute. So, here, let's say we were in this dashboard and we did want to drill down in these unhealthy pods, all of the views within observe are actionable. In fact, we can see a further listing of the unhealthy pods down below here. And if we wanted to drill in and start doing analysis on these unhealthy pods and frankly just these unhealthy pods in this environment, we can click up in the upper right
 here and say that we want to drill down into that pod data set to get more information. So, what we can do is we land within the dashboard itself. It's going to very similarly provide us the high level information, not about all pods, but of course, contextually, just the unhealthy pods that we were drilling down in and not. In fact, if we take a look here, we can see that right now those pods themselves are not actually running in this environment. And this, of course, is due to the temporal nature of observe in monitoring these pods, which in this environment could be really a failure on the environment. So, if we actually drag the time scrub over, we can see when specific pods are running. And see the change. So, it looks like in fact there was maybe a deployment that happened here earlier during the day, about two and a half hours ago. And then of course, if we go down a little bit further here, we can see key information about things like the memory, the CPU usage and the environ
ment. And we can take a look here and see some somewhat problematic patterns in this data where we're seeing that typical saw to pattern. And we're also seeing over here a restart count continuing to take up in this environment. So, it looks like this pod after the deployment is starting to have an issue when we're starting to see restarts in the environment. In fact, if we wanted to visualize this from a temporal perspective, we could take a look and say instead of taking a look at which ones are active, we could take a look at restarts count. And again, grab that time scrubber and see when the specific pods were running. And of course, as we drag along, we can see the pod going rather and rather as the restart count is taking up here higher and higher. Of course, we've identified that there are some issues in the environment, we see that restarts are happening, but these metrics are great from a high level perspective to understand that events are occurring or specific metrics like r
esources might be impacting that. One thing we know, however, is that this isn't even approaching the amount of memory or CPU that's allocated to this system. So we not know it's not necessarily a quote unquote resource issue that's causing these restarts to happen. So we might want to do is drill down a little bit further into the associated telemetry data about this environment. By clicking over to logs and events, we can take a look at any of the additional telemetry information that's related to this environment. And of course, observe because of these graph links relationships are going to take and bring in any of the associated data sets, the application logs, the container logs, the web logs, pod updates, events that happen in environment. So there are frankly notifications that observe itself has sent out. They're all here at your fingertips to take a look at that information. And in fact, if we take a look at those notifications, we can see us in fact, we do see errors in aler
ts that were sent out by observe maybe into your Slack channel about plots frequently restarting or errors in the container logs where this is something we might want to drill into. And of course, we're already notifying people proactively of this issue in event. Since we're already here, instead of drilling down in notifications, it's where we see the pods are frequently frequently restarting. We can drill directly into the container or application logs and start doing analysis on any of the logs that are in this environment. So for instance, if we wanted to take a look at the errors in the container logs. We can go ahead and filter and see any of the errors that are occurring in these container logs quickly and easily to see what's going on in this environment. What we do see here is that there are a number of different errors. It looks like we're seeing an error here about a web application that was stopped threads not being reviewed over time and potentially a probable memory leak.
 We know that memory isn't necessarily an issue. We also see some other errors and exceptions here from the application itself. In fact, we can see that these are all from the app server container that's running within the pod. And we can see some specific issues around span IDs and exception messages around the errors decoding car of the way. Now, of course, these are the container logs. We may have actually pulled the application logs from these. In fact, we have so if we switch over the application logs and filter for errors here. We'll very likely see those same errors and see them in a much more process context. So we want to see errors. So it doesn't contain errors. Sorry about that. So if we drill in here, we can see now, yes, these are the errors code and cars array. We're seeing the specific user session, the span and trace ID and in fact the actual stack trace. Now, as a site reliability engineer, I may or may not be a developer and may know may or may not know what this spec
ific exception is happy is and how it relates to the specific service. But if I needed to involve others, I could take this data. Maybe I want to go ahead and copy this link to this, including all the data, set that up into my Slack and then invite additional developers to kind of to really and on the issue. Now, a developer themselves could even take this context automatically open up this copy link. We'll take a take you directly in context to that specific view that I the SRE was viewing so they can do deeper analysis on the specific error. Now, of course, we have a potential would cause, but as a developer, we want to go further again, we might want to use something like a graph link and drill into the actual trace or the spans of those transactions. And we have a deeper information about the specific error, where it's happening in context of what transactions happening. And we of course can drill into that data within this specific transaction and get deeper information. Now, of c
ourse, I'm going to stick with being in SRE and one of the things that we talked about earlier is, you know, we might want to include others in the analysis of this. And of course, it'd be very helpful to understand who, what developers are either working on this code or frankly, since we saw that the deployment, trying to understand what builds happened and what builds were rolled out in this environment and by who who made the commit of the code. Because let's face it, about 50% of the time it's been proven and also reviewed this again and again that changes are often the root cause of issues in the environment. Now, of course, within observe, graph link is incredibly powerful, so we could actually ask this question. If we wanted to, we could go up to graph link here and say, hey, I wanted to see all the builds in this environment. That happened in the last four hours and see during that roll out what were the builds and who was involved in that build process, who were the developers
 that made commits or changes. Now, you might want to stop me and say Brian, wait a second, graph link in the linking is wonderful, but how did you get from problematic pod to the builds that were happening in that. Well, as I mentioned earlier, we're monitoring a variety of different to our training this environment. And if we take a look at the graph link here, we can actually follow the breadcrumbs of how we got from pod all the way to builds. We see that pods are related to containers, containers have images images have a Jenkins image Jenkins mapping, which is basically how it's being deployed as that image in the environment. And of course, Jenkins does the bills, so we have the build information. Now, this has done obviously through an integration to Jenkins to bring this information in, but this is all correlated automatically for you. So when you want to ask the question, instead of going to Jenkins to find out that information, you can go right with an observed use graph link
 and go right to the specific builds in this environment. And here we see the builds. In fact, we can see that this is a small change to caching code. Now, we might want to take it a step further and understand, well, caching that certainly could relate to an array within this this environment. We might want to take graph link one step further and say, hey, let's see who made the commit to this so I can include them in the analysis of this specific recalls. So now not only do we have the recalls, but maybe we can bring in the specific resource that made this change and understand, is it impacting in this this definitively the recalls, is this the small change of caching code. Well, here is the specific commit. We can see Tom bachelor was the author who actually made that commit. In fact, if we wanted to, we want to get really interesting, we can even follow this and take a look at the code changes within the GitHub environment itself to take a look at the code. Now, I want to take a mi
nute and switch over to some of those explorer views so you can get an understanding of how you would work with your telemetry data and let's call it a more unimpeded way, as well as address some of those key criteria that I talked about earlier around the flexibility of ingest and the ability to work with schema on demand. So we kind of take a look here, I'm switching over to the container logs within our log explorer view. We've got a number of different container logs and of course we have the ability to filter down in and in fact, let's take a look at how we would process these container logs to create, let's say an app server log out of the container logs that are coming in. So we have the log data coming in here, we have span and trace data, we've got a whole host of other things that are coming in from the actual container logs. We might want to actually extract some of this data to leverage sort of schema on demand to take some of the information, important information we might
 want to add here. So let's go ahead and let's grab and extract the span, the trace ID. Let's extract the method. The level. And maybe the session ID. And then maybe the message as well, those are some some key fields that we might want to pull out and extract from the container logs as we're creating sort of like our subset of that in the application logs. Now you can see all that data is pulled out on demand. And then we can go even further however, let's say for instance we have session ID, but we want to pull the session identifier over from the actual session ID table. Well, we could go deep into this into towards creating a new data set by just opening this up quickly in a worksheet. From our explorer view and in this worksheet, taking that ID that we're looking at here in session ID, saying hey, we want to go ahead and pull in data and link this from another data set. In this case, the user session data set coming over from the web server. We want to link based on the session ID
 because we have a session ID and both hit apply and now those two data sets are linked. And that graph link relationship, but we've actually pulled over the session identifier, the user ID from that user session table quickly easily. It's here if we wanted to pull other information from that table that session ID table, we could go ahead and bring in additional data, let's say for instance, someone at a related fields. And for username not only do I want the username, but I also want to know the country they're coming from. That easy. Now I'll add into this new sort of data set that we're creating here. We have the actual user. Country as well. So we now quickly have that information go even further into creating these links. In fact, we can even use compound keys like let's say we want trace ID and span ID and we want to go ahead and relate that to the span data set. Trace IDs trace ID span ID span ID hit apply and we now have that complex join to that span data set. So it has to hav
e that trace and span ID and now we have a direct linkage to the open telemetry stand data. And that's how quick and easy it is to create your own custom correlations here within observe using schema on demand. Now, of course, the log explorer and the other explorers are just good for creating and working with schema on demand. Also, of course, very actionable for you to be working with your data. So for instance, we have something we're filtered down to ask for your data. We might want to do the same thing and take a look at errors in the environment. We hit run. Now we're taking a look at just the error messages or the error logs messages that are coming in from this environment. Now, we might want to go ahead and take a look at from a different perspective, like maybe visualize this data, being able to see that data over time. In fact, let's take a look not just the last 15 minutes. Let's take a look at the last 24 hours of data. So taking a look at the errors that are happening ove
r a time. In fact, we might want to even go further by saying, hey, we wanted to have this broken down by container. In fact, so we don't just want to take a look at the overall error trends here. We want to actually have it broken down by container. I'll hit run now and now we'll have it broken down by container as well. Great visualization we might want as we're doing or creating a dashboard. And of course, once we have that visualization that we were looking for here, let's say this break break down by container on all of your error messages. We can just quickly add this right to a dashboard. So this would be a great start to a custom dashboard and custom dashboarding is very easy within observe. Now, maybe we don't want a custom dashboard. Maybe we want to actually use this data instead of a monitor for creating again those contextual events and alerts being able to drill down in. Creating a monitor is equally easy. You can just say I want to create a monitor. And now we have that 
great trend data that preview of that data and start creating a monitor. Let's say, for instance, during the last five minutes, if we're seeing more than, I don't know, say 30 errors during that specific time frame. I want to understand and understand across the last 24 hours whenever that happens. Identify that and show me when that alert would be fired. In fact, we can see that visually represented above. We see the actual visualization. We see when the errors and events happen. And of course, down below, we can see the specific events when they'd occurred when across threshold. This is a little noisy, but maybe we're okay with that. And of course, we would set up a new action like sending out an email or a Slack or sending an event, a major duty when these things occur. And then we're often running, saving off this alert for again, getting that contextual and set for information when again errors or events are going over as well. We'll stop right now because that was quite a bit. We
've gone through the full demo. We want to see if we have any questions. Sure. There we go. Thank you, Brian, for the demo and presentation. I'll take a look at some of the questions coming in. The first one I see here is do you support other cloud versions of Kubernetes like Azure Kubernetes service? Yes, we absolutely do support really any version of Kubernetes that you have from the different cloud providers to on premises to even if you have something like a, you know, a commercialized version of Kubernetes. Excellent. Can we just one moment? The next question I see is how are you pulling in information from Jenkins and GitHub? So we have again, as I stated earlier, those applications, they have a set of integrations or pollers for something like a Jenkins and GitHub to actually pull for those pieces of information to build information, the commit information, etc. So you're bringing that important contextual data from your CICD pipeline. So it's a pretty easy and simple setup. And
 then you can bring in that data and have it directly correlated into the rest of your telemetry. Awesome. Give me just one moment. This question is saying we subscribe to configuration as code. How do you support that? And can we use terraform to deploy your agents? Yeah, and so observe also as a big believer in configuration as code. So I mentioned earlier in terms of deploying any of the agents in your environment or any of the collector in collection in your environment, you could use something like a home chart because Kubernetes. That's a main way for deploying within the environment. But we also provide terraform providers for deploying the same agents that's right into the environment. And again, we do subscribe to configuration as codes. If you want to store any of this configuration in like let's say your GitHub repository and then utilize something like terraform for deployment, that's something it's 100% supported. Excellent. If anybody in the audience has further questions
, please type them into the question window. I'll give it just a few seconds here Brian, as I think we've got the three top questions answered. Looks like that's it for the questions. So our next webinar will be enhanced security through observability with the observed basic threat Intel app. This webinar will occur on December 7th at 10 a.m. Pacific time. This concludes our webinar and I'd like to thank everybody for joining us today. Thank you, Brian. Thank you.
 Imagine it's 3am and you get paged. You open a dashboard and see a giant spike. It should be easy to get to the root cause from here, but it's not. The spike is actually a dead end. When we built our dashboarding feature, we did not want to build another single glass of paint. Observe understands relationships in your data, so our dashboards do too. When you see a spike, you can click on it and actually figure out what it means. Whether that's a node, user, log, metric, or whatever else you have linked in the system.
 Welcom
e to Observe Dashboards. From the Explore menu and Observe, underneath the Dashboards tab, you'll see Dashboards from Observe apps and also Custom Dashboards from Observe users. Let's create a dashboard together. We'll do so from the top right and click New Dashboard, which will bring us into the new Observe Dashboard Editor. We'll click Add Card where we get several options, and I'll select a Dataset table, so to search for a Dataset, which will allow me to add a stream of logs to my dashboard. I'll add the Container Logs Dataset, and then the Dataset card is added to my dashboard immediately, from which I can click Add Card again, and then select a metric visualization where I can search for a metric related to my container. I'll look at Container Memory Usage, select that, and then it is also added to my dashboard. Now, if I want to edit each of these cards further, I'll select the Pencil icon to edit the card, and then when brought into the Dashboard Card Editor, I am focused in on
 a single card, so to simplify which visualization I am editing. From here, I can title this card, and then I can also make use of the Opal Console or various different table controls, so to customize this Dataset table visualization. However, once I'm finished, I can go up to the top left and go back to the dashboard, and see my changes. Next, I'll edit the second card by clicking on the Pencil icon again, and here, for a metric visualization, I have options to change that visualization and configure it, or change the underlying data that is being represented. Now, if I want to go back to editing my Logs card, I don't have to go back to the dashboard, but I could use the editing card, drop-down menu, to quickly switch between cards. I can also then go back to my dashboard, and optionally choose to hide any given card that I don't want to display. I can see an undue option from here in the bottom left, but if I hide this card, and go back into the Editor mode, I can see in the editing 
card drop-down menu, all of my available cards, including hidden ones, which can easily be added back to the dashboard. Once you've finished editing all of your different stages, you can go back to the dashboard and title it as well, and once you're finished displaying the cards in the order that you would like, you can click Save Changes to publish this dashboard. If I leave the Editor, I'll see the dashboard, but I could also go back to the Explore menu, and under the Dashboards tab, search for my newest dashboard, and then select it from the list. And this concludes an introduction to the new and improved dashboard editor in Observe.
 Hello and welcome to the Observability Trends webinar series. I'm Grant Swanson your host and today we will be discussing our latest product launch Trace Explorer. We'll kick off with a short presentation, followed by a live product demo. Feel free to type any questions into the questions window at any time during the webinar. All questions will be ans
wered via email immediately after the session concludes. Everyone who registered for the webinar will receive a link to the recording and a link to the upcoming workshop. Now I'd like to introduce our guest speaker, director of product management, Rakesh Gupta. Welcome Rakesh. Thank you Grant. Hey everyone, I'm Rakesh. I'm the product manager for distributed tracing here at Observe. And I'll be talking today about what distributed tracing is and what it's used for, how observed handles distributed tracing data and how you can get started with tracing in the Observe platform today. So a little bit about me. I'm based in the San Francisco Bay area. I have a little more than a decade of product management experience. Most of that is in observability. Most recently prior to Observe I was at LightStep for almost three years, both free and post acquisition. LightStep is a distributed tracing product and for those who are not familiar, it was founded by the same founders as open telemetry. Pr
ior to LightStep I was at AppLanamics for about four years. Again joined free acquisition and stayed on after the acquisition. I initially was working on the dynamic language agents. So that was known in Python but also expanded to Ruby to go as well. I did that for a couple of years and then moved on to the APM platform at AppLanamics. So extending the data model so it would work with more distributed systems and more modern environments and architectures. So overall I've had the chance to work really closely with dozens of customers in a variety of architectures, a variety of industries as well and to really help them realize the value of tracing data. But also when not to use it, when is it not a good idea to solve a problem with tracing? And also just to help them solve business problems with it. So let's kick things off with an overview of what distributed tracing is and who uses it. So I've seen a variety of personas that make use of tracing data but there is definitely one that 
stands out and that's developers. So the typical scenario is customers are saying the experience is slow or they had an error but the user requests coming in are flowing through dozens or hundreds of microservices running on all sorts of architectures. If you're trying to debug a bad request, the questions you're answering are where is the issue? Where did this fail? Was it upstream of me? Was it downstream of me? Is it my own code? I've worked with dozens of customers where these are the needs that their developers have. And I've also heard from customers that adding tracing data or answering these questions with tracing data can cut down the meantime to resolution from hours or days to even as low as 15 minutes just because it brings together so much contacts in a single place. And you can use tracing data to model business services and business objects which other folks in the org outside of developers may find useful. But that's for another webinar. So I say a tune for that. So if 
you're new to tracing and especially for coming from a logs background and you are super familiar with traces, no worries, this is going to give you a baseline of knowledge. So a trace is a representation of a single end-to-end user request flowing through a distributed system. The trace is made up of a bunch of spans, each of which has the trace ID. And each span represents some bit of work done by some part of some microservice. And it has a bunch of metadata saying what work it did, how long it took, what resources were involved in the doing of that work. And you can kind of think of a span as a log with a trace ID on it as well as some context about who is pyramid. And so that's really what the trace instrumentation is doing that makes it distinct from pure logging is it's propagating that trace ID across the whole request lifecycle, creating spans and adding attributes and that trace ID to them to make them useful for debug them. And again, this is the very basics of distributed t
racing. There's much more to unpack, but this should be enough to follow along for the rest of the presentation and also just to get started with using distributed tracing at observe. So let's talk about in a little bit more detail how are people actually using tracing data. So let's start on the left with number one, looking at an individual trace. So this particular request went bad. I want to inspect it in great detail to find out exactly what the call structure was, where the time was spent, what services were involved, what infrastructure those services are running on. I need to get to a narrative that tells me exactly what happened with this request. And trace data is a fantastic place to start for that. With the trace data, you can say, if flow through these services, it hit this endpoint and then this service and then for example, it made a thousand database calls or it made one database call that was very slow. Or it called an external API and that was slow like your payment p
rocessor or it hit an inefficient code path or we ran two things in sequence that could have been run in parallel. Or all of these things happened in a single request and it was just death by a thousand cuts. And that's great information because now I can go and debug this request and I can make it faster the next time somebody calls it. So now we go to the right for number two, which is basically searching across all the requests coming into the system or just to my service that I own as a developer and refining that search to figure out what groups of requests are slower or more error prone than others. Maybe requests with the latest version are slower than requests with earlier versions or requests that ran on a particular Kubernetes pod all errored out but requests that ran on all the other Kubernetes pods are fine. And really what you're trying to do is form and test hypotheses about which of these populations of requests has the issue so you can narrow down to just a small set of
 traces and look at those and really start to understand more deeply what's going on. And there are other needs that that users have when it comes to distributed tracing but these are definitely needs that ranked at the top and these are really key uses of distributed tracing. So now with that sort of baseline knowledge, let's talk about observe and observe distributed tracing solution. So how do we handle tracing data? So the first thing to note is that we fully support open telemetry and we have an API endpoint that you can send traces to that's fully compatible with open telemetry tracing data. So take anything that corresponds to the open telemetry spec for tracing and send it to that endpoint and we'll be able to accept that data. We also provide a number of tools and scripts to really just help instrument your apps and get that data into observed. And so some of these things are a guided setup flow in the product. So if you use our free trial, it's one of the very first things th
at you're going to see as soon as the free trial loads is a couple of different flows for basically just getting data in will help you, you know, as soon as you start. We also offer a Helm chart. So if you're in a Kubernetes environment, you can use that. We also have scripts and examples in GitHub that basically show how to use the hotel auto instrumentation, which exists for a number of really popular languages to get your app instrumented automatically with tracing without having to make any code changes and get that data into observed really easily as well. And so I also wanted to show some of the ways that we are making use of open telemetry data. You can see some of the tracing features that our product has and which hotel data is used to power that feature. In most cases, the auto instrumentation libraries for hotel are going to automatically collect this data. So you don't have to worry about collecting it yourself. But if you are using custom instrumentation, you can use this 
as a guide to plan your instrumentation in order to make the most use out of our product features to solve your use cases. You can also find this table in our docs and I'll just take some examples. So and we'll go through this live in the product as well. But if you are doing a search and you want to compare versions or you want it to filter just a particular environment or you want to see which populations of spans or traces are erroring, you can do that using the filters that we have built into our trace search feature and the actual data that you're filtering on basically comes from these open telemetry attributes. So now let's pivot over into the product and actually see some of these tracing workflows in action. So here we've instrumented the open telemetry demo application, which is an e-commerce service and it's sending its tracing data to observe into this tenant here. So we've got the trace explorer open. You just navigate to it via the left nav. And when you search for traces
 and observe, we return a list of traces that match your query as well as some summary stats about the traces that we return. So for example, how many spans we return over the query window that count of errors in the query window, which in this case is 60 minutes, the distribution of latency across all the requests in the window. And then we also have how the latency has changed over time. So you can use these graphs to basically hone in on the traces of interest so you can start your debugging. For example, if I go to this duration chart and I group it by version, what I'm going to see is that requests with version 1.6.0 are consistently exhibiting more latency than previous versions. And I might want to filter my query to just those requests so I can dive deeper into what's going on. So there's two ways that I can do that. I can either use the filter bar up here and you just go ahead and type in service version people to 1.6.0 and a lot of correct for me. That's one way to do it. Ano
ther way to do it is I can use these facet filters over here on the left and we can go down and find service version 1.6.0 and we'll go ahead and check that and then it'll automatically rerun my query to now just look at requests with version 1.6.0. So remember we saw higher latency so I might want to filter my list down here to just the slow requests and so I can do that by going back to the distribution chart and just scrubbing the latency distribution here, the higher end of the latency distribution. So now my list of results down here is going to contain just the higher latency traces. So let's now click on one to examine it in greater detail. So earlier today I found a good example of one to really take a look at here and now we're looking at a flame chart for this trace. And what a flame chart is, it's a visualization that shows a number of things. It shows all the operations, so all the spans that executed as part of this trace over time, as well as the parent child relationship
s from top to bottom of those spans. And there's a lot of information about this request that we can glean from this flame chart. So for example, what are all the services that were called in this request? We can open up the legend over here and basically see that that's what these colors correspond to. So we can just look at this flame chart and instantly see that it started in the front end. It went to the checkout service, checkout service made a call to shipping service later on, the checkout service made a call to the email service and so on. We can also see the full call chain and the sequence of events and any parallelism that has occurred just by looking at how these operations are laid out here in the flame chart. If there were any error spans, those would show up as bright red. We don't have any errors in this particular trace. Now let's go ahead and dive into one of these spans in particular and see what information we have about each one. So I clicked into this place order 
span and now we go down to this list of attributes. We can see all of the attributes on this span. So for example, the Kubernetes infrastructure cluster namespace and pod and also node. We see that here as well and deployment and so we can tell that this span executed in a Kubernetes environment, we can see that it was a GRPC call. We can see that it ran on Alpine Linux. We have the Linux distribution here. We can see that it ran on GOG that this is a GO microservice that this operation is executing on. So there's a bunch of information that we can glean from looking at these attributes. We also have span events and span events are kind of like logs that are emitted by open telemetry tracing instrumentation that are associated with spans as well. And in this spans events, we have a couple of custom events which give us more information about this particular request. So we have a tracking ID and we have a transaction ID. And this is the type of information that we can use to make other 
correlations with business context and other parts of observe. And if this was an error span, we would also have a span event here that basically contains the statutes for that exception. So you would just click on here and be able to view the full statutes. So let's go back to the attributes. And let's say I have a hypothesis that I've looked at a couple traces now and I think, well, this pod name keeps showing up. And I want to basically test to see, is it actually spans with this pod name? Is this pod name actually the cause of any particular slowness in my application? So the way that I can test this hypothesis is I can select it here and then I can click show similar filtered spans. And so what this does is it will open a new trace explorer search and it will just search for spans that executed on that particular pod. And now I can sort that population by latency. I can look at traces that just contains spans that contain that ran on that particular pod and I can quickly test that
 hypothesis. A second thing that we can do to continue our investigation is we can jump over to logs that are associated with the trace with this button up here, view logs related to the trace. And so what that's going to do is it's going to open up a log explorer search for basically any logs that have that trace ID in them. And we can continue to see in our log data, maybe these logs were not even emitted by open telemetry. If there are any other signals, maybe it's application logs, maybe it's infrastructure logs or anything that might be associated with this request, we can continue our search from there. So at this point, I have a pretty good picture of what happened in this particular request. I can tell the story of what happened. I can refine my search through the show similar filter spans. I can jump off to logs related to this trace to continue my investigation or I might even have enough information at this point to now go and debug this issue or go and optimize this request
. And that wraps up our introduction to trace. Excellent. Thank you, Rekesh. I wanted to highlight a few points about trace explorer. Customers of observed get industry leading retention durations where 100% of traces are stored for 13 months by default. There are no limits on trace duration. And you can quickly visualize all spans for a trace. As Rekesh mentioned, if you go to our website, we have a fully featured free trial offer that includes our long explorer, metric explorer, trace explorer, dashboards and alerts. We also have a pricing calculator available on our website. Thank you for joining us today as this concludes our session.
 Hello and welcome. My name is Kelsey and I work on our sales engineering team here at Observe. And today I'm going to walk through a demo of the observability cloud. Data is ingested into Observe using open source collectors and does not require certain schemas or structures, reducing operational overhead. Here you can see high level stats of the dat
a you have stored. There's about 70 or so terabytes in data in total, which includes various log data, permethiast metrics, hotel traces, and even contextual data like GitHub and Jenkins events. Of the data that is an Observe, about 30% is accelerated. Unlike other tools, you can access all your raw data at any time. However, acceleration is the process of curating data into datasets to facilitate rapid optimized queries. Let's take a look at Observe's data set graph, where you can see all the datasets built on top of the data light and the connections between them. At a high level, you can start to see the different out of the box apps we have deployed. For example, Kubernetes and AWS. If you want an observable system that enables you to reduce troubleshooting time and increase efficiency, then you need to be able to go from point A to point B without having to know the route. IE, you don't want your teams to have to have tribal knowledge to do investigations. Similar to a GPS, this i
s what Observe does. If I click on pod, you can see all the related datasets and all related traces that went through a set of pods. Or if I want to understand the Jenkins builds related to the pods that are restarted. No matter what questions you're trying to ask, graph link or these relationships between these out of the box datasets enables you to start anywhere and navigate to various layers of your text stack quickly and seamlessly without needing tribal knowledge. Let's show this in practice. Let's use the Kubernetes out of the box app as an example. Opening the app will drop us into the out of the box Kubernetes dashboard. This is an overall view of the health of your cluster. You are able to see high-level stats like cluster size and some other key metrics. If I scroll down here, I can see that there's some unhealthy pods. Unlike a lot of other dashboards and other tools, Observes dashboards are actionable. Meaning you can use graph link to investigate related datasets. Let's d
rill down into the related pods. Now you can see all the metrics for those unhealthy pods and the outlier metrics like high CPU and memory become more apparent. Over on the top and the activity tab, by default, you can see notifications which are out of the box alerts that have fired to specific teams via different channels like Slack or PageRDuty. But let's take this farther. Since metrics only tell us part of the story, often you want to take a look at other data like related logs. You can quickly pull up other related data, not just what is provided on the dashboard. Let's look at container logs. Notice that we didn't have to filter to the same time frame or the pods we were looking at at the dashboard view. It automatically keeps that context as you look at related data. Furthermore, I could seamlessly navigate to all the traces that went through these pods. By using graph link on this dashboard, I see I have traces sorted by response time. And there are some slow ones in here. So 
let's pick one and take a look. On the waterfall, I can see we started this trace in Engine X, then called down to microservice container in Kubernetes, which made some elastic cache and SQL calls, which then invoked a Lambda. Oh, we can see the problem here. That Lambda has made lots and lots of SQL calls. Pretty quickly, we have come to a conclusion that the car rating Lambda, that is the problem. We have some sort of SQL call loop. But what is that SQL call? Let's open one of these up. Here we have the span attributes, and I can see specific sequels that are being called. Further down this page, I get the tabular view of all the spans, and then below that, all the logs. We have a across this environment for this specific trace. Thank you.
 We continuously make efficiency improvements to our back end to save our customers' time and money when using observe. Some highlights from recent work. A 75-95% reduction in transform costs for resource datasets with semi-structured fields, that 
means JSON. Incremental streaming implementations of most aggregate functions that includes min, max, average, percentile and count distinct, which are found in many monitor definitions. Streaming aggregation reduces transform costs by large factor where applicable. On top of these two big optimizations, changes to how immaterialized monitor state and schedule monitor execution have led to an overall reduction of 25% in monitor costs across all our customers, with some customers seeing cost reductions of over 60%. And finally, a dramatic reduction of transform tasks queuing thanks to various improvements to our last degree source management. Previously, a bit more than 1 in 100 transform tasks was queued for 60 seconds or more, which was already pretty good, whereas now it is less than 1 in 100,000 tasks. The net defect is fresher datasets and monitors running and firing more accurately than ever before.
 Welcome to Observe. A great way to investigate metrics in your environment is thr
ough the metric explorer here available on the left hand menu. The first thing you want to do is search for your specific metric. Here I can search for my demo data memory to see related container memory usage metrics. Once selected, I can see this metric here visualized in the explorer and then I can use the expression builder to then search to specific fields like the container to specify my specific recommendation service. Up here I can select auto run or run the query myself. I can extend the time frame here with the time picker. But I can also drill into my visualization by clicking and dragging to then narrow in my search. I can also adjust the visualization settings and then if I'm happy and want to share this with the rest of my team, I can use the share button to copy and share a link which incorporates the exact state of this query. Lastly, I can use the actions menu to either create a monitor or add this to an existing dashboard or create a new one and start from scratch.
 S
ince releasing our dashboarding and metric expression builder features in May, we've been hard at work getting feedback from our users and making improvements. Let's review a few of the major ones. First off, the metric expression builder is now available in worksheets and dashboards. That means users can visualize complex, multi metric expressions without using query language. We've also added support for parameters in metric expressions, so users can easily make their dashboards configurable. Power users will be happy to know that the opal for their query is always a click away in the script tab. On the dashboarding front, we've taken a look at the things that users struggle with when creating dashboards. Textboxes and drop downs are great to put on dashboard when you know exactly the sort of filter a user might want. But sometimes, you just can't know that ahead of time. To solve this, we've introduced a new type of parameter that we call a filtered data set. The filtered data set p
arameter makes every field in a data set available for filtering. It supports many different kinds of filtering, like inclusion and exclusion, wild cards, numeric ranges, and more. This leverages all of the powerful context that observe extracts from your data. Lastly, we made a big improvement to our filter bar by adding first class JSON support. Previously, users had to extract JSON fields before filtering on them. With this latest improvement, users can access and filter any JSON field on the fly. This also extends to filtering and grouping in the metric expression builder. We're really looking forward to seeing the stories our users can now tell with these exciting new features. Thank you.
 Ugh, missed it by about 10% for the timing, which by observability metrics is pretty decent or so I'm told. We'll be right back after this brief message about a company we're legally not allowed to mention by name.
 I'm Tony Noons. I'm the North American and South American support director for P
roject 44. Project 44 is a logistics kin offering a great software to compile all your data and make end-to-end visibility for your shipments. And observability is a huge part of that for us. So to take something like observability and being able to dive into data dive into every piece of data that we have is really what our business is about. We wanted to make things simpler, clearer, easier, friendlier. Tracking shipments, I know it seems really simple from the front end. You look on the website and you find out where your bathroom is or your Wonder Woman cake pan. But you're ultimately stuck with all this data that's happening behind the scenes. Project 44 is an organization. Compiles that data normalizes it for customers. But for us looking at it and support, it's not normalizing customers. We have to kind of do that on our own. One of the biggest benefits that we see is turnaround time. And resolve time is everything in support. Being able to find access to the pin-pointed data th
at you need to understand is this problem with us. Is it with a carrier? Is it with the API? Is there a communication problem? To be able to locate that quickly is the difference between a happy customer and a sad customer. One of the new rollouts of the dashboards that that observe put together really helps staff starting from square one. We're able to build dashboards that say, here are the pieces you're going to need to know about this shipment. Here's your API log, here's your server request, here's your client request, here's all the data that you need to assess the shipment for the customer. The possibilities for observer are really endless and it's what your imagination can picture with that data.
 CEOs realize that their customer engagement models have changed with COVID-19, and some have changed forever. As organizations have migrated from a traditional technology stack to an application and infrastructure environment that includes cloud, multi-cloud containers, Kubernetes, mi
croservices, this environment is so much more complex and so much more dynamic than a traditional technology stack. When you have an issue, it can be rather like a murder mystery trying to find out what the issue is. So organizations that are doing microservices, they think about how can we pull together all of the information in context to better understand problems if and when they occur. And it has a bunch of new and different requirements in terms of trying to be able to figure out what's going on. It requires a new approach to monitoring. One of the key tenets of observability is really building systems that are designed to be observed and managed accordingly. And the way I like to think about it is that it's really a love letter to your future self or perhaps to someone on your team. It's really thinking about wait a second. Let's try and ensure that the outputs are going to be those that can help us to solve problems. If you were to be an observability vendor, I think there's a 
couple of key things that you're going to end up having to offer in order to meet demand from customers. So one is scale because especially as end users are in these cloud native environments, they're collecting a much bigger volume of operations data. So you have to be able to scale to be able to affordably collect that volume of data. A tool has to collect different types of data as well because there's increasing recognition that there's value in metrics and distributed traces and events and logs and errors. So you have to be able to collect all these different types of data within a single system. You've also got to have pretty sophisticated analytics. Then correlating that of course with business metrics, other kinds of metrics, so that the people that matter are the people that you can help from a customer service perspective. Or a couple of things from a product standpoint, first and foremost, full stack visibility, really having an opportunity to collect metrics, logs, traces, 
external and internal information, really bring it all together in context of an application service or a system. We've also found that this whole theme of it's not just about operational data, but collecting business data, understanding the nuances of the dependencies of different data components of a system. Got observability natives and others that are probably doing some observability washing. So certainly if we look at vendors across a number of spaces, certainly distributed tracing, logging and metrics, all of those vendors are repositioning around this observability notion. You have all of these different kinds of tools, different categories of tools out there. And that creates some problems for end users in terms of being able to correlate data that's collected and siloed within these different tools. And then you've got a new set of players that have come in and they're really thinking about observability in a different kind of way, where they're coming out of the notion as I 
say of troubleshooting and they're coming out observability very much in terms of understanding the behavior of a system from its outputs. Dealing with organizations and helping them to have a new way of working. And why are executives thinking about this? Well, the business drivers. This is really about reliability and customer experience. When we start telling together some of that infrastructure information and the telemetry around the infrastructure or applications are running, what's stuff that we're seeing in places like GitHub where we could take the social coding information and really begin to understand who checks something in, when they check something in, and you can begin to correlate system information with human telemetry. And I think that is really interesting. One of the things I want to start seeing from observability then just going forward, because that's really where the puck is going, I think.
 All right, so we've been working closely with the big batch of new cus
tomers or the past few months, get them up and running and observe. One of the common requests we've been getting from these new customers is to provide really more prescriptive guidance on how to configure observe to monitor and troubleshoot their applications. So towards that goal, we've been working on our out-of-the-box solutions to support more common scenarios you might encounter and I wanted to highlight a couple of these improvements. So first, we've greatly expanded on the observed AWS integration that we introduced last year. So now in addition to the 109 data sets that we provide to organize all of this data streaming out of AWS, we now provide dozens of pre-configured alerts to help you proactively monitor your services. And we've also expanded on the breadth of this integration with new support for Fargate-backed ECS and EKS clusters and much, much more. Second, we've released a new solution for monitoring plain Linux servers. Our Linux host monitoring solution brings toge
ther everything we've learned about collecting data from plain servers and includes support for scraping logs, monitoring host metrics like disk space and memory usage, gathering configuration information like what packages are installed, and even collecting performance metrics about individual processes running on those servers. We provide out-of-the-box data sets, dashboards, and alerts for all this data, and we even link it with Graphlink to all the content available in our other integrations. Now beyond these two, we're trialling new integrations for Google Cloud Platform, for Jenkins, and there's much, much more to come throughout the year.
 Welcome to Observe Dashboards. You can find dashboards on the left-hand menu, and then you can find dashboards provided by applications and or custom dashboards created from scratch. You can use this filter to navigate dashboards by application, and then use them as is to see the related data. If I want to create a dashboard myself, I'll go to
 the top right menu and click New Dashboard. And now I'm brought within the dashboard editor. The first thing I'll do is title my dashboard, and then I can begin to add things to my dashboard with this button here, or I could use the button within the dashboard itself, and see the various different options for what can be added to my dashboard. The top three here are most important as they relate to the actual data that can be brought into a dashboard. Beyond that we have different objects which can be used to help organize your dashboard and make it more appealing. And lastly we have parameters and filters, drop-down filters, and open text search bars, which can be incorporated into your dashboard to interact with the underlying data. However, we'll cover this in more detail in a separate video. To begin with dashboards, let's look at the three dashboard cards first. And first I'll select a data set table, which merely prompts me to select a data set, which can be brought in a tabular
 form into my dashboard. I'll search for Container Logs, and see that this card is brought in with a default format in no title. If I want to edit this, I'll need to click on the pencil icon, which will bring me into the dashboard card editor. From here I can title this card, and then I can choose to rearrange, or actually maybe even hide columns that I don't need to see in this particular view. And then when I go back to the dashboard, I'll see those changes reflected. Lastly, I can rearrange this card, or resize it. Next I'll add another card in my dashboard, and I'll move on to the data set visualization, which also prompts me to select a data set, and to bring it full circle, I'll look at Container Logs again. And so no visualization is configured by default, so I'll open it up to edit it, again title it, and then confirm that the visualization is configured properly using the expression builder, and also the visualization settings. Once I'm happy with this, I'll again go back to m
y dashboard, and rearrange this one as well. Now last but not least, I'll add a metric visualization, which brings me directly into the dashboard card editor, where I'll select a metric here. I can look at CPU utilization. Maybe I want to change the aggregation function. I'll keep the grouping as is. I might need to actually filter to my app namespace, last but not least, title it, and now I'm complete with this card. And I could go back to the dashboard, but if I wanted to navigate between cards and continue editing further, I could also use this menu up here to quickly switch between the two. Going back to my dashboard, I'll see my final card here, rearrange it, and I could continue further by adding collapsible sections, text blocks, or images, and or parameters and filters to my dashboard. However, I'll stop here, and I can save these changes, leave the editor, and now I can see my new dashboard in its full form.
 Hello and welcome to the observability trends webinar series. I'm Gr
ant Swanson, your host for today's session. We will be talking about observability best practices for multi-cloud environments. Our agenda begins with a brief presentation, setting the stage for those managing work modes across multiple clouds. We'll then dive into a live product demo of our observability cloud. Lastly, we'll open the floor for questions and answers. We recently launched our 14-day free trial offer for observability cloud. If you decide to sign up, our data engineering team will provide access and will be available if you need assistance with data ingestion or any specific use cases you're looking to solve for. Now I'd like to introduce our guest speaker, Max Kydin, a distinguished solution engineer. Take it away, Max. Before we dive into the demo, let me cover some of the observations, best practices, and lessons learned that ad-observe will learn from ourselves as well as our customers. Let's talk a little bit about the challenge first. The problem is that cloud plat
form, especially multi-cloud environments, they unlocked many unique technical business opportunities, but they also created many new challenges, especially from the observability perspective. And the reason for it is because these environments are based on ephemeral services, microservices, event-driven architectures, highly distributed computing, and all of these led to the explosion of observability to limitry. Sometimes reaching hundreds of terabytes per day in log metrics and trace data. And that really creates a lot of noise and difficulty in issue prioritization. Cloud providers have their own built-in observability and monitoring tools. But there's a couple of limitations in these tools. One is they're very focused on the specific cloud environment that they belong to. So for instance, Azure Monitor, well, is specifically for Azure. Google Stackdriver is specifically for GCP. So typically you end up leaning on third-party vendors, conventional monitoring platforms, and not very
 well-sutined for the modern cloud-native world, especially multi-cloud type of setups. One of the leading problems with them is they become prohibitively expensive in scale, because they're not based on an architecture that can handle massive volumes of telemetry data economically. And so what you end up doing as a result, you end up going with aggressive sampling, low data retention, juggling multiple storage tiers, all so that you can control the cost. While that may help you in terms of the control in cost, it all erodes the value of the telemetry that you collect. Additionally, because you store in data in disparate platforms, you may end up in a situation where you are unable to do correlations between this data, with this makes troubleshooting extremely difficult. So at Observe, we learned these hard lessons and we built the platform that is designed to address specifically multi-cloud challenge. So first and foremost, we believe that what you need is unified observability at sc
ale. In the modern architecture, you need to collect more data than you think you need. And the reason for it is because you want to have the possibility of dealing with future unknowns. And if you have sacrificed a lot of your data, if you decided not to collect it, you get hit by an issue, you may just simply not have the telemetry. And at that point, you're kind of stuck because you cannot go retroactively and retrieve it. So our belief is that you should collect everything and store it in a single data store, and the data need to be perpetually caught. You cannot go and juggle tiers and then deal with data rehydration in the middle of an incident. Data needs to be always available to you for the full duration of the retention period. And the retention period needs to be long. Two weeks are not going to cut it these days. You cannot do seasonal analysis, you can undo trend in analysis. Typically, you need to store data for more than a year in order to be able to make most use out of
 it. And once you collect that data, you need to be able to do something with it. And here we believe that schema on ingest, or as they call it schema on right, is actually the wrong solution. You need schema on demand or schema on read, which essentially gives you the ability to ingest any type of data. It doesn't really matter if it was structured unstructured, semi-structured, could be log, metric, trace, or any other type of data really, and just store it in the data lake for future use. But then let's say you often use a specific type of data, a specific set of metrics as a specific set of logs, you can retrieve it from the data lake and accelerate it in real time. Also, while you're at it, make sure to bring not just observability data, but what we call contextual data. So for instance, if you're using GitHub, then bringing GitHub Action Runs, bringing commits interior observability platform, they will help you to do meaningful data correlations and do root cause analysis, which 
brings us to the next topic, which is the platform the observability platform needs to be able to link and correlate data both temporally as well as based on the relationships between the data. And all of this needs to happen automatically in real time on the fly. So when you get hit by an incident, you can troubleshoot in context, and you can drive an understanding of impact and probable root cause analysis a lot faster. Now finally, if you're thinking of choosing the solution in this space, you need to lean on the vendors that provide out of the box content that can get you started with the common cloud environments out there. The reason why having an out of the box content is just as important as the flexibility of the platform is because you want to get started fast. Let's say you just migrated a portion of the application from a monolith sitting in a data center into a cloud environment, you want to be able to get going and monitoring it day one. And then finally, you need to get 
from all of this data, from all of this content, you need to get actionable insights and alerts just so that you don't get overwhelmed. You want to be able to leverage those relationships, the data correlations, to have actionable alerts and the dashboard that provide you that context and aid in triage and prioritization. So with that being said, let's dive into observe and see how we deliver these capabilities within our platform. Under the hood, observe is a data cloud. It can entice any type of data whether from your cloud environment such as AWS, GCP or Azure, as well as any on-prem data from physical data centers that you might have. Upon ingest, the data gets parsed in real time using flexible schema on demand to create a so-called data sets. Each data set represents a particular type of data, particular type of event. So for instance, cloud which locks is an example of a data set. GCP cloud function metrics is an example of another data set open to let me trace is yet another da
ta set as you may have already gleaned from this picture. Data sets are not isolated from each other. Observe in real time creates correlational links between them that aid troubleshooting in context. Let me show you what I mean by that. Let's say you got an alert for a particular AWS account. Now this alert might be related to an EKS cluster that is running within the account. Observe knows that EKS clusters are actually under the hood Kubernetes clusters. And it knows that Kubernetes clusters run a variety of different things. Most important of them is being pods. Pods in term have containers and those containers in term are running microservices that are instrumented with open telemetry. And this microservices are receiving regular updates when engineers are making changes to the application code and committed it to GitHub. So notice as I was traversing the graph, observe cap track of my actions by creating this breadcrumb trail on the bottom and look at this functionality graph lin
k which is essentially the ability to create a series of correlational links through the graph. This is something that allows you to ask interesting questions of the system. So for instance, you can say we have an alert for this EKS cluster. Show me all of the services that might be affected by an issue that we're investigating here. And by the way, pull in all of the GitHub commits that may have went in and can help us understand the root cause better. Just before we dive into a more in depth investigation, let's see what kind of content observe has out of the box that it can help us with our troubleshooting efforts. Observe has this concept of data apps or applications. And what these essentially are, there are bundles of content that is targeting a specific technology stack. So for instance, I have an AWS app, GitHub integration, GCP app, Kubernetes, primatious open telemetry and so on and so forth. And each one of these apps comes with four things. Data set and data sets can be log
s, metrics, or traces or really any other arbitrary type of data that you might be sending in through the system for that particular application. Dashboards, monitors and correlations. All of these come bundled with the application. So once you install the application, you just have these things out of the box. Now a lot of folks will start their troubleshooting naturally from dashboards. So let's say I want to understand the health of my AWS environment. I could just go ahead into a high level overview dashboard for AWS. And I have at my fingertips information about my AWS environment. Same goes for GCP. If I want to understand the GCP, I can go again, I had look at the high level overview dashboard for GCP and see what I have running up here. And in any, if any of these things are under pressure, if there's any sort of issues going on with this particular environment, there's also a lot of specialized dashboards that will give me an understanding of specifics of each individual servi
ce. So if I want to, for instance, look at GKE, I have a dashboard for that as well. Now, if a high level dashboard, which is really at just an aggregation of different metrics, is not enough and you want to dig deeper. Let's say you want to pick a particular metric for a particular service where you can jump into metrics explorer. A metrics explorer conveniently exposes all of the metrics that are coming into this environment. And by all, we don't just necessarily mean that metrics that are collected by the clouds themselves, such as Azure Monitor or Google Stackdriver, but also pretty much any other metric you may be sending into the system. So let's say you have open to elementary metrics coming in here, they'll show up in the metrics explorer as well. Now, we could go ahead and look, let's say, for instance, for GCP metrics in here. And let's say I want to understand the volume of a traffic that has been processed by my cloud functions, the volume of data. Well, it's very easy for 
me to go start customizing these different charts and just kind of filter in on a specific maybe lambda function and just get a really solid understanding of what's going on under the hood. So all of these functionality is available here. This is very powerful and can help you get further along in terms of the in depth understanding. Now, once I get these charts in a way in which I want, I can immediately spin up monitors from them. I can add them to dashboard and do a lot of other creative things with them that can help drive my troubleshooting process. We also have traces in here. So this is coming from the open to limited instrumentation. And this can be extremely powerful if you're trying to understand the behavior of microservice that spans multiple cloud environments. So I mentioned earlier, the cloud native tool and for instrumentation is usually focused on just one individual cloud. So AWS is going to do a tremendous job with X-ray for just AWS environment. But if you want to b
ring in your monolith applications from the data centers or you want to your microservices, let's say, span multiple clouds, it's going to become very difficult. But observe doesn't come with that limitation. You can very easily have a trace that spans multiple clouds. And finally, we have monitors. Observe comes with a number of monitors out of the box packaged with the applications. Now, as you may have noticed here, these come as templates, which essentially means that they're in a disabled state until you enable them. You can also customize them to your specific requirements. But this cover a lot of things out of the box. So you don't need to guess on things that you need to start monitoring once you onboard a particular cloud environment. And this is not just observability and monitoring. Some of these things might be related to security, such as S3 bucket permissions change. So there's really a really good coverage of the things that you might want to look out for across all of y
our different cloud environments. In the next section, we're going to go a little bit beyond of the out of the box content that is provided by Observe. We're going to create our own custom dashboard for a microservice that spans multiple cloud environments. And we're going to try to understand if there are issues in particular cloud environments that may be causing this microservice to malfunction. Let's see how we can explore a behavioral microservice that spans multiple cloud environments. Now, for our troubleshooting, we're going to jump into application logs. And we're going to look for a particular microservice. Let's say we have some people complaining that maybe there's something wrong with our shopping cart microservice. So I'm going to go ahead and select this microservice and filter on it and just explore the logs a little bit. This particular type of record at item A sync looks very interesting. And so I want to go ahead and filter on it. And I could say log approximately eq
uals add item. Let's go ahead and filter on that. There's not much I can do with this type of log yet because this information in this log is kind of locked behind the structure of this log, which is in this case is key value pairs. So what if I wanted to extract this using schema on demand into individual columns? In Observe, we have built-in parsers. So for instance, I could use a built-in key value log pair parser or I could use only GPT to generate the type of logic that is needed in order to extract the data from the log. And I'm going to go ahead and extract this data using the logic that has been suggested to me by Observe. And so now I have my individual columns, which include product IDs and quantities. And now we can go ahead and start visualizing this data. Observe is going to suggest the default visualization, but this is not exactly what I want. So I'm going to go ahead and massage it a little bit. I'm going to say let's go ahead and look at the quantities of different pro
ducts. And instead of counting the values, we want to sum the values as we want to group them by product ID, which is an information that we have just extracted. This probably will look better as a stacked area. And I personally like to have my ledges of the right. Now what can I do with this visualization? I want to create a little dashboard. At this point, this chart represents everything for all cloud environments. But what if I wanted to understand the performance of different shopping cart microservices that are spread across multiple clouds? How could I do that? What's very easy to do? I can just duplicate this chart. Let's go ahead and expand it a little bit. And I can edit it and say, well, instead of grouping by product IDs, I don't go ahead and group it by the cloud provider. And so now I could see that my my business is actually hosted on three different clouds on GCP, AWS, and I'm also using an IS provider popular in Europe called Hertzner. So I have a separate environment 
running in there. And I can see that majority of our traffic is actually served from Europe. Probably we have a lot of shoppers in there. I see some that are served from AWS and GCP that are located in North America. Now what if I didn't want to understand? Well, which specific EKS, GKE, or Kubernetes clusters, are servicing this microservice? Well, I can break down by that too. I can go ahead and now add another grouping by cluster. And so now I see that this is actually serviced by four separate clusters. One of them is located in Amazon, two in GCP. One is located in Hertzner. So this is already very useful information. If I do have some sort of issue, I can very easily pinpoint a particular cloud environment in here. In the interest of time, I'm going to scroll a little bit forward and show you a more complete version of this dashboard that where I leveraged matrix explorer and traces explorer to bring in some certain metrics from the .NET runtime. This particular service is runnin
g on .NET. So in this case, we're looking at garbage collection give utilization as well as we're bringing open telemetry tracing data to understand the lead-unces between different functions, between different operations within this microservice. And so this just showcases the power of the underlying data lake platform that allows you to seamlessly bring different types of data into a single dashboard across different cloud environments. In this final part of the demo, I'll show you how to leverage observe to troubleshoot an error that is generated by a service or an application or a piece of infrastructure with the goal to dig deeper into the root cause analysis as well as trying to understand which part of the cloud is actually creating this problem. I'm looking at a notification for a high service error rate and observe and I can tell right away this is related to the ad service. So I'm going to go ahead and click on this notification to see a little bit more detail of what's happe
ning in an observe. I can see the time frame on when the notification is triggered. I can also see any prior instances of this particular alert been raised for the service. I'm going to go ahead and drill into this service to understand what's going on here in more detail. As I explore red matrix within the dashboard for this particular microservice, which is the ad service, I can see that the latency, the rate of execution looks pretty good, but my error rate is spiking quite severely and at some point it reaches 50% of all of the requests. I can also tell which specific function call is causing an issue, which specific route. I can understand the upstream and the downstream impact as well as understand the slowest operations that have taken place in here. If I'm an SRE, the part of the screen that I'm probably most interested in is this one here on the bottom that shows me all of the error spans and traces. So my immediate reaction might be, just go ahead and click on one of these tr
aces. I can see that the culprit is here, which is this get ads microservice, and I can get additional contextual information by looking at the specific error messages. So here I can see that we have some sort of GRPC error message that says resource exhausted, but unfortunately there isn't a lot more of additional information our error messages now. At this point, I might decide to escalate this to an engineer, and it's very easy to do it with observer. I can just simply share a link with them that encapsulate all of the filters that I have applied in the current time of my investigation. So when the engineer receives this link, maybe via Slack or Teams call or whatever, they are taken into the exact same place I was looking at when I was doing my investigation. So they don't have to start from scratch. They can pick up where I left off. An engineer might decide to do a little bit more of an ad hoc exploration of the spans. And the way to do it is by spinning up what we call a workshe
et and worksheet is essentially kind of like a running the runbook that encapsulate all of the changes that you're making in observe as you're investigating an issue. So one thing that they might decide to do is say, well, for this particular microservice, is this localized to a particular cloud environment? Is this maybe localized to a cluster or something like that? And yeah, here we can see that all of our problems seem to originate in our IAS provider and localize into their EU location one. Immediately, they can say, why don't we open up the view into the virtual infrastructure that is running this microservice and see if there's any problems in there? And what observe will do is again, it will use the power of correlations, it will use graphene in order to take us from spans and traces into the underlying infrastructure in here. We can see that this particular piece that has been running fine for an hour. There are no issues necessarily that are happening here. My CPU and memory 
utilization metrics are looking pretty healthy, not seeing really anything that would indicate a hardware or a virtual infrastructure failure in here. So at this point, I might want to pull in additional logs. And it's very easy to do. I can go ahead and say, for this set of containers that we're looking at in here, just go ahead and put additional logs. And what observe will do is again, it will use the power of graphene to create a correlation on the flight for the specific logs that we're interested in. I'm going to go ahead and filter on failures and look a little bit deeper into the logs to see if I can glean any additional information in addition to what we had in traces. And unfortunately, I'm kind of a little bit out of luck here. I could still see this resource exhausted, but the description and the causes now. So unfortunately, we're not further along than we were before. Now, this is where bringing in contextual information within an observe is so important from any cloud en
vironment. And so what I could do here is I could say, for my service, let's see if we made any changes to the code that is run on the service. Have there been any GitHub commit? And indeed, we can see that around the time of the incident, somebody made a commit that set at service resource alignment. So that could explain potentially why we're running into the resource issues. And I could just go ahead and jump right into GitHub and say that, yes, indeed, Max Skybin committed a commit a commit about an hour ago. And this is something that is potentially causing our issue now. So now I see the culprit. And I can go back to Max and asking for to have a look and maybe roll back that particular commit. Now, notice that as we were doing our investigation, we essentially build this worksheet, this runbook that we can now save. And this is something that's going to be available not just for us, but for other people. And other people can modify, for instance, the time frame that where they wa
nt to troubleshoot an incident, or they can select a different microservice where they can make modifications to other filter and then selections that we've done. And so they can make this workbook their own and apply to their own use case. And this way, we can disseminate the knowledge that experts have about our infrastructure and applications to other folks who maybe knew to our platform. So at this point, we're going to wrap up the section and open up for Q&A. Thank you, Max, for the demo and presentation. We have a number of questions coming in that I'm monitoring at this point. The first one I see is, can I create my own custom correlations? Yes, certainly. While we do provide a vast array of correlations out of the box with our data apps, you're also at liberty to create your own on demand based on the data that you're sending into the system. Excellent. Let me take a look and see the next question. You mentioned cost savings compared to other platforms. Can you elaborate where 
they come from? Yeah, certainly. And that's a big topic, obviously, in and of itself. But generally speaking, the cost savings that you get in observe are a function of our architecture. And one of the critical pillars of that, if you will, is a separation between storage and compute. So the way we take the data is that we compress it on the fly and what you end up paying is the compressed storage of that data. Now, the data is still perpetually hot. It's not like we're putting it in a cold storage. So we're stored compressed hot data that you can retrieve at any point and work with. Now, when you're retrieving the query in this data, the portions of that data are become in accelerated. And so you consume compute as your accident already, a fraction of accelerated data that is stored in the data lake. Over time, that ends up being significant cost savings comparing to a lot of other platforms on the market. Got it. Thank you for that. We have, give me just one second. I'm looking the n
ext one. Do you support Azure? I didn't see it in the demo. Yeah, yeah. So I briefly mentioned Azure as we're going through the demo. And yes, we do support it. I'll post in the chat a link to our documentation to our Azure data app. Just simply, I didn't make it into this particular demo environment. But yes, we definitely support Azure as a cloud platform. Excellent. So the next question is interesting. What are the challenges associated with data governance and compliance across multiple cloud providers? Yeah. So I tried to answer some of these questions in the in the general chat as they were being answered. But obviously, these are really, really big and media topics. Right. And remember, we think about these topics from observability perspective, right? So we're not necessarily a data governance type of platform. We're thinking about them again from more of monitoring and observability perspective. But we see this manifest itself in a couple of different areas. One is it's very c
ommon to have a conversation about data residency data governance. And so, observe as presence in multiple Geos, we have customers, for instance, in Europe or in Asia, in order to be able to satisfy a specific governance posture within those specific geographies. Often times where the question also comes up is I have all of these multiple cloud environments. And I have some data in Azure, some data in GCP. All of this is subject to maybe PII, PHI, some of the other data governance restrictions that are specific to the domain that you're operating in. Now, I've sent that data into observability provider. How do I ensure that my role-based access control is applicable in this new environment? And so we provide a very robust data, sorry, Rbeck framework that allows you to leverage our data sets under the hood to do per column locking of information. So, for instance, let's say within your data, you have a field that represents some sort of PII data, maybe an email address or a password. T
his is something that we can upfuscage using schema on demand. And let's say only give full read access to security team for that particular type of data. Everybody else, like an SRE or somebody else, might only see the off-fuscated data within Observe. So you can have very, very granular controls. If anything else fails, like let's say you accidentally leaked that data that you were not supposed to to us, we can just go and wipe it out from the data warehouse and provide you a report that that action has been taken place to satisfy your compliance requirements. Excellent. So I think that answers all the questions. I appreciate that, Max. Thank you so much. I want to let everybody in the audience know that our next event will be on November 16th at 10am Pacific time, where we will discuss real-time incident detection and resolution and complex environments. We will also send a link via email with this recording and we will invite everyone who registered for this event to the upcoming w
ebinar. This concludes our session for today. Thank you for joining us and have a very day. Thanks, Max. Thanks, Grant. Thanks, everybody for joining. Bye.
  handmade uncult갈 TLG
 My name's Ethan Lilly and the engineering manager over here at Topgolf. I've been at Topgolf for two and a half years now. When I was growing up, the stars was the 90s. They were just making it big here in Dallas and they ended up winning the Cup in 99 and that's when I really got into hockey. I used to just play out in the street with some of my friends. It's one of the few sports I'm actually decent at. I mean, I always grew up around computers. I don't even remember the first computer we had. So I've been heavy into technology my whole life thanks to my dad that we're going to rate the NV into it himself. Our main mission is creating moments that matter. People do things like gender reveal parties or they do proposals at Topgolf. Any manner of big life event, we also have top-tracer technology which traces
 the flight of the ball and we can use that to do all kinds of things. This year we put out an Angry Birds game. You can actually play Angry Birds with a golf ball which is pretty crazy. Things that I've been working on since I joined is moving things to Kubernetes, more modern Docker orchestration engine. When I first started programming how it's on, we were maybe pushing updates to production six months to a year. That was pretty much our cadence and whenever we did it, it was very painful. And then over time, I've seen shows to quicker and quicker release cycles, smaller and smaller chunks getting pushed out to here at Topgolf. We're now on a weekly cadence. We push updates every single week. The four rules are mostly elastic search for centralizing a lot of our logging information. We're limited to what Kibana would allow us to do which they have some capabilities with dashboards and visualizations but we mainly use it just to dig through logging. Kibana uses a different search syn
tax called Lucine which is very annoying to use. It's like regular expressions but it's not which is really annoying for people. I would never really advertise it to anybody. Observe helps us to monitor the game system and the integrations with our POS system for giggling the checks. We send a lot of data. We send over 1.25 billion of insid day and all of that data is anything from a ball going into a target or a pendant light changing for a day or a new reservation coming in. What we've really gained with Observe is the ability to link our data from different data sources and ways that we never could before. Being able to link the data better between just our microservices alone and including different things like infrastructure with service now, with all these different platforms that we use that we wouldn't have even thought of on Game Before. We've also been exploring different ways to get more data into the platform before where previously we were trying to figure out ways to stop
 sending data because it was costing so much money. It's saving us time. It's allowing us to more quickly resolve escalations which is better for our guests. Nobody who wants to be sitting around waiting for some IT get to fix your problem. Before it was like a cat. We had this cat that had its own mind and wanted to do what it wanted to do and no matter what I wanted it to do with it matter. But now it's like having a dog. It's your buddy, he'll help you out when you ask him and he's not too complicated.
 Welcome to Observe Enablement. In today's video, we'll be going over Data Modeling. To go over Data Modeling within Observe, we're going to go over what Data Modeling is an Observe, why we should perform Data Modeling and when to do so most appropriately. Then we'll move into how Data Modeling is actually achieved, which is through the use of worksheets within Observe. And then lastly, we'll go over some considerations and tying everything together when it comes to publishing data se
ts, which are ultimately the result of any Data Modeling we do in Observe. So what is Data Modeling? Data Modeling is simply a set of filters and or transformations applied to the data, resulting in a new curated view of that data. And this is simply taking blobs of text or JSON or CSV data, however it's being sent into Observe and then transforming it into a table of relevant columns. Next, I want to take a moment to relate Observe to the rest of the industry and other observability concepts shared amongst other organizations. And that is introducing the idea of ETL versus ELT. This acronym represents extract, transform and load, which is the process of extracting data from source systems. This could be our logs or metrics. And then we have a decision as to whether we should transform it on the source system itself, which usually requires complex source specific configuration, or we could load it to an observability platform and then send the raw data and leave any and all transformat
ions to be performed within the platform itself. As you might guess, Observe very much embraces the ELT mindset we prefer to always model and observe. We think this is best in terms of managing source systems and also being able to adapt to the unknown unknowns. And finally, to conclude what is data modeling in Observe, we want to relate a data model or the output of any modeling that we do to a data set. A data set is a construct within Observe that represents the filters and transformations applied to a particular stage of data. And that captures not only the data transformations, but then provides a way of interacting and viewing that data. So a data set is a key term here and something we'll revisit throughout this enablement video. So before we get more into how we actually model data or when to do so, we'll take a moment to appreciate why we're modeling data in the first place. We're sending raw data to an observability platform. Is there a reason why I have to do anything? Well,
 the main reason we model data in the first place is for usability. To actually make our data useful, we typically need to curate those logs or events or metrics into data sets, making it easier to investigate that data, visualize it, and also monitor and alert on that data. Secondarily, some queries require parsing the data to some extent so to be able to perform the query intended. And this is in the case of filtering on nested values within some larger piece of data. And so the reason we want to model data is ultimately to make that data really usable. However, when it comes to using this data and modeling it, there is a best way of doing so. And so another reason why we model data the way we do and observe is for the sake of efficiency. Define data models, or in other words, data sets represent pre-process data which can be done in batch. By taking the incoming data and performing pre-defined data transformations that we set up in advance, we can allow our optimizer to best process
 that data. So that it is both most efficient in terms of query performance, but also most efficient in terms of usage cost and the underlying computation required to perform such a query. And so why do we model data? Ultimately, we want to make data usable and so we need to model it from its raw form into something more comprehensible. But then furthermore, in terms of actually modeling that data, we're going to do so with pre-defined models so to make it most efficient. All right, so you might be convinced on why we need to model data. When do we model data? Or in other words, when should we create a data set? Because at this point, I'm going to start having to use my words more carefully because modeling data can mean very different things in observe. This could be visualizing data, monitoring data, or just modeling it out for future investigations. And in this case scenario, we're mainly speaking about modeling data for the sake of investigations. And really, this is just modeling 
data to produce a data set. So when looking at when to model data or when to create a data set more specifically, yes, we would want to create a data set if any of the following conditions apply. The first condition is if this type of query will be run repeatedly, maybe it's a specific set of logs that need to be extracted into a proper form. If this is something the remaining team would also like to view regularly, then we definitely want to publish this as a data set. More than just looking at logs and event data is this a specific type of view, maybe a visualization that will be needed more than once by yourself for the remaining team. This could be time series data to look over the number of logs, errors or the latency within your application. If this is something that's needed regularly, yes, we would want to model this and publish it as a data set. And lastly, in the case that we need to monitor or alert on a specific set of data like app errors, then we would want to model this 
into its own data set to make that monitor most efficient. But there is cases where we would not want to model our data and publish it into a data set. Maybe we just want to model data and save it as a worksheet. And so worth noting here, we're still modeling data at the end of the day, but the difference is coming in between whether we publish a data set or just save it as a worksheet. And so we'll look at what this means when we go into observe and actually see how we create a data set, what it means to use a worksheet and how these two work together. But for now, it's worth appreciating that a data set is a published long standing data model, whereas a worksheet is just a snapshot. And so we would only want to work sheet if it's only going to be needed a few times or just once as part of an investigation. So now let's look at actually how to model data and I've already started mentioning worksheets without properly introducing them. And those are ultimately the vehicle for how we mo
del data and observe how we create data sets and more. And so let's look at what a worksheet is because that's what it means to create and model data. Worksheets are a data editor or a scratch pad, a tool within observe that allows you to model data or transform it in any which way to the fullest extent that observe can provide this way of modeling data applies not only to data sets, but also dashboards and monitors. We use worksheets to model data and publish them into data sets, but we often then also want to model data so that we can visualize that data in a dashboard. And finally, we also model data in case scenarios where we need something specific for the sake of monitoring for errors or certain messages. And so worksheets will be a familiar UI tool that you'll see in various places within observe. A worksheet is comprised of three components. There's a lot going on in the interface which we'll see in a moment, so I want to take a second to simplify it and to just these three com
ponents. The first is data stages. We'll see in a worksheet how there is a stage which actually contains the event data or logs that we're trying to model. Below that we'll see the opal console and here we're introducing maybe for the first time, opal, the observed processing and analysis language. Very similar to promql or other similar query languages out there. Opal is an abstraction over sequel which makes it tremendously easier to model and query time dependent data. And so the opal console will be our editor for writing and editing opal that actually transforms and filters that data. And then finally we'll see there's a right hand menu where we can apply further configuration to the data we're modeling or a filter menu to help quickly filter on columns. So let's jump into observe and take a look at how to actually put all this together. So now that we are within the observed platform, we're going to look at one of the most common use cases for modeling data. Most common because i
t has to do with Kubernetes, but also it's one of the most simple ways of modeling data too. Because we're able to take advantage of some of this pre-modeled data that comes from the Kubernetes app. You should have installed the Kubernetes app as one of your observed applications and ingested Kubernetes data using the observed agent. And then that would lead you to having the container logs data set here available in your data sets tab. So I can click on this and then I'll be brought to the log explorer where I see the container logs and all of its data. It might also be possible that you're looking at the data set page for the container logs. And in either case, those are going to be our starting points for modeling data. By identifying the source data that we want to work with, we'll go to either the log explorer or that corresponding data set page so that we can then create a worksheet. So a worksheet is always created from the source data set that you're hoping to build upon and yo
u'll look for it in the top right corner. On a data set page, you might just see a button called create worksheet. Here in the log explorer, you'll find this underneath the actions menu where you can click on actions and then open in a worksheet. This will bring you into a new tab which presents a worksheet. And like we mentioned earlier, a worksheet can have a lot going on at first glance, but ultimately there's only three major components here. The first one is our data stage, which by default is named stage one, but we could change this into whatever we'd like. I can resize this stage and I can see the actual data here as defined by the query window. Next is the Opal console, which you may find hidden down below, which you can click to open up or maybe it's already open by default. But down here below your data stage, you'll see the Opal console and like we mentioned earlier, this is where we'll be writing Opal, the query language for helping model and filter our data. And so I can 
apply filters or I can make new columns will come back to talk more about Opal in just a moment, but for now we can actually hide the Opal console because we won't need any raw Opal to perform the data modeling that we need to do. And today is exercise. But lastly, before we get into the data modeling itself, we'll look at the third component of worksheets, which is this right hand menu, which will have various configuration options or options to filter based on how you're interacting with the worksheet. And so let's now get into modeling our data and I'll jump into it by making use of this filter ability here. I'll go over and I'm looking at these container logs, I can see various metadata and I want to use the container column to filter down to my app logs. So just by clicking on this checkbox here, I can instantly do that and now I'm only looking at my app logs, which appear to have a JSON format. And so one way we can interact with the data here is by double clicking into any of th
ese stages, so to bring up the inspector, the inspector is merely a better way of viewing the individual cells of data and you'll see it pop up right alongside the Opal console. And so this can be resized to better view the data and I can actually interact with this data right here in the inspector. If I wanted to filter to a specific value like only debug logs, I could do so or I could exclude debug logs or I could just extract this field into its own separate column. And then I can see all of the different options and filter even more using this new column now available for the level. And that's how easy it is to extract data from one column into a new column. And so now I probably want to extract more of this data given there's so many fields and I can do so more easily instead of one by one by instead clicking on the header drop down. And then I'm going to look to extract well, I would like to extract from JSON, but I see here that it only says extract from string. So I might click
 on this to see what I can do and I could use a custom regular expression or I could leverage common patterns to be able to parse out specific data from an arbitrary string. However, this doesn't really help me in terms of extracting fields from a blob of JSON. And so what's wrong here and this is very common so it's worth pointing out here, you'll need to convert this column to a proper data type. And in this case, JSON. But it is also worth mentioning that oftentimes when you can't work with columns that look like numeric values, but don't support numeric operations, you may need to convert them to an integer or a float. But in this case, I'll convert my string blob to a JSON block. And then I can click again on this drop down menu and see that the extract from option has changed to JSON instead of string. So I'll extract from JSON and this gives me way better options for extracting multiple fields. And so I can extract all of the different fields that I'd like. I already got the lev
el and so maybe that's all I need for right now. I'll click apply. And now I can see I have this data available within their own separate columns. And one of the main reasons for doing this is one is it's much more comprehensible, but also I can click on these columns and quickly filter and see the number of different values within this set, which again is specific to this query window. So now what I've done is I've been able to pull out the data I want. I don't really need this log column anymore. And so maybe I want to hide it. Now I'll point out here the table controls button, which is often overlooked, but very powerful in helping model and view data exactly as you'd like. You can rearrange columns or hide as many as you'd like so to clean up this view of data that you had. But now I want to take a second to put this full circle before we publish this because we have a curated view now, which is extracted the data for some from some underlying Jason. And so really this concludes mo
deling our data. We've been able to filter to a specific set of app logs and then extract fields from a blob of Jason. Ultimately, we could jump right into publishing this as a new data set. But before we do so, I want to take a second to appreciate the Opal console once more. Although we can do most of our modeling within the UI, and this is often our first step in getting things started because it's easiest. However, even with basic modeling, it's best to take a peek at the Opal console just to double check that the automatically generated Opal is appropriate. And so what we can see here is two main Opal verbs being used. And you can really do a lot with just these two verbs filter and make call, which represents make call. However, it's abbreviated. And these two verbs are very simple, but yet very powerful. And we can see here retracing our steps from what we did earlier to model this data. I filtered to a specific container. And then I extracted the level column on its own when I 
was using the inspector. And so the way it did that is it actually parsed the log column in Jason in line so to extract the level column. However, I then wanted to extract all of the other fields. And so I parsed the log column again. And then I was able to extract more easily the remaining columns. So if I wanted to optimize my Opal here, I could avoid having to parse the log column as Jason twice. So what I'm going to do is I'm going to take the creation of this level column and just move it into the make column statement here below. You can see here that we can create multiple columns as a comma delimited list. I can delete this statement here. And so now I've cleaned up my Opal and optimized it. But yet it produces the same exact result as before. What I'll need to do though to verify is run. Note that if you save a worksheet or a data set without running the Opal console, those recent changes will not have been applied. And so always make sure you run any Opal changes you've made.
 Now I mentioned that filter and make call are the two most common Opal verbs that you can use to do basic modeling. There is one last one I want to introduce before we leave Opal. And again, these three verbs alone could be used to perform a ton of different modeling that produces a lot of value. And that last Opal verb is pit call. Pit call is simply a way of listing out only the columns that I want to keep within this data set. So any columns that are not listed will be dropped and excluded from this view. This also determines the order of columns and how they're presented in my data set. And so pit call is often used as a way of removing unnecessary columns and placing other columns in the proper order. So I can just start listing out my columns with a comma delimited list. I do need to make sure though that I include a column that represents my time stamp. All of this event data within observe is very much dependent on time. And so whenever we use pit call, you can choose to pick 
as many columns as you like or exclude as many columns as you like so long as one of the columns includes a time stamp. So we got our time stamp. We'll go through and start adding the remaining columns. Here I always like to put certain columns first to make it easier to digest and filter. So we have our level, our method, maybe our trace ID and span ID. Notice we can take advantage of auto complete here. And this includes all of the columns that I extracted from the JSON blog. However, there was some other columns here, including Kubernetes metadata that was already pulled out before I even started modeling. And we definitely want to carry these columns with us. And so to do that, we'll need to include these individual columns in the pit call statement as well. And we can start doing so if I type container. I can see container name. Maybe I need the pod name cluster namespace. And you may have noticed that some of these columns don't exactly match up with the column headers that you s
ee here. And this has to do with the fact that these are links. And so it's worth discussing some of these nuances with links so that when you're modeling data, you understand what's going on. So links are a whole separate topic within observe something that will have its own dedicated enablement content for so to properly set up links and to use them best. However, for right now, we just need to understand how links show themselves within data sets. So here we have Kubernetes metadata attached to each log message, which tells me which container and pod that this log message came from. And in doing so, I can link together this log message with other pod data relevant to this specific pod. And so from the API updates, which is a completely different set of data within observe coming from Kubernetes. But this API update will give me information about the pod in terms of its status, its restart count, its underlying node, etc, etc. And so by linking together container logs with pods, we c
an then see both sides of the data within just a few clicks. And so this applies to containers. I'll see the columns here render after a short bit of loading. Also with the cluster, the namespace or the node. And I actually need to click on individual cells to see the specific data for that resource. And so that's why linked columns exist is so to be able to jump between different sources of data quickly. And so there's a lot more to links, but the reason they're introduced here is so to appreciate how they're used in terms of data modeling. So here we can see a column that represents the pod, but of course this is only the pod label, which is the pod name, but in this case, it's not actually the pod column because this is representing a link. There's an underlying column here that I can reveal by clicking on this button, which is the pod name. And so in this case scenario, I can see the pod name is the same as the pod label. And so these columns match up, but if I look over to the clu
ster column, I can unhide its key columns and scroll over and see the cluster UID. So here for a cluster resource, I can see the cluster name as its label, which makes it a lot easier to filter and view in the UI. But underneath the hood, the log message and the data here in this data set really only contains the cluster UID. The cluster UID is really what I'm going to want to use when picking it out using pick call. And so with this, I can run and by selecting all the underlying key columns, the links will carry through automatically. And so now to conclude with my data modeling exercise, I want to save and publish this into something that I can use later. We've talked a lot about worksheets, what they are and how to use them. We've been using them thus far to create a new view of our data. And the way we save worksheets is by simply titling it up here and then using this button to save this worksheet. And now this exact snapshot will be saved for me to come back to it later. If I go 
into the worksheets tab and then look under my worksheets, I would see my new worksheet here, which brings me back to exactly what I saw just a moment ago. However, if I want to actually publish this into more than just a worksheet and into a longstanding data set, I can click on my stage and go over to the published new data set button here. I'll click on this and see that there's already a data set called app logs and so I'll need my name to be unique. And I can also prefix any data set name with a package name separated by a forward slash and then I can name this data set and group it together with other relevant data sets. And so if I publish this, I'll see that I'm brought back into a worksheet, but instead of my input being container logs, my input is now the app logs. And so going back to the data sets I can search for my package. And now I can see a new grouping for my data sets here called my package. And if I click into my newly created data set called app logs, I can see exa
ctly what I modeled out here now more comprehensible and more usable in terms of filtering. And this concludes a brief exercise into modeling data with observe. To wrap up this short little demo, we'll go over what we just did at a very high level. What we did is we started from a data set, our source in this might have happened through the log explorer as well. And then we clicked into a worksheet so that we could really start modeling this data. And then ultimately we can publish this data into its own data set and or save it as a worksheet. But that's really it when it comes to modeling data and observe and using just the tools shown in this introduction, you can achieve a lot of value by modeling out large sets of logs into app or team specific subsets. And really all you need to use is filters and basic extractions. However, we'll conclude today's enablement video by taking a brief look at some additional topics, which you can look into to become a more advanced data modeler. And 
so we'll take a peek at five different topics related to advanced data modeling, each of which will have their own dedicated enablement video, which you can use to get up to speed. And so just to introduce these topics so that you're aware and can know where to look next if you want to get more involved. The first one is a multi stage lineage, or I could say multi stage data modeling. What we just did in our exercise is we looked at a worksheet that contained only a single stage of data from which we applied all of our filters and transformations and then use only that single result as our published data set. However, in this little example shown here, you can imagine we might take raw data, which starts with a single stage, but then is split out into two additional stages. One is filtered to app one, and that stage contains JSON data, and then we have a separate stage filtered to app two, which contains plain text key valued pairs of data. And what we want to do is extract those two f
orms of data and normalize it or map it into a common format. And so what we would do is start from one stage, split into two stages, and then recombine back into a single stage using a union. And that would produce our general data set, which contains all of the different logs within one format. And so that's what we mean by multi stage lineage. There's definitely demos and exercises we could go through to help better appreciate how this works and observe. The next one is advanced transformations. Within observe, we took a small peak at what we can do using Opal, the processing language that we use to model data. There's many things we can do within the UI to extract data or make additional transformations. However, there will be certain things only possible through the Opal console and through writing Opal code. And this is a time to introduce the Opal documentation. Again, there will definitely be dedicated enablement content just for how working with Opal. But for now, you can take
 a look at the observed public docs, specifically the Opal ones, which you can find here on the left menu and get a high level overview of the language itself. But then also most importantly worth pointing out is the list of Opal verbs, which shows all of the different ways you can start a new Opal statement. So without getting into the actual details of how to use Opal, you can see though there's really only 30 to 50 Opal verbs. It isn't the most extensive language like you would expect with learning Python or GoLing. But in addition to Opal verbs, we also have Opal functions and functions operate very similar to other programming paradigms in which you can do simple operations to help manipulate the data or calculate different things. And so that concludes what we mean in terms of advanced transformations, just taking some of the stuff we did earlier and going a bit further. Next we have aggregations and windows, which are actually another form of advanced transformations. But being 
so comprehensive, aggregations and windows deserves its own separate topic. And of course, its own separate dedicated enablement video, which you can find linked to this. Aggregations though represent looking at multiple events or multiple logs or metrics over a certain time window and then determining how we should summarize aggregate or calculate something based on those many events. Next up we have joins and linking. We also saw a little introduction today into how linking data works and observe and we'll see how we can use Opal to actually define those links and very related to linking disparate data sets together. We can join disparate data sets together so to pull in that relevant information like the cluster name, if we need to actually use it in different data sets. And then last but not least, we have sub queries available and observe. I mentioned earlier that Opal is an abstraction on top of some complicated SQL. And so you can imagine that given how common sub queries are wi
thin SQL, we support a syntax that allows sub queries within observe using Opal. However, to conclude, I don't want to end on all of the different advanced topics. Those are available and can help you realize really interesting insights from your data. However, the exercise we went through together in which we only use the UI to make simple filters and extractions really represents 80% of the data modeling done by our customers at observe. By taking large sets of logs or metrics and deducing them into simpler, more comprehensible sets in which the underlying data is surfaced to the top. That is all that's needed to really bring value to your observability platform. So thank you for joining us today on introducing data modeling. We hope to see you again to learn more.

